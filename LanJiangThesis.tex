%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
% IIT Sample THESIS File,   Version 3, Updated by Babak Hamidian on 11/18/2003
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% File: sample3.tex                                   %
% IIT Sample LaTeX File                               %
% by Ozlem Kalinli on 05/30/2003                      %
% Revised by Babak Hamidian on 11/18/2003             %
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%                                                     %
% This is a sample thesis document created using      %
% iitthesis.cls style file. The PDF output is also    %
% available for your reference. In this file, it has  %
% been illustrated how to make table of contents,     %
% list of tables, list of figures, list of symbols,   %
% bibliography, equations, enumerations, etc.         %
% You can find detailed instructions                  %
% for using the style file in Help.doc,               %
% TableHelp.doc, FigureHelp.doc, and                  %
% Bibliography.doc files.                             %
%                                                     %
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Note: The texts that are used in this sample3.tex   %
% file are irrelevant. They are just used to show     %
% you the style created by iitthesis style file.      %
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\documentclass{iitthesis}

\input .tex
% Document Options:
%
% Note if you want to save paper when printing drafts,
% replace the above line by
%
%   \documentclass[draft]{iitthesis}
%
% See Help file for more about options.

\usepackage[dvips]{graphicx}    % This package is used for Figures
\usepackage{rotating}           % This package is used for landscape mode.
\usepackage{epsfig}
\usepackage{mathrsfs}



%\usepackage{amssymb, amsmath, amsthm, color,enumerate}
\usepackage{multirow}
\usepackage{url}
%\usepackage{algorithm}
\usepackage{mcode}
\usepackage{amsfonts,amsmath,amssymb,amsthm,color,enumerate,mathtools,bbm,booktabs,array,tikz,pifont,comment,multirow,url,graphicx,float,verbatim,mathrsfs}





%\usepackage[dvips]{graphicx}    % This package is used for Figures
%\usepackage{rotating}           % This package is used for landscape mode.
%\usepackage{epsfig}
%\usepackage{subfigure}          % These two packages, epsfig and subfigure, are used for creating subplots.
%% Packages are explained in the Help document.
%\usepackage{mathrsfs}
%\usepackage{mathptmx}       % selects Times Roman as basic font
%\usepackage{helvet}         % selects Helvetica as sans-serif font
%\usepackage{courier}        
%\usepackage{mathtools}% selects Courier as typewriter font
%\usepackage{commath}
%\usepackage{makeidx}         % allows index generation
%\usepackage{graphicx}        % standard LaTeX graphics tool
%\usepackage{stackengine}                 % when including figure files
%\usepackage{multicol}        % used for the two-column index
%\usepackage{amsthm}
%\usepackage{amsmath}
%\usepackage{amsfonts}
%\usepackage{amssymb}
%\usepackage{color}     % Do not use: colors will appear in gray scale!
%\usepackage{url}
%\usepackage{verbatim}
%\usepackage{cite}
%\usepackage{chemarrow}
%\usepackage{listings}
% \usepackage{alltt}
% \usepackage{mcode}
% \usepackage{booktabs}
% \usepackage{array}
%\newcolumntype{L}{>{\centering\arraybackslash}m{3cm}}
%\usepackage[autolinebreaks]{mcode}
\input Ljiangdef.tex
%
%\newcommand{\bbz}{{\bf z}}
%\newcommand{\bbx}{{\bf x}}
%\newcommand{\bby}{{\bf y}}
%\newcommand{\bbX}{{\bf X}}
%\newcommand{\bbY}{{\bf Y}}
%\newcommand{\bbH}{{\bf H}}
%\newcommand{\bbB}{{\bf B}}
%\newcommand{\bbA}{{\bf A}}
%\newcommand{\bbalpha}{\boldsymbol\alpha}
%\newcommand{\bbbeta}{\boldsymbol\beta}
%\newcommand{\bbc}{{\bf c}}
%\newcommand{\bba}{{\bf a}}
%\newcommand{\bbb}{{\bf b}}
%\newcommand{\bbh}{{\bf h}}
%\newcommand{\bbn}{{\bf n}}
%\newcommand{\bbd}{{\bf d}}
%\newcommand{\vgamma}{\boldsymbol\gamma}
%\newcommand{\veczero}{\mathbf{0}}
%\newcommand{\EE}{\mathbb{E}}
%\newcommand{\NN}{\mathbb{N}}
%\newcommand{\DD}{\mathcal {D}}
%\newcommand{\RR}{\mathcal {R}}
%\newcommand{\PP}{\mathcal {P}}
%\newcommand{\HH}{\mathcal {H}}
%\newcommand{\FF}{\mathcal {F}}
%\newcommand{\XX}{\mathcal {X}}
%\newcommand{\QQ}{\mathbb {Q}}
%\newcommand{\FFF}{\mathbb {F}}
%\newcommand{\UU}{\mathbb{U}}
%\newcommand{\Order}{\mathcal O}
%\newcommand{\wup}{{\mathcal U}}
%\newcommand{\ccup}{{\mathcal C}}
%\newcommand{\reals}{\mathbb{R}}
%\def\abs#1{\ensuremath{\left \lvert #1 \right \rvert}}
%\newcommand{\ip}[3][{}]{\ensuremath{\left \langle #2, #3 \right \rangle_{#1}}}
%\newcommand{\ch}{\mathcal{H}}
%\newcommand{\fix}{\mathrm{fix}}
%\newcommand{\var}{\mathrm{var}}
%\newcommand{\spann}{\operatorname{span}}
%\newcommand{\eps}{\varepsilon}
%\newcommand{\bx}{{\mathbf x}}
%\newcommand{\by}{{\mathbf y}}
%\newcommand{\bz}{{\mathbf z}}
%\newcommand{\bc}{{\mathbf c}}
%\newcommand{\bv}{{\mathbf v}}
%\newcommand{\ba}{{\mathbf a}}
%\newcommand{\rad}{r^*}
%\newcommand{\XXX}{{\mathfrak X}}
%\newcommand{\rt}{k}
%\newcommand{\E}    {\operatorname{E}}
%\newcommand{\V}    {\operatorname{Var}}
%\newcommand{\err}  {\operatorname{error}}
%\newcommand{\cost} {\operatorname{cost}}
%\newcommand{\id}   {\operatorname{id}}
%\newcommand{\scp}[2]{\langle #1, #2 \rangle}
%
%\newcommand{\vx}{\boldsymbol{x}}

\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{algorithm}[theorem]{Algorithm}
%\newtheorem{defn}{Definition}[theorem]
\theoremstyle{definition}
\newtheorem{defn}[theorem]{Definition}
%\newcommand{\meanMCB}{\tt meanMCBer\_g}
%\newtheorem{corollary}{Corollary}[chapter]
%\newtheorem{definition}{Definition}[chapter]
%\newtheorem{remark}{Remark}[chapter]
%\newtheorem{proof}{Proof}[chapter]
%\renewcommand{\theenumi}{A\arabic{enumi}}
\begin{document}
% Define all the symbols used.

%%% Declarations for Title Page %%%
\title{Guaranteed Adaptive Monte Carlo Methods for Estimating Means of Random Variables}
\author{Lan Jiang}
\degree{Doctor of Philosophy} \dept{Applied Mathematics}
\date{May 2016}
%\copyrightnoticetrue      % crate copyright page or not
%\coadvisortrue           % add co-advisor. activate it by removing % symbol to add co-advisor
\maketitle                % create title and copyright pages


\prelimpages         % Settings of preliminary pages are done with \prelimpages command


%%%  Acknowledgement %%%
\begin{acknowledgement}     % acknowledgement environment, this is optional
\par  This dissertation could not have been written without Dr. Fred J. Hickernell who not only served as my supervisor but also encouraged and challenged me throughout my academic program. He and the other faculty members, Dr. Greg Fasshauer guided me through the dissertation process, never accepting less than my best efforts. I also thank my committee members, Dr. Lulu Kang and Dr. Gady Agam, for their encouragement, insightful comments, and hard questions. I appreciate the joint work with Dr. Art Owen and all the GAIL team members including Sou-Cheng Choi, Yuhan Ding, Yizhi Zhang, Llu{\'i}s Antoni Jim{\'e}nez Rugama, Xin Tong and Xuan Zhou. I would also like to thank my parents, my husband, and my kids for their supports.
\end{acknowledgement}


% Table of Contents
\tableofcontents
 \clearpage

% List of Tables
\listoftables

\clearpage

%List of Figures
\listoffigures

\clearpage



%%% Abstract %%%
\begin{abstract}% abstract environment, this is optional

Monte Carlo is a versatile computational method that may be used to approximate the means, $\mu$, of random variables, $Y$, whose distributions are not known explicitly. This thesis investigates how to reliably construct fixed width confidence intervals for $\mu$ with some prescribed absolute error tolerance, $\varepsilon_a$, relative error tolerance, $\varepsilon_r$ or some generalized error criterion. To facilitate this, it is assumed that the kurtosis, $\kappa$, of the random variable, $Y$, does not exceed a user specified bound $\kmax$. The key idea is to confidently estimate the variance of $Y$ by applying Cantelli's Inequality. A Berry-Esseen Inequality makes it possible to determine the sample size required to construct such a confidence interval. When relative error is involved, this requires an iterative process.  This idea for computing $\mu = \e(Y)$ can be used to develop a numerical integration by writing the integral as $\mu = \e (f(\vx)) = \int_{\reals^d}f(\vx)\rho(\vx)\dif\vx$, where $\vx$ is a $d$ dimensional random vector with probability density function $\rho$. A similar idea is used to develop an algorithm for computing $p = \e(Y)$ where $Y$ is a Bernoulli random variable. All of the algorithms have been implemented in the Guaranteed Automatic Integration Library (GAIL).

\end{abstract}
\textpages     % Settings of text-pages are done with \textpages command
% Chapters are created with \Chapter{title} command
\Chapter{INTRODUCTION}\label{introduction}

\Section{Monte Carlo simulation}

Monte Carlo is a widely used simulation method that can be applied to evaluate means of random variables, perform numerical integration, estimate probabilities, understand and control complex stochastic systems, price financial derivatives and solve other real world problems.

 In the case of estimating the mean, $\mu$, of a random variable, $Y$, i.e.: $\mu = \e(Y)$, one generates $n$ independent and identically distributed (IID) $Y_i$ from the distribution of $Y$ and take the sample average, 
$$\hmu_n :=\frac1n\sum_{i=1}^nY_i.$$ The Strong Law of Large Numbers ensures that the sample mean converges to the true mean almost surely, i.e.: $\lim_{n \to \infty} \hmu_n =\mu \text{ a.s.}$ \cite[Theorem 20.1]{JP04}. 

An important special case of computing $\mu=\e(Y)$ is when $Y = f(\vX)$ for some function $f:\reals^d \to \reals$ and some random vector $\vX$ with probability density function $\rho: \reals^d \to [0,\infty)$. One may then interpret the mean of $Y$ as the multidimensional integral
\begin{align*}
I= \mu(f) = \e(Y) = \int_{\reals^d}f(\vX)\rho(\vX)\dif \vX.
\end{align*}
The Monte Carlo method becomes a method for numerical integration.

In this thesis, we mainly focus on IID sampling instead of quasi-Monte Carlo (qMC) sampling, since qMC usually requires the integrand $f$ to have mixed partial derivatives which defines the variation of the integrand $f$ in order to satisfy the specified error condition as suggested in Koksma-Hlawka Inequality \cite{Hickernell14}, whereas IID sampling does not. In fact, IID sampling does not require $Y=f(X)$, The random variable $Y$ can be quite general. We only need user provide the error tolerance and a blackbox to generate the random variables. It is much simpler than the traditional numerical integration approach.

If the random variable $Y$ is a Bernoulli one, which takes only two values, 0 and 1, with  $\Pr(Y=1) =p$. The case of computing the mean becomes estimating the probability $p = \e(Y)$. 

This thesis suggests the ways to estimate the means of the random variables mentioned above to a prescribed error tolerance with a high confidence level by determining an appropriate sample size required. 

\Section{Guaranteed confidence interval}

The Central Limit Theorem (CLT) stated in Theorem \ref{clt} provides a way to construct an \emph{approximate} confidence interval for $\mu$ in terms of the sample mean assuming a known variance,
\[
\Pr(\abs{\hmu_{n}-\mu} \le \varepsilon) \approx 1- \alpha. \qquad \]
Unfortunately, this is an asymptotic result. The proposed procedure attains the desired coverage level in the limit as $\varepsilon\to 0$ but does not provide coverage guarantees for fixed $\varepsilon>0$.  
%We want such fixed $\varepsilon$ guarantees. 

\begin{figure}[htbp]
\centering
\begin{minipage}{7cm}\centering 
\includegraphics[width=7cm]{mixturegaussian/mixturegaussianpdf-2015-12-06-17-30-15.eps} 
\end{minipage}
\caption{The probability density function in \eqref{pdfy}}\label{fig:mixturegaussianpdf}
 \end{figure}
 
Consider a random variable $Y$ with the density function 
\begin{align}\label{pdfy}
\rho(y) = \frac{1}{\sqrt{2\pi}}\left(0.99\exp\left(-\frac{x^2}{2}\right)+0.01\exp\left(-\frac{(x-200)^2}{2}\right)\right).
\end{align}
It is a mixture of two Gaussian distributions. Figure \ref{fig:mixturegaussianpdf} shows the probability density function. It is obvious that the data are clustered around 0 and it has a heavy tail around 200. The kurtosis $\kappa \approx 98$. The mean of the random variable $Y$ can be written as
$$\e(Y) = \int_{-\infty}^\infty y\rho(y)dy.$$
If we use CLT to estimate the mean with confidence level $99\%$, absolute error tolerance $0.01$ and initial sample size 100, we get the answer correct $65.6\%$ of the time.

Why CLT fails? The first reason is that the insufficient samples (100) are used to estimate the variance, we do not have a conservative variance estimation to handle this heavy-tailed distribution. The other reason is, CLT uses an approximate error bound, not the guaranteed error bound.

In this thesis, we investigate the ways to estimate the mean of a random variable that satisfies the fixed width confidence interval condition with some prescribed absolute error tolerance $\varepsilon_a$,  
\begin{align}\label{absCI}
\Pr(\abs{\mu-\hmu_n} \leq \varepsilon_a) \geq 1-\alpha,
\end{align}
or relative error tolerance $\varepsilon_r$,
\begin{align}\label{relCI}
\Pr\left(\abs{\mu-\hmu_n}\leq \varepsilon_r\right) \geq 1-\alpha,
\end{align}
or some generalized error criterion, $\tol(\varepsilon_a,\varepsilon_r \abs{\mu})$, 
\begin{align}\label{hybridCI}
\Pr\left(\abs{\mu-\tmu_n}\leq \tol\left(\varepsilon_a, \varepsilon_r\abs{\mu}\right)\right) \geq 1-\alpha.
\end{align}
Our assumption is a bounded kurtosis of the random variable $Y$,
\begin{equation} \label{def:boundedkurtosis}
\kappa = \frac{\e[(Y-\mu)^4]}{\sigma^4} \le \kappa_{\max},
\end{equation}
where $\sigma^2$ is the variance of the random variable $Y$ defined in \eqref{truevar}, and $\kmax$ is some bound defined in Definition \ref{def:kmax}. 

After reviewing the existing literature, we found the existing theory of fixed width confidence interval may make distributional assumptions that are too strong. Stein \cite{stein45} proposed a similar two stage procedure to our Algorithm \ref{alg:meanMCg}, however, his result requires normally distributed data and has no relative error criterion involved. In Monte Carlo applications one typically does not have much information about the underlying distribution. The form of the distribution for $Y$ is generally not known, $\var(Y)$ is generally not known, and $Y$ is not necessarily bounded. We are aiming to derive fixed width confidence intervals that do not require such assumptions.  

The width of a confidence interval tends to become smaller as the number $n$ of sampled
function values increases. In special circumstances, we can choose $n$ to get a confidence interval of at most the desired length and at least the desired coverage level, $1-\alpha$. For instance, if the variance, $\sigma^2=\var(Y)$, is known, then an approach based on Chebyshev's Inequality in Theorem \ref{ChebyThm} is available, though the actual coverage will usually be much higher than the nominal level, meaning that much narrower intervals would have sufficed. Known variance in addition to a Gaussian distribution for $Y$ supports a fixed width confidence interval construction that is not too conservative \cite{stein45}. The CLT provides a confidence interval that is asymptotically correct, but our aim is for something that is definitely correct for finite sample sizes. Finally, conservative fixed width confidence intervals for means can be constructed for bounded random variables, by appealing to exponential inequalities such as Hoeffding's \cite{H63} or Chernoff's Inequality \cite{Chernoff52}. We will discuss the way to estimate the mean of Bernoulli random variables by such inequalities.

If the relevant variance or bound is unknown, then approaches based on sequential statistics \cite{Siegmund85} may be available.  In sequential methods one keeps increasing $n$ until the interval is narrow enough. Sequential confidence intervals require us to take account of the stopping rule when computing the confidence level. Unfortunately, all existing sequential methods are lacking in some aspects. 

Serfling and Wackerly \cite{SerflingaWackerlyb76} consider sequential confidence intervals for the mean (alternatively for the median) in parametric distributions, symmetric about their center point. The symmetry condition is not suitable for general purpose Monte Carlo applications.

Chow and Robbins\cite{ChowRobbins65} develop a sequential sampling fixed width confidence interval procedure for the mean, but its guarantees are only asymptotic (as $\varepsilon \to 0$). \cite{MukhDatta96} give a procedure similar to Chow and Robbins's, and it has similar drawbacks.

Bayesian methods can support a fixed width interval containing $\mu$ with $1-\alpha$ posterior probability, and
Bayesian methods famously do not require one to account for stopping rules. They do, however, require strong distributional assumptions.


%An important case pf computing $\mu = \e(Y)$ arises in the situation where $Y=f(\vX)$ for some function 
%Suppose we want to evaluate the multidimensional integral with integrand $f:\reals^d \to \reals$ with respect to some probability density function $\rho: \reals^d \to [0,\infty)$.
%%and some random vector with probability density function $\rho: \reals^d \to [0,\infty)$.
%The analytical solution may be written as:
%$$\mu = \int_{\reals^d}f(\vx)\rho(\vx)\dif\vx \quad \text{   where } \vx \in \reals^d \text{ and } \vx \sim \rho(\vx).$$


\Section{Automatic algorithms}

\begin{figure}[htbp]
\centering
\begin{minipage}{7cm}\centering 
\includegraphics[width=7cm]{FoolingFunAutomaticAlg/chebintcolor-2015-12-06-18-08-01.eps} \\ {\tt chebfun}  \end{minipage}% requires the graphicx package
\begin{minipage}{7cm}\centering 
\includegraphics[width=7cm]{FoolingFunAutomaticAlg/integralcolor-2015-12-06-18-00-40.eps} \\{\tt integral}
\end{minipage}
\caption{Plots of the fooling functions $f$ with $\mu = \int_0^1f(x)\dif x=1$, for which the corresponding algorithms return values of $\hmu=0$. Since there are too many peaks for {\tt integral} between 0 and 1, we decide to plot from 0 to 0.01 to show the basic shape of the function.}\label{fig:foolingfun}
 \end{figure}
 
Automatic algorithms commonly determine the computational cost required to approximate the solutions that differ from the true solutions by no more than a given error tolerance, $\varepsilon$. In this way, one requires a user specified error tolerance $\varepsilon$ and a black box that generate the random variables or function values. Several commonly used software packages have adaptive, automatic algorithms for integrating functions of a single variable. Unfortunately, they do not provide any guarantees. On the other hand, most existing guaranteed automatic algorithms are not adaptive, they do not determine the computational cost needed based on random variables sampled. Some examples of the adaptive automatic algorithms are
\begin{itemize}
\item {\tt integral} \cite{Shampone08} in MATLAB, which uses the adaptive Gauss-Kronrod quadrature based on {\tt quadva};
\item {\tt chebfun} toolbox \cite{Chebfun14} for MATLAB, which approximates integrals by integrating interpolatory Chebyshev polynomial approximations to the integrands.
\end{itemize}
For these two automatic algorithms, one can easily probe where they sample the integrand, feed the algorithms with zero values and then construct fooling functions for which the automatic algorithms will return zero values for the integral. Figure \ref{fig:foolingfun} displays these fooling functions for problem $\int_0^1f(x)\dif x$ for these two algorithms. Each of these algorithms is given the error tolerance $10^{-14}$. Unfortunately, the true absolute error is 1. We are not criticizing that these algorithms can easily be fooled, instead, we have concerns that there is no theory to tell us when the algorithms are fooled or the problem with the integrand.


\Section{Outline of the thesis}

The outline of this thesis is as follows: Chapter \ref{basicInequalities} provides some basic definitions, inequalities and theorems that support the algorithms. Chapter \ref{chapter:meanMCabsg} describes two guaranteed algorithms that estimate the means of random variables and perform numerical integration with some absolute error tolerance. Two similar algorithms with some generalized error tolerance are described in Chapter \ref{chapter:meanMCg}. Chapter \ref{chapter:meanMCberg} explains the ways to estimate the means of Bernoulli random variables with some absolute or relative error tolerance with guarantees. An introduction to Guaranteed Automatic Integration Library (GAIL) is presented in Chapter \ref{chapter:gail}. This thesis ends with a conclusion and some thought about the future work.

\Chapter{Basic theorems and inequalities}\label{basicInequalities}

In Monte Carlo simulation, one wants to estimate the mean $\mu$ of a real valued random variable $Y$. Usually, this problem could be written as the expectation form $\mu:=\e(Y)$. Sometimes, the random variable $Y$ depends on some underlying random vector $\vX \in \reals^d$ with probability density function $\rho$, where $Y = f(\vX)$. In other situations, the random vector $\vX$ may have a discrete distribution or may have infinite dimension. In some practical situations, the process governing $Y$ may have a complex form, i.e. the probability of a bankruptcy or the fair price of an option. In these cases, we may be able to generate an IID sample of $Y$, but may not have a simple formula for computing $\rho$ analytically. 
In this chapter, we will present some theorems, inequalities and terminologies that may be used to develop our algorithms and prove our theorems in Chapters \ref{chapter:meanMCabsg},\ref{chapter:meanMCg} and \ref{chapter:meanMCberg}.

\Section{Moments}

Let $Y$ be a random variable. Given the sample size $n$, one generates $Y_1, Y_2, \cdots, Y_n$ samples and calculates the sample mean 
\begin{align}\label{samplemean}
\hmu_n = \frac1n\sum_{i=1}^n Y_i,
\end{align}
and the sample variance
\begin{align}\label{samplevar}
s^2_n = \frac{1}{n-1}\sum_{i=1}^n (Y_i-\hmu_n)^2.
\end{align}
The true mean of $Y$ may be written as the expectation form,
\begin{align}\label{truemean}
\mu = \e(Y).
\end{align}
The true variance is defined as
\begin{align}\label{truevar}
\sigma^2 = \e\left[(Y-\mu)^2\right],
\end{align}
and the standard deviation is $\sigma$. The skewness of $Y$ is given as 
\begin{align*}%\label{screwness}
\gamma = \frac{\e\left[(Y-\mu)^3\right]}{\sigma^3}.
\end{align*}
Also, define the third centered moment as
\begin{align}\label{def:M3}
M_3 = \frac{\e\left[\abs{Y-\mu}^3\right]}{\sigma^3},
\end{align}
and the modified kurtosis has the definition,
\begin{align}\label{kurtosis}
\kappa = \frac{\e\left[(Y-\mu)^4\right]}{\sigma^4}.
\end{align}
%Note that, sometime the kurtosis has the form $\tilde{\kappa} = \kappa-3$. In this thesis, we only use the definition of the modified kurtosis \eqref{kurtosis}.

Our main result of this thesis relies on an upper bound on the modified kurtosis, which also implies that the variance and skewness are both finite. See \cite{HJLO12}.

\Section{The Central Limit Theorem}

The Central Limit Theorem (CLT) describes how the distribution of $\hmu_n$ approaches a Gaussian distribution as $n \to \infty$.
\begin{theorem}[Central Limit Theorem {\cite[Theorem 21.1]{JP04}}] \label{clt} 
If $Y_1, \ldots, Y_n$ are IID with $\e(Y_i)=\mu$ and $\var(Y_i) = \sigma^2$, then
$$
\frac{\hmu_n-\mu}{\sigma/\sqrt{n}} \to \dnorm(0,1) \quad \text{in distribution, as} \ \ n\to\infty.
$$
\end{theorem}
This theorem implies an approximate confidence interval, called a CLT confidence interval, of the form
\[
\Pr(\abs{\hmu_{n}-\mu} \le \varepsilon) \approx 1- \alpha \qquad \]
for 
\begin{equation}\label{NCLT}
n= N_{\CLT}(\sigma/\varepsilon,\alpha) := \left \lceil z_{\alpha/2}^2 (\sigma/\varepsilon)^2 \right \rceil,
\end{equation}
where $z_{\alpha/2}$ is the $1-\alpha/2$ quantile of the standard Gaussian distribution.  When $\sigma^2$ is unknown, it may be replaced by the sample variance $s_n^2$ defined in \eqref{samplevar}.

\Section{Chebyshev's Inequality}

Chebyshev's Inequality may be used to construct a fixed-width confidence interval for $\mu$.  It makes relatively mild assumptions on the distribution of the random variable.

\begin{theorem}[Chebyshev's Inequality {\cite[6.1.c]{LB10}}] \label{ChebyThm}
If $X$ is a random variable, for any $\varepsilon>0$, 
$$\Pr(\abs{X-\e(X)} \ge \varepsilon) \le  \var(X)/\varepsilon^2.$$
\end{theorem}
Let $Y_1, \ldots, Y_n$ be IID. Choosing $X=\sum_{i=1}^n Y_i/n = \mu_n$, noting that $\e(X) = \e(Y) = \mu$, $\var(X) = \var(Y)/n = \sigma^2/n$, and setting $\var(X)/\varepsilon^2=\sigma^2/(n\varepsilon^2) = \alpha$ leads to the fixed-width confidence interval 
\[
\Pr(\abs{\hmu_{n}-\mu} \le \varepsilon) \ge 1- \alpha 
\]
 for 
 \begin{align}\label{NCheb}
n=N_{\Cheb}(\sigma/\varepsilon,\alpha):= \left \lceil \frac{1}{\alpha} (\sigma/\varepsilon)^2\right \rceil.
 \end{align}
 Chebyshev's Inequality ensures that the random variable $\hmu_n$ is not too far from its true value $\mu$ with a fixed width confidence interval. However, this inequality is too conservative and requires a significantly larger sample size than CLT. In some settings, we need a one sided inequality like Chebyshev's. We will appeal to Cantelli's Inequality.
 
\Section{Cantelli's Inequality}

\begin{theorem}[Cantelli's Inequality {\cite[6.1.e]{LB10}}]\label{CanThm} If $Y$ is a random variable with mean $\mu$, and variance $\sigma^2$ then for any $a \geq 0$, it follows that: 
\begin{align}
\Pr(Y-\mu \ge a) \le  \frac{\sigma^2}{a^2+\sigma^2}.
\end{align}
\end{theorem}
Cantelli's Inequality may be used to get a reliable upper bound on the variance $\sigma^2$ of a random variable $Y$ in Chapters \ref{chapter:meanMCabsg} and \ref{chapter:meanMCg} for Algorithm \ref{alg:meanMCabsg}, \ref{alg:cubMCabsg}, \ref{alg:meanMCg} and \ref{alg:cubMCg}.

\Section{Jensen's Inequality}

\begin{theorem}[Jensen's Inequality{\cite[8.4a]{LB10}}]\label{Jensen}
Let$X$ be a random variable and $g$ be a convex function on $\reals$. Suppose that the expectations of $X$ and $g(X)$ exist, then
$$g(\e(X)) \leq \e(g(X)).$$
Equality holds for strictly convex $g$ if and only if $X=\e(X)$ a.s.
\end{theorem}
Jensen's Inequality could be used to bound the third moment $M_3$ defined in \eqref{def:M3} of a random variable $Y$ by its fourth moment $\kappa$ from above. This following lemma shows the result.
\begin{lemma}\label{M3kappalemma}
Let $Y$ be a random variable with mean $\mu$, and variance $\sigma^2>0$, third centered moment $M_3 < \infty$ and modified kurtosis $\kappa < \infty$. Then the third moment $M_3$ may be bounded as 
\begin{align}\label{M3kappa}
M_3\leq \kappa^{3/4}.
\end{align}
\end{lemma}
\begin{proof}
By Jensen's Inequality in Theorem \ref{Jensen}, let the convex function be $g(y) = y^{4/3}$ and define the random variable $X = \abs{Y-\mu}^3$. Then we have:
$$g(\e(X)) = \left (\e\abs{Y-\mu}^3\right)^{4/3} \leq \e\left(\left(\abs{Y-\mu}^3\right)^{4/3}\right) = \e(g(X)).$$
By dividing both sides of the inequality by $\sigma^3$, the inequality yields:
$$\frac{\e\left(\abs{Y-\mu}^3\right)}{\sigma^3} \leq \left (\frac{\e\left(\abs{Y-\mu}^4\right)}{\sigma^4}\right)^{3/4}.$$
By appealing the definition of $M_3$ and $\kappa$, the proof is complete.
\end{proof}
This inequality may be used to get the sample size determined by the Berry-Esseen Inequality.

\Section{The Berry-Esseen Inequality}

\begin{theorem}[The Berry Esseen Inequality {\cite[Theorem 3]{HJLO12}}]\label{BEThm}
Let $Y_1,\cdots,Y_n$ be IID random variables with mean $\mu$, variance $\sigma^2 >0$, and third centered moment $M_3 < \infty$. Let $\hmu_n = (Y_1+\cdots+Y_n)/n$ denote the sample mean. Then
\begin{align}
&\abs{\Pr\left(\frac{\hmu-\mu}{\sigma/\sqrt{n}}<x\right)-\Phi(x)} \nonumber\\
& \leq \delta_n(x,M_3):=\frac{1}{n}\min\left( A_1(M_3+A_2),  A_3(M_3+A_4), A_5M_3, \frac{A_6M_3}{1+\abs{x}^3}\right) \label{deltaBEconstant}
\end{align}
where $\Phi$ is the standard Gaussian cumulative distribution function, $A_1 = 0.3322$, $A_2 = 0.429$, $A_3=0.3031$, $A_4=0.646$, $A_5=0.469$ \cite{She13} and $A_6=18.1139$ \cite{NeShe12}.
\end{theorem}
By applying Theorem \ref{BEThm} with given $M_3$, let $\hmu_n$ donate a sample mean of $n$ IID random instances of $Y$, then the Berry-Esseen Inequality implies that
\begin{align} 
&\Pr\left[\abs{\mu-\hmu_n}  \le \varepsilon \right]\nonumber\\
&=\Pr\left[\frac{\hmu_n - \mu}{\sigma/\sqrt{n}} \le \frac{\sqrt{n}\varepsilon}{\sigma} \right]-\Pr\left[\frac{\hmu_n - \mu}{\sigma/\sqrt{n}} < -\frac{\sqrt{n}\varepsilon}{\sigma}\right] \nonumber \\ 
&\ge \left[\Phi(\sqrt{n}\varepsilon/\sigma)-\delta_n(\sqrt{n}\varepsilon/\sigma,M_3)\right] -\left[\Phi(-\sqrt{n}\varepsilon/\sigma) + \delta_n(-\sqrt{n}\varepsilon/\sigma,M_3)\right] \nonumber \\
&=1-2[\Phi(-\sqrt{n}\varepsilon/\sigma) + \delta_n(\sqrt{n}\varepsilon/\sigma,M_3)] =: g(n, \sigma, M_3, \varepsilon), \label{BEresult}
\end{align}
since $\delta_n(-x,M_3)=\delta_n(x,M_3)$. The probability of
making an error no greater than $\varepsilon$ is bounded below by $1-\alpha$, where $\alpha$ is the uncertainty level, i.e., we want $1-2[\Phi(-\sqrt{n}\varepsilon/\sigma) + \delta_n(\sqrt{n}\varepsilon/\sigma,M_3)] =: g(n, \sigma, M_3, \varepsilon) \geq 1-\alpha$. The fixed width confidence interval \eqref{absCI} holds with $\hmu=\hmu_n$, provided $n \ge N_{\BE}(\sigma/\varepsilon,\alpha,M_3)$, where the Berry-Esseen sample size is
\begin{equation}\label{NBE}
N_{\BE}(\sigma/\varepsilon,\alpha,M_3) := \min  \left \{ n \in \naturals : \Phi\left(-\sqrt{n}/(\sigma /\varepsilon) \right)+\delta_n\left(\sqrt{n}/(\sigma /\varepsilon),M_3\right)
\le \frac{\alpha}{2} \right \}.
\end{equation}
It is interesting to note that the first term $\Phi\left(-\sqrt{n}/(\sigma /\varepsilon) \right)$ is the CLT part, while the second term $\delta_n\left(\sqrt{n}/(\sigma /\varepsilon),M_3\right)$ is the Berry-Esseen extra part. To compute $N_{\BE}(\sigma /\varepsilon,\alpha,M_3)$, we need to know $M_3$. In practice, substituting an upper bound on $M_3$ yields an upper bound on the necessary sample size.
As $M_3 \leq \kappa^{3/4}$ by Lemma \ref{M3kappalemma}, the sample size calculated by \eqref{NBE} is also $N_{\BE}\left(\sigma /\varepsilon,\alpha,\kappa^{3/4}\right) $.
%As the condition of the 
%Define the sample sized calculated by Chebyshev's inequality and Berry-Esseen inequality as:
%\begin{align*}
%N_{CB} (\varepsilon_a/\hsigma,\alpha_{\mu}, \kmax^{3/4}) = \min \left\{N_{\BE}, N_{\Cheb}\right\}
%\end{align*}

\Section{Hoeffding's Inequality}

As mentioned in the introduction, we investigate the way to estimate the mean of Bernoulli random variables, namely, the probability, $p$. In view of the particular form of the Bernoulli distribution, the inequalities that assume some bound on the random variable may be more feasible.  Here we use Hoeffding's Inequality \cite{H63}, which seems more suitable than the Chernoff bound \cite{Chernoff52},which is more conservative than Hoeffding's Inequality.  Below is a special case of Hoeffding's Inequality for random variables lying in $[0,1]$.
\begin{theorem}[Hoeffding's Inequality {\cite{H63}}] \label{hoeff}
If $Y_1$, $Y_2$, $\cdots$, $Y_n$ are IID observations such that $\e(Y_i)=p$ and $0 \leq Y_i \leq 1$, define $\hp_n=\sum_{i=1}^n Y_i/n$. Then, for any $\varepsilon>0$, 
\begin{equation}\label{ineq:HoeffonesideB}\
\Pr(\hp_n-p \geq \varepsilon) \leq e^{-2n\varepsilon^2},
\end{equation}
\begin{equation}\label{ineq:HoeffonesideA}
\Pr(p-\hp_n \geq \varepsilon) \leq e^{-2n\varepsilon^2},
\end{equation}
\begin{equation}\label{ineq:Hoefftwoside}
\Pr(\abs{\hp_n-p} \geq \varepsilon) \leq 2e^{-2n\varepsilon^2}.
\end{equation}
\end{theorem}
The reason we use $p$ represent the mean instead of $\mu$ is that the Hoeffding's Inequality is used to develop an algorithm that estimate a probability, we feel $p$ may be a better notation.
 
\Section{Fr\'{e}chet Formula}

\begin{theorem}[Fr\'{e}chet Formula] \cite{Frechet35}
Suppose $A_1, A_2,\cdots,A_n$ are events, the probability of a logical conjunction satisfies the following inequality:
\begin{multline}
 \max(0, P(A_1) + \cdots +P(A_n)-(n -1)) \leq P\left(\bigcap_{i=1}^n A_i\right) \leq \min(P(A_1),  \cdots, P(A_n))
\end{multline}
\end{theorem}
This inequality bounds the probability of joint events by the probability of the individual events without any dependence assumptions. It will be used to prove success of our algorithms in Chapter \ref{chapter:meanMCg} and \ref{chapter:meanMCberg}.

\Section{Additional definition and lemmas}

\begin{defn}[kurtosis maximum]
Given the sample size $n$, uncertainty level $\alpha$ and fudge factor $\fudge$, define the maximum allowable kurtosis as
\begin{equation}
\label{def:kmax}
\kmax(\alpha, n,\fudge):= \frac{n-3}{n-1} + \left(\frac{ \alpha n}{1-\alpha}\right) \left(1 - \frac{1}{\fudge^2}\right)^2.
\end{equation}
\end{defn}

\begin{defn}[generalized tolerance function]\label{def:tolfun}
 Define a generalized error tolerance function $\tol: [0, \infty) \times [0, \infty) \to [0,\infty)$. Let it be non-decreasing in each of its arguments and satisfy a Lipschitz condition in terms of its second argument, i.e.:
\begin{align*}
|\tol(a,b)-\tol(a,b')| \leq |b-b'| \quad \forall a,b,b' \geq 0.
\end{align*}
\end{defn}
Two examples that one may choose are
\begin{align*}
\tol(a,b) = \max (a,b),
\end{align*}
\begin{align*}
\tol(a,b) = (1- \theta) a + \theta b, \quad 0 \leq \theta \leq 1
\end{align*}

\begin{lemma}\cite[Lemma 1]{HJLO12} \label{samplevarbound}
Let $Y_1,\cdots,Y_n$ be IID random variables with variance $\sigma^2>0$ and modified kurtosis $\kappa$. Let $s_n^2$ be the sample variance defined in \eqref{samplevar}. Then the following inequalities hold: 
\begin{subequations}
\begin{gather}
\Pr\left( s_n^2 < \sigma^2 \left (1+\sqrt{\left (\kappa-\frac{n-3}{n-1}\right)\left (\frac{1-\alpha}{\alpha n}\right )}\right)\right) \geq 1-\alpha,\label{boundsamplevar1}\\ 
\Pr\left( s_n^2 >\sigma^2 \left (1-\sqrt{\left (\kappa-\frac{n-3}{n-1}\right)\left (\frac{1-\alpha}{\alpha n}\right )}\right)\right) \geq 1-\alpha.\label{boundsamplevar2}
\end{gather}
\end{subequations}

\end{lemma}
%this is the lemma to bound the \hsigma from above
\begin{lemma}\label{lowerboundhsigma}
Under same assumptions as in Lemma \ref{samplevarbound}. Given sample size $n_\sigma$, uncertainty $\alpha_\sigma$ and fudge factor $\fudge$, define an upper bound on the variance as $\hsigma^2 = \fudge^2 s_{n_{\sigma}}^2$. If the kurtosis of the random variable $Y$ satisfies the condition $$\kappa \leq \kmax =\kmax (n_\sigma,\alpha_\sigma, \fudge),$$ then, $$\Pr (\hsigma^2 > \sigma^2)\geq 1-\alpha_{\sigma}.$$
\end{lemma}
\begin{proof}
Starting with inequality \eqref{boundsamplevar2} in Lemma \ref{samplevarbound}, since the assumption is $\kappa \leq \kmax$, then we have the following inequality: 
\begin{align*}
&\Pr\left( s_{n_{\sigma}}^2 > \sigma^2 \left (1-\sqrt{\left (\kmax -\frac{n_{\sigma}-3}{n_{\sigma}-1}\right)\left (\frac{1-\alpha_{\sigma}}{\alpha n_{\sigma}}\right )}\right)\right)\\& \geq\Pr\left( s_{n_{\sigma}}^2 > \sigma^2 \left (1-\sqrt{\left (\kappa-\frac{n_{\sigma}-3}{n_{\sigma}-1}\right)\left (\frac{1-\alpha_{\sigma}}{\alpha_{\sigma} n_{\sigma}}\right )}\right)\right)\\&\geq1-\alpha_{\sigma} 
\end{align*}
 Plugging in $\kmax(n_\sigma,\alpha_\sigma, \fudge)$ defined in \eqref{def:kmax} yields:
 \begin{align*}
&Pr\left( s_{n_{\sigma}}^2 > \sigma^2 \left (1-\sqrt{\left (\kmax -\frac{n_{\sigma}-3}{n_{\sigma}-1}\right)\left (\frac{1-\alpha_{\sigma}}{\alpha n_{\sigma}}\right )}\right)\right)\\&
=  \Pr\left( s_{n_{\sigma}}^2 > \sigma^2 \left (1-\left(1-1/\fudge^2\right)\right) \right) \\&
=\Pr\left(\fudge^2s_{n_{\sigma}}^2  > \sigma^2 \right) \\&
=\Pr\left(\hsigma^2  > \sigma^2 \right) \\&
\geq  1-\alpha_{\sigma} 
\end{align*}
\end{proof}
This inequality explains that if the modified kurtosis $\kappa$ is bounded by $\kmax$, then the variance $\sigma^2$ will have a probabilistic upper bound $\hsigma^2$ with confidence level $1-\alpha_{\sigma}$.
\begin{lemma}\label{upperboundhsigma}
Under same assumptions as in Lemma \ref{lowerboundhsigma}, define a probabilistic upper bound on $\hsigma^2$ as:
\begin{align}\label{sigup}
\hsigma_{\up}^2 (\beta) := \left(\fudge^2+(\fudge^2-1)\sqrt{\frac{\alpha_{\sigma}(1-\beta)}{(1-\alpha_{\sigma})\beta}}\right)\sigma^2.
\end{align}
Then the following inequality must be true: 
\begin{align}\label{hsighsigupineq}
\Pr\left(\hsigma^2 <\hsigma_{\up}^2(\beta)\right) \geq 1-\beta.
\end{align}
\end{lemma}

\begin{proof}
Starting with inequality \eqref{boundsamplevar1} in Lemma \ref{samplevarbound}, multiply both sides of the inequality inside the probability by the fudge factor $\fudge$. Since we assume $\kappa \leq \kmax$,  replacing the modified kurtosis $\kappa$ by $\kmax$ yields:
\begin{align*}
1-\beta &\leq \Pr\left( s_n^2 < \sigma^2 \left (1+\sqrt{\left (\kappa-\frac{n_{\sigma}-3}{n_{\sigma}-1}\right)\left (\frac{1-\beta}{\beta n_{\sigma}}\right )}\right)\right)\\&
 = \Pr\left( \fudge^2 s_n^2 < \fudge^2 \sigma^2 \left (1+\sqrt{\left (\kappa-\frac{n_{\sigma}-3}{n_{\sigma}-1}\right)\left (\frac{1-\beta}{\beta n_{\sigma}}\right )}\right)\right)\\&
 \leq \Pr\left( \hsigma^2 < \fudge^2 \sigma^2 \left (1+\sqrt{\left (\kmax-\frac{n_{\sigma}-3}{n_{\sigma}-1}\right)\left (\frac{1-\beta}{\beta n_{\sigma}}\right )}\right)\right).
 \end{align*}
Plugging in the $\kmax$ defined in \eqref{def:kmax}, and $\hsigma_{\up}$ defined in \eqref{sigup} yields:
\begin{align*}
&1-\beta \\
&\leq \Pr\left( \hsigma^2 < \fudge^2 \sigma^2 \left (1+\sqrt{\left (\frac{n_{\sigma}-3}{n_{\sigma}-1} + \left(\frac{ \alpha n_{\sigma}}{1-\alpha}\right) \left(1 - \frac{1}{\fudge^2}\right)^2- \frac{n_{\sigma}-3}{n_{\sigma}-1}\right)\left (\frac{1-\beta}{\beta n_{\sigma}}\right )}\right)\right)\\
& = \Pr\left( \hsigma^2 < \sigma^2 \left(\fudge^2+(\fudge^2-1)\sqrt{\frac{\alpha(1-\beta)}{(1-\alpha)\beta}}\right)\right)\\
& =\Pr \left ( \hsigma^2< \hsigma_{\up}^2 \right)
\end{align*}
\end{proof}
The following two lemmas will be used to prove the stopping time in Algorithm \ref{alg:meanMCg} is finite.
\begin{lemma}[continuity of probability]\label{contiofprob}
Let $A_1, A_2, \cdots$ be a sequence of events increasing to $A$. Then
\begin{align}
\lim_{n \to \infty}\Pr(A_n) =\Pr(A).
\end{align}
\end{lemma}
\begin{proof}
Since $A_i \subseteq A_{i+1}$, then we know $A_i \cap A_{i+1} = A_i$. Therefore, define $C_{i+1} = A_{i+1} \cap (A_{i+1}\backslash A_i)$ for $i = 2,3,\cdots$, and let $C_1 = A_1$. Thus, we have
$$\cup_{i=1}^n C_i = A_n$$
$$\cup_{i=1}^\infty C_i=\cup_{i=1}^\infty A=A$$
As the probability has the property of countably additive, we conclude
$$\Pr (A) = \Pr (\cup_{i=1}^\infty C_i) = \sum_{i=1}^\infty \Pr(C_i) =  \lim_{n \to \infty}\sum_{i=1}^n\Pr(C_i)=\lim_{n\to \infty}\Pr(A_n).$$
\end{proof}

\begin{lemma}[preservation of inequality]\label{presofineq}
Suppose that $f, g: A \to \reals$ and c is a limit point of $A$. If
\begin{equation*}
f(x) \leq g(x) \text{ for all } x\in A,
\end{equation*}
and $\lim_{x\to c}f(x)$, $\lim_{x\to c} g(x)$ exist, then
$$\lim_{x\to c}f(x)\leq\lim_{x\to c} g(x)$$
\end{lemma}
\begin{proof}
Let
\begin{equation*}
\lim_{x \to c} f(x) =L, \lim_{x \to c}g(x) = M.
\end{equation*}
Prove by contradiction, suppose that $L >M$, and let
$$\varepsilon =\frac12(L-M) >0.$$
From the definition of the limit, there exsit $\delta_1, \delta_2>0$ such that
\begin{equation*}
\abs{f(x)-L}<\varepsilon \quad \quad \text{if } x \in A \text{ and } 0<\abs{x-c}<\delta_1,
\end{equation*}
\begin{equation*}
\abs{g(x)-M}<\varepsilon \quad \quad \text{if } x \in A \text{ and } 0<\abs{x-c}<\delta_2.
\end{equation*}
Let $\delta=\min(\delta_1,\delta_2)$. Since $c$ is a limit point of $A$, there exists $x \in A$ such that $0<\abs{x-a}<\delta$, and it follows that
\begin{align*}
f(x)-g(x) = (f(x)-L)+(M-g(x))+L-M >L-M-2\varepsilon>0,
\end{align*}
which contradicts the assumption, hence the proof proceeds.
\end{proof}
\Chapter{Guaranteed Monte Carlo method with an absolute error tolerance}\label{chapter:meanMCabsg}

In this chapter, we will present two guaranteed automatic adaptive algorithms: {\tt meanMCabs\_g}, which estimate the mean $\mu$ of a random variable $Y$; {\tt cubMCabs\_g}, which estimate the integral $I$ with the integrand $f$ with respect to some density function $\rho$ within the multivariate interval $(\va,\vb)$. In order to estimate the mean $\mu$ of a random variable $Y$, we feed {\tt meanMCabs\_g} with a random number generator $\Yrand$ and an absolute error tolerance $\varepsilon_a$. This algorithm automatically and adaptively determines the sample size needed in order to guarantee that the answer $\hmu_n$ satisfy the fixed width confidence interval condition: $\Pr (\abs{\mu-\hmu_n} \leq \varepsilon_a) \geq 1-\alpha$ if the random variable satisfies the bounded kurtosis condition $\kappa \leq \kmax$. Similar for the {\tt cubMCabs\_g}, which estimate the integral $I=\int_{\va}^{\vb} f(\vx)\rho(\vx)\dif\vx$, we feed this algorithm with the integrand $f$, integration interval $(\va,\vb)$, the probability density function $\rho$, and the absolute error tolerance $\varepsilon_a$. We have the guarantee that the answer $\hIn$ satisfies the fixed width confidence interval condition $\Pr \left(\abs{I-\hIn} \leq \varepsilon_a\right) \geq 1-\alpha$.
%In details, the algorithm {\tt meanMCabs\_g} works as follows: In the first stage, we sample $n_\sigma$ IID samples from the distribution of $Y$, from this sample we compute the sample variance, $s_{n_\sigma}^2$, according to \eqref{samplevar} and estimate the upper bound on variance $Y_i$ by $\hsigma^2 = \fudge^2s_{n_{\sigma}}^2$, where $\fudge^2>1$ is the fudge factor. For the second stage, we replace the true variance $\sigma^2$ by its upper bound $\hsigma^2$, then use the Berry-Esseen Theorem \ref{BEThm} to get the sample size needed for computing the sample mean $\hmu$. If the modified kurtosis $\kappa$ is less than $\kmax$, then we guarantee that the fixed width confidence interval condition \eqref{absCI} may be satisfied.

\Section{The Algorithm {\tt meanMCabs\_g}} \label{sec:meanMCabsg}

Before illustrating the algorithm, we first define some parameters: the uncertainty level $\alpha$, the sample size $n_{\sigma}$ and the fudge factor $\fudge$, with default values listed in Table \ref{meanMCabsgparam}.

Define the sample size calculated by CLT in Theorem \ref{clt} as:
\begin{equation}\label{NCLTmeanMCabsg}
N_{\CLT}(\sigma/\varepsilon_a,\alpha)
= 
\Bigl\lceil
z_{\alpha/2}^2(\sigma/\varepsilon_a)^2
\Bigr\rceil.
\end{equation}
Define the sample size calculated by Chebyshev's Inequality in Theorem \ref{ChebyThm} as:
\begin{equation}\label{NChebymeanMCabsg}
N_{\Cheb}(\sigma/\varepsilon_a,\alpha)
= 
\Bigl\lceil\frac{1}{\alpha}(\sigma/\varepsilon_a)^2\Bigr\rceil.
\end{equation}
By applying Theorem \ref{BEThm}, we get the sample size calculated by the Berry-Esseen Inequality as:
\begin{equation}\label{NBEmeanMCabsg}
N_{\BE}\left(\sigma/\varepsilon_a,\alpha,M_3\right) := \min \left \{ n \in \naturals : \Phi\left(-\sqrt{n}/(\sigma/\varepsilon_a)\right)+\delta_n\left(\sqrt{n}/(\sigma/\varepsilon_a),M_3\right)
\le \frac{\alpha}{2} \right \}.
\end{equation}
As our assumption for the bounded kurtosis is $\kappa \leq \kmax$, applying \eqref{M3kappa} yields 
\begin{align}\label{M3kmax}
M_3 \leq \kmax^{3/4}.
\end{align}
Thus, we define the sample size obtained by both Chebyshev's and the Berry-Esseen Inequality as:
\begin{align}\label{NCBmeanMCabsg}
N_{\CB} \left(\sigma/\varepsilon_a,\alpha, \kmax^{3/4}\right)  = \min \left \{ N_{\Cheb}\left(\sigma/\varepsilon_a,\alpha\right),N_{\BE}\left(\sigma/\varepsilon_a,\alpha, \kmax^{3/4}\right)\right \}.
\end{align}

 In details, the two-stage algorithm works as follows:

\begin{algorithm}[{\tt meanMCabs\_g}] \cite[Algorithm 1]{HJLO12}\label{alg:meanMCabsg}
This algorithm requires the user to provide a random number generator $\Yrand$ that takes the sample size $n$ as the input and generates $n$ random numbers from the distribution of $Y$ and the absolute error tolerance $\varepsilon_a$. 
\begin{table}[ht]
\caption{Parameters specified in Algorithm \ref{alg:meanMCabsg} and \ref{alg:cubMCabsg}.\label{meanMCabsgparam}}
\begin{tabular}{c|c|c}
      \hline
      \hline
      \text{parameters} & description & default value\\
      \hline 
      $\alpha$ &  the uncertainty level, $0<\alpha<1$ & $1\%$\\
      $\alpha_\sigma$ & the uncertainty level for variance estimation, $0<\alpha_\sigma<\alpha$ & $\alpha/2$ \\
       $n_{\sigma}$ &  the sample size for variance estimation, $n_\sigma \in \naturals$ & $10^4$\\
       $\fudge$ & standard deviation inflation factor (fudge factor), $\fudge>1$ & $1.2$\\
      \hline
    \end{tabular}
\end{table}
%\begin{itemize}
%\item the uncertainty level $\alpha \in (0,1)$,
%\item the uncertainty level for variance estimation $\alpha_\sigma \in (0,\alpha)$,
%\item the sample size used to estimate the variance, $n_{\sigma} \in \naturals$, $n_{\sigma} \geq 2$,
%\item the variance inflation factor $\fudge^2 >1$.
%%\item the kurtosis max $\kmax(n_\sigma,\alpha_\sigma,\fudge)$ as defined in xxxxx.
%\end{itemize}

With the parameters and their default values given in Table \ref{meanMCabsgparam}, we first calculate 
\begin{equation}	\label{kmaxinalg}
\kmax=\kappa_{\max}(n_\sigma,\alpha_\sigma,\fudge) =\frac{n_\sigma-3}{n_\sigma-1} + \left(\frac{ \alpha_\sigma n_\sigma}{1-\alpha_\sigma}\right) \left(1 - \frac{1}{\fudge^2}\right)^2.
\end{equation}
as defined in \eqref{def:kmax}, and then do the following:
\begin{enumerate}
%\renewcommand{\labelenumi}{\alph{enumi})}
\item Compute the sample variance, $s^2_{n_{\sigma}}$, using $n_\sigma$ IID random samples from the distribution of  $Y$. Let $\hsigma^2 = \fudge^2 s^2_{n_\sigma}$ be the upper bound on variance.
\item Compute the uncertainty for the second stage $\alpha_\mu = 1-(1-\alpha)/(1-\alpha_{\sigma})$ and the sample size for the mean estimation,
\begin{align}\label{def:meanmcabsgnmu}
n_\mu = N_{\CB} \left(\hsigma/\varepsilon_a,\alpha_\mu, \kmax^{3/4}\right).
\end{align}
Sample $n_\mu$ IID samples from the distribution of $Y$ that are independent of those used to estimate $\sigma^2$. Calculate the mean $\hmu$,
\begin{align}\label{def:meanmcabsghmu}
\hmu = \frac{1}{n_\mu}\sum_{i = n_\sigma+1}^{n_\sigma+n_\mu}Y_i.
\end{align}
Return $\hmu$ as the answer.
\end{enumerate}

\end{algorithm}


Note that, we usually fix the uncertainty level $\alpha$ as $1\%$, which means our answer will be true with probability at least $99\%$. Decreasing $\alpha$ will cause more sample required according to the formulas \eqref{NCLTmeanMCabsg}, \eqref{NChebymeanMCabsg} and \eqref{NBEmeanMCabsg}. As our algorithm {\tt meanMCabs\_g} is a two stage one, in each stage, we choose the uncertainty level as $\alpha_\sigma$ and $\alpha_\mu$ which satisfy the condition $(1-\alpha_\sigma)(1-\alpha_\mu)=1-\alpha$.

The sample size $n_\sigma$ is usually fixed as $10^4$, this is the sample size used to estimate the sample variance. As the total cost is defined as $n_\sigma +n_\mu$, increasing this sample size will cause more total sample used while we have a better estimation of the sample variance. 

The fudge factor $\fudge$ is usually set as $1.2$, which could be thought as enlarging the sample standard deviation by $20\%$ ( enlarging the sample variance by $44\%$). This factor ensures that the probability of underestimating the variance is low. 

These three parameters, $\alpha_\sigma$, $n_\sigma$ and $\fudge$, define $\kmax$, which is not a parameter to be prescribed, but a reflection of the robustness of one's Monte Carlo algorithm having chosen $\alpha_\sigma$, $n_\sigma$ and $\fudge$. Our suggestion is that, user should not skimp on $n_\sigma$, but choose $n_\sigma$ as large as needed to ensure that $\kmax$ is is large enough to bound the true kurtosis $\kappa$. The fudge factor $\fudge$ is usually chosen close to unity.

The success of the Algorithm \ref{alg:meanMCabsg} is proved in the following theorem.
\begin{theorem} \cite[Theorem 5]{HJLO12}\label{thm:meanMCabsg}
Let $Y$ be a random variable with mean $\mu$, variance $\sigma^2 >0$, and modified kurtosis $\kappa$. Let $\kmax(n_\sigma,\alpha_\sigma,\fudge)$ be as defined in \eqref{def:kmax}. For any random variable $Y$ with a bounded kurtosis $\kappa\leq\kmax(\alpha_{\sigma},n_{\sigma},\fudge)$, Algorithm \ref{alg:meanMCabsg} above yields an estimate $\hmu$ given by \eqref{def:meanmcabsghmu} that satisfies the fixed width confidence interval condition
\begin{align}
\Pr\left( \abs{\mu-\hmu} \leq \varepsilon_a \right) \geq 1-\alpha.
\end{align}
\end{theorem}
\begin{proof}
Let $\varepsilon_a$, $\alpha_\sigma$, $n_\sigma$, $\fudge$ be the parameters and $\hsigma^2$ be a random variable depending on the random samples $Y_i$, $i = 1,2,\cdots, n_{\sigma}$ as given in Algorithm \ref{alg:meanMCabsg}. By Lemma \ref{lowerboundhsigma}, if the modified kurtosis $\kappa \leq \kmax(\alpha_{\sigma},n_{\sigma},\fudge)$, then we can find a probabilistic upper bound on $\sigma^2$, which is:
$$\Pr \left(\hsigma^2 > \sigma^2\right)\geq 1-\alpha_{\sigma}.$$
Since $\hsigma$ is random, $n_\mu$ as defined in \eqref{def:meanmcabsgnmu} is also random. Thus, the randomness of $\hmu$ comes both from the sample size $n_\mu$ and the random variable $Y$. By adding the extra condition of the conservative variance estimation inside the probability, we have the inequality below:
\begin{align}
\Pr(\abs{\mu-\hmu} \leq \varepsilon_a) &\geq  \Pr({\abs{\mu-\hmu} \leq \varepsilon_a}, {\hsigma >\sigma}) \nonumber \\
&= \Pr(\abs{\mu-\hmu} \leq \varepsilon_a |\hsigma>\sigma)\Pr(\hsigma >\sigma) \nonumber \\
&\geq \Pr(\abs{\mu-\hmu} \leq \varepsilon_a |\hsigma > \sigma)(1-\alpha_\sigma)\label{proof:meanMCabsgstep1}.
\end{align}
As the sample $n_\mu$ used to calculate $\hmu$ is defined by Chebyshev's Inequality and the Berry-Esseen Inequality in \eqref{NCBmeanMCabsg}, under the assumption that $\kappa \leq \kmax$ and given $g$ as defined in \eqref{BEresult}, we have:
\begin{align}
&\Pr(\abs{\mu-\hmu} \leq \varepsilon_a|\hsigma>\sigma) \nonumber \\& \geq \Pr\left( \left({\abs{\mu-\frac{1}{n_\mu}\sum_{i=1}^{n_\mu} Y_i} \leq \varepsilon_a},{n_\mu= N_{\CB}\left(\hsigma/\varepsilon_a,\alpha_\mu, \kmax^{3/4}\right)}\right)\,\bigg\vert \,\hsigma>\sigma\right) \nonumber \\&\geq g\left(n_\mu,\hsigma, \kmax^{3/4}, \varepsilon_a\right) \nonumber  \\&\geq (1-\alpha_\mu) \label{proof:meanMCabsgstep2}.
\end{align}
Combining \eqref{proof:meanMCabsgstep1} and \eqref{proof:meanMCabsgstep2} completes the proof.
\end{proof}

\Section{An upper bound on cost of the Algorithm {\tt meanMCabs\_g}}\label{sec:meanMCabsgcost}

The cost of the Algorithm \ref{alg:meanMCabsg} is determined by two parts: the sample size $n_{\sigma}$ used to estimate the variance $\sigma$ in stage one, and the sample size $n_\mu$ used to estimate the mean $\mu$ in stage two. Thus, the total cost is 
$$n_{\tot} = n_\sigma+n_\mu.$$
Although the $n_\sigma$ is deterministic, $n_\mu$ is a random variable. The cost of Algorithm \ref{alg:meanMCabsg} may be defined probabilistically. Here we have a theorem that bounds the cost of the Algorithm \ref{alg:meanMCabsg} from above.

\begin{theorem}\label{thm:meanMCabsgcost}
The two stage Monte Carlo algorithm for fixed width confidence intervals based on IID sampling described in Algorithm \ref{alg:meanMCabsg} has a probabilistic cost bound $n_{\up}(\beta) =n_\sigma+\bar{n}_{\mu}(\beta)$, where $\bar{n}_{\mu}(\beta)=N_{\CB}\left(\hsigma_{\up}(\beta)/\varepsilon_a,\alpha_\mu, \kmax^{3/4}\right)$.
\begin{align}
\Pr \left(n_{\tot} \leq n_{\up}(\beta)\right)  \geq 1-\beta.
\end{align}
\end{theorem}

\begin{proof}
As the only random quantity in $n_\mu$ is $\hsigma^2$, we need to bound $\hsigma^2$ so as to bound $n_{\mu}$.
 By applying the definition of $\hsigma_{\up}^2(\beta)$ in Lemma \ref{upperboundhsigma}, we have:
\begin{align*}
\Pr(n_{\mu} \leq \bar{n}_{\mu}(\beta))  &= \Pr\left (N_{\CB}\left(\hsigma/\varepsilon_a,\alpha_\mu, \kmax^{3/4}\right) \leq N_{\CB}\left(\hsigma_{\up}(\beta)/\varepsilon_a,\alpha_\mu, \kmax^{3/4}\right) \right)\\&
 \geq \Pr \left(\hsigma^2 < \hsigma_{\up}^2(\beta) \right) \geq 1-\beta
\end{align*}
Adding the constant $n_\sigma$ inside the probability we have:
\begin{align*}
 \Pr(n_\sigma+n_{\mu} \leq n_\sigma+\bar{n}_{\mu}(\beta))   \geq 1-\beta.
\end{align*}
Hence the proof is complete.
\end{proof}
\Section{Numerical integration via Monte Carlo sampling}\label{sec:numericalintegrationviaMC}

An important special case of computing $I =\e(Y)$ is when $Y = f(\vx)$ for some function $f:\reals^d \to \reals$ and some random vector $\vx$ with probability density function $\rho: \reals^d \to [0,\infty)$. One may then interpret the mean of $Y$ as the multidimensional integral
\begin{align}\label{integralI}
I= \mu(f) = \e(Y) = \int_{\reals^d}f(\boldsymbol{x})\rho(\vx)\dif \vx.
\end{align}
Note that given the problem of evaluating $I = \int_{\reals^d}g(\vx)\dif\vx$, one must choose a probability density function $\rho$ for which one can easily generate random vectors $\vx$, then set $f = g/\rho$. The quantities $\sigma^2$ and $\kappa$ defined above can be written in terms of weighted $\mathcal{L}_p$ norms of $f$:
\begin{align*}
\norm[p]{f}:=\left\{ \int_{\reals^d}\abs{f(\vx)}^p\rho(\vx)\dif\vx\right\}^{1/p}, \quad \sigma^2 = \norm[2]{f-\mu}^2,\quad \kappa = \frac{\norm[4]{f-\mu}^4}{\norm[2]{f-\mu}^4}.
\end{align*}
For a given $g$, the choice of $\rho$ is not unique, and making an optimal choice belongs to the realm of the importance sampling. The assumption of bounded kurtosis in \eqref{def:boundedkurtosis} required by Algorithm \ref{alg:meanMCabsg} corresponds to an assumption that integrand $f$ lies in the cone of functions \cite{CDHHZ13}
\begin{align*}
C_{\kmax} = \left\{ f \in \mathcal{L}_4: \norm[4]{f-\mu(f)} \leq \kmax^{1/4} \norm[2]{f-\mu(f)}\right\}.
\end{align*}
This is in contrast to a ball of functions, which wound be the case if one was satisfying a bounded variance condition.
Below, we will present our Algorithm {\tt cubMCabs\_g} which estimates the integral \eqref{integralI}
as $\hat{I}$ in order to satisfy the fixed width confidence interval condition
\begin{align}\label{cubMCabsgCI}
\Pr \left(\abs{\hat{I}-I} \leq \varepsilon_a \right) \geq 1-\alpha.
\end{align}

\Section{The Algorithm {\tt cubMCabs\_g }}\label{sec:cubMCabsg}

\begin{algorithm}\label{alg:cubMCabsg} 
This algorithm requires the user to provide an integrand $f$, an integration interval $(\va,\vb)$, a measure $\rho$, and an absolute error tolerance $\varepsilon_a$. With the parameters with the default values specified in Table \ref{meanMCabsgparam}, we first calculate $\kmax=\kappa_{\max}(n_\sigma,\alpha_\sigma,\fudge)$ as defined in \eqref{def:kmax}, and then do the following:
\begin{enumerate}
%\renewcommand{\labelenumi}{\alph{enumi})}
\item In the first stage, sample $n_\sigma$ values of random vector $\vX_i$ from the distribution of $\vX$ with probability density function $\rho$ and obtain $n_\sigma$ function values of $f(\vX_i)$. Use this to calculate the sample mean $$\hat{I}_{n_\sigma} = \frac{1}{n_\sigma}\sum_{i=1}^{n_\sigma} f\left(\vX_i\right),$$ and sample variance $$s_{n_\sigma}^2 = \frac{1}{n_\sigma-1}\sum_{i=1}^{n_\sigma} \left(f(\vX_i)-\hat{I}_{n_\sigma}\right)^2.$$ Also approximate the upper bound on variance by $\hsigma^2 = \fudge^2 s^2_{n_\sigma}$. 

\item After the preparation of the upper bound on variance, compute the uncertainty level for the second stage $\alpha_\mu = 1-(1-\alpha)/(1-\alpha_{\sigma})$ and the sample size for the mean estimation,
\begin{align}
n_\mu = N_{\CB} \left(\hsigma/\varepsilon_a,\alpha_\mu, \kmax^{3/4}\right).
\end{align}
Sample $n_\mu$ values of random vector $\vX_i$ that are independent of those used to calculate $\hsigma^2$ from the distribution of $\vX$ with probability density function $\rho$, then calculate the $n_\mu$ function values of $f(\vX_i)$. Use this to calculate the sample mean 
\begin{align}\label{hmun}
\hat{I}= \frac{1}{n_\mu}\sum_{i = n_\sigma+1}^{n_\sigma+n_\mu}f(\vX_i).
\end{align}
\end{enumerate}
\end{algorithm}
As {\tt cubMCabs\_g} uses the same argument as {\tt meanMCabs\_g}, just treat the function values $f(\vX) $ as the random numbers $Y$ and estimate the mean. Hence, the proof of the success and the cost of the algorithm will be the same as in Theorem \ref{thm:meanMCabsg} and Theorem \ref{thm:meanMCabsgcost}. The only difference is that, it requires users to input the integrand $f$ and integral hyperbox $(\va,\vb)$ instead of the random number generator $\Yrand$, which may be easier for user who is just seeking the result of a integral or wants to compare different quadrature methods. In the next section, we will show some numerical examples using {\tt cubMCabs\_g} to solve numerical integration problems.

\Section{Numerical examples for {\tt cubMCabs\_g}}

In this section, we will show some numerical examples to integrate some test functions using different algorithms including {\tt cubMCabs\_g}, {\tt cubSobol\_g} \cite{HicJim16a},\\ {\tt cubLattice\_g} \cite{JimHic16a}, {\tt integral} \cite{Shampone08} and {\tt chebfun} \cite{Chebfun14}.

\Subsection{Integrating a product function}
%mat file used to plot   TestcubMCgFixedCovon-product-N500d2abstol0.001rel0-2015-11-24-16-01-55.mat
\begin{figure}
\centering
\begin{minipage}{9cm} \centering \includegraphics[width=9cm]
{fig_productfun/productiidErrTime-2015-11-24-16-11-57.eps} \\ {\tt cubMCabs\_g}  \end{minipage}
\begin{minipage}{7cm} \centering \includegraphics[width=7cm]
{fig_productfun/productcubSobolErrTime-2015-11-24-16-11-57.eps} \\ {\tt cubSobol\_g}  \end{minipage}
\begin{minipage}{7cm} \centering \includegraphics[width=7cm]
{fig_productfun/productcubLatticeErrTime-2015-11-24-16-11-57.eps} \\ {\tt cubLattice\_g}  \end{minipage}
\caption{Execution times and errors for the product function \eqref{productfun} with $d$ random chosen between 2 and 20, and the absolute error tolerance $\varepsilon_a=10^{-3}$. Those points to the left/right of the dashed vertical line represent successes/failures of the automatic algorithms.  The solid green curve shows that cumulative distribution of actual errors, and the dot-dashed curve shows the cumulative distribution of execution times. \label{fig:cubMCabsgproductfun} }
\end{figure}

A product function is used here to perform an integration problem $\mu=\int_{[0,1]^d} f(\vx) \, \dif \vx$, where the integrand is chosen as
\begin{equation}\label{productfun}
f(\vx) = \prod_{i=1}^d(x_i^2+a_i),
\end{equation}
where $\vx = (x_1,x_2,\cdots,x_d)$, $d$ is IID uniform on $[2,20]$, $a_i$ is  IID uniform in $[0,4/3]$, for $i=1,2,\cdots,d$. Accuracy and time are recorded in Figure \ref{fig:cubMCabsgproductfun} with 500 replications performed using algorithms {\tt cubMCabs\_g}, {\tt cubSobol\_g} \cite{HicJim16a} and {\tt cubLattice\_g} \cite{JimHic16a}. For all of these three algorithms, the absolute error tolerance is $10^{-3}$, the initial sample size is $2^{13}$, and the sample budget is $10^{10}$. With the uncertainty level $\alpha = 0.01$ and $\fudge = 1.2$, the kurtosis maximum is $\kmax = 4.84$ for {\tt cubMCabs\_g}.

In the plots for {\tt cubMCabs\_g}, an asterisk is used to label those points satisfying $\kappa \le \kmax$, where $\kappa$ is defined in \eqref{kurtosis}. All such points fall within the prescribed error tolerance,
which is even better than the guaranteed confidence of $99\%$.  Those points labeled with a dot, are those for which $\kappa > \kmax$, and so no guarantee holds. The points labeled with a diamond are those for which  {\tt cubMCabs\_g}  attempts to exceed the cost budget that we set, i.e., it wants to choose $n_\mu$ such that $n_{\sigma}+n_\mu > n_{\max}:=10^{10}$. In these cases $n_\mu$ is chosen as $\lfloor 10^{10} - n_\sigma \rfloor$, which often is still large enough to get an answer that satisfies the error tolerance. 
 
For {\tt cubSobol\_g} there are 444 out of 500 samples that reach the desired error tolerance, which is, the probability of success is $88.8\%$. Similarly, for {\tt cubLattice\_g}, there are 347 out of 500 samples that have the error less than the tolerance, i.e. the probability of success is $69.4\%$. Both of these quasi Monte Carlo methods have not reach the desired confidence level, $99\%$.

% These two quasi-Monte Carlo thods perform not as good as {\tt cubMCabs\_g} since this particular test function has a high effective dimension, which make the Fourier coefficient decays slow.
\Subsection{Integrating a single hump}\label{subsec:meanmcabssinglehump}

%using this file load TestcubMCon-gaussian-uniform-N500d1abstol0.001-2015-11-27-18-31-04.mat
\begin{figure}
\centering
\begin{minipage}{7cm} \centering \includegraphics[width=7cm]
{colorfigureofcubMCabsgusingGaussianTestFun/gaussianintegralErrTime-2015-11-28-01-26-58.eps} \\ {\tt integral}
 \end{minipage}
\begin{minipage}{7cm} \centering \includegraphics[width=7cm]
{colorfigureofcubMCabsgusingGaussianTestFun/gaussianchebfunErrTime-2015-11-28-01-26-59.eps} \\ {\tt chebfun} \end{minipage}
\begin{minipage}{7cm} \centering \includegraphics[width=7cm]{colorfigureofcubMCabsgusingGaussianTestFun/gaussiancubSobolErrTime-2015-11-28-01-26-57.eps} \\  {\tt cubSobol\_g}\end{minipage}
\begin{minipage}{7cm} \centering \includegraphics[width=7cm]{colorfigureofcubMCabsgusingGaussianTestFun/gaussiancubLatticeErrTime-2015-11-28-01-26-58.eps} \\ {\tt cubLattice\_g} \end{minipage}
\begin{minipage}{9cm} \centering \includegraphics[width=9cm]
{colorfigureofcubMCabsgusingGaussianTestFun/gaussianiidErrTime-2015-11-28-01-26-57.eps} \\ {\tt cubMCabs\_g}  \end{minipage}
\caption{Execution times and errors for the single hump test function \eqref{GaussianTestFun} for $d=1$ and absolute error tolerance $\varepsilon_a=10^{-3}$. \label{fig:GaussianTestFun} }
\end{figure}
Accuracy and timing results have been recorded for the integration problem $\mu=\int_{[0,1]^d} f(\vx) \, \dif \vx$ for a single hump test integrand
\begin{equation} \label{GaussianTestFun}
f(\vx)=a_0 + b_0\prod_{j=1}^d\left[ 1 +b_j \exp \left(-\frac{(x_j-h_j)^2}{c_j^2}\right) \right].
\end{equation}
Here $\vx$ is a $d$ dimensional vector, and $a_0, b_0, \ldots, b_d, c_1, \ldots, c_d, h_1, \ldots, h_d$ are parameters. Figure \ref{fig:GaussianTestFun} shows the results of different algorithms being used to integrate $500$ different instances of $f$.  For each instance of $f$, the parameters are chosen as follows:
\begin{itemize} 
\item $b_1, \ldots, b_d \in [0.1,10]$ with $\log(b_j)$ being IID\ uniform,
\item $c_1, \ldots, c_d \in [10^{-6},1]$ with $\log(c_j)$ being IID\ uniform,
\item $h_1, \ldots, h_d \in [0,1]$ with $h_j$ being IID\ uniform,
\item $b_0$ chosen in terms of the $b_1, \ldots, b_d, c_1, \ldots, c_d, h_1, \ldots, h_d$ to make $\sigma^2 =\norm[2]{f-\mu}^2 \in [10^{-2}, 10^2]$, with $\log(\sigma)$ being IID\ uniform for each instance, and
\item $a_0$ chosen in terms of the $b_0, \ldots, b_d, c_1, \ldots, c_d, h_1, \ldots, h_d$ to make $\mu=1$.
\end{itemize}
These ranges of parameters are chosen so that the algorithms being tested fail to meet the error tolerance a significant number of times.

%Those points to the left/right of the dashed vertical line represent successes/failures of the automatic algorithms.  The solid line shows that cumulative distribution of actual errors, and the dot-dashed line shows the cumulative distribution of execution times.  For the {\tt cubMCabs\_g} the points labeled * are those for which the Theorem \ref{thm:meanMCabsg} guarantees the error tolerance.
These $500$ random constructions of $f$ with $d=1$ are integrated using {\tt integral} \cite{Shampone08}, {\tt chebfun} \cite{Chebfun14}, {\tt cubSobol\_g}, {\tt cubLattice\_g}, and {\tt cubMCabs\_g}. For all but {\tt chebfun}, the specified absolute error tolerance is $\varepsilon_a=10^{-3}$. The algorithm {\tt chebfun} attempts to do all calculations to near machine precision but we still measure the success relative to absolute error tolerance $\varepsilon_a=10^{-3}$. The initial sample size is set as $2^{13}$ for all of the three Monte Carlo algorithms. {\tt cubMCabs\_g} takes $\alpha=0.01$ and $\fudge = 1.2$, which corresponds to $\kmax =4.84$.The observed error and execution times are plotted in Figure \ref{fig:GaussianTestFun}.  

Figure \ref{fig:GaussianTestFun} shows that {\tt integral} is quite fast, nearly always providing an answer in less than $0.01$ seconds.  Unfortunately, it successfully meets the error tolerance only about $55\%$ of the time, which is, only 275 out of 500 samples meet the error criterion. The performance of {\tt chebfun} is even worse than {\tt integral}, only 199 out of 500 samples meet the error tolerance, thus, the accuracy is $39.8\%$. For {\tt chebfun} plots, there are a significant proportion of the data that do not appear because their errors are smaller than $10^{-5}$.  The difficult cases are those where $c_1$ is quite small, and these algorithms miss the sharp peak.
%

The two quasi-Monte Carlo methods perform better than {\tt integral} and {\tt chebfun}. There are 402 out of 500 samples satisfy the error criterion for {\tt cubSobol\_g}, which provides a success rate of $80.4\%$. On the other hand, for {\tt cubLattice\_g}, 407 out of 500 samples attain desired accuracy, which gives a success rate $81.4\%$. 

In the plots for {\tt cubMCabs\_g}, an asterisk is used to label those points satisfying $\kappa \le \kmax$, where $\kappa$ is defined in \eqref{kurtosis}. All such points fall within the prescribed error tolerance, i.e. the accuracy is $100\%$, which is even better than the guaranteed confidence of $99\%$.  Those points labeled with a dot, are those for which $\kappa > \kmax$, and so no guarantee holds. For {\tt cubMCabs\_g} the accuracy is $82.4\%$.

 {\tt cubMCabs\_g} performs somewhat more robustly than all other algorithms, because it does not assume any smoothness of the integrand.  On the other hand, {\tt cubMCabs\_g} is generally much slower than the other algorithms. The more important point is that given the kurtosis of the random variable less than the kurtosis maximum, {\tt cubMCabs\_g} has a guarantee.
 
 % mat file TestcubMCon-gaussian-uniform-N500d5abstol0.001-2015-11-28-07-20-19.mat
\begin{figure}
\centering
\begin{minipage}{9cm} \centering \includegraphics[width=9cm]{colorfigureofcubMCabsgusingGaussianTestFun/gaussianiidErrTime-2015-11-29-16-31-35.eps} \\ {\tt cubMCabs\_g} \end{minipage}
\begin{minipage}{7cm} \centering \includegraphics[width=7cm]{colorfigureofcubMCabsgusingGaussianTestFun/gaussiancubSobolErrTime-2015-11-29-16-31-35.eps} \\ {\tt cubSobol\_g} \end{minipage}
\begin{minipage}{7cm} \centering \includegraphics[width=7cm]{colorfigureofcubMCabsgusingGaussianTestFun/gaussiancubLatticeErrTime-2015-11-29-16-31-36.eps} \\ {\tt cubLattice\_g} \end{minipage}
\caption{Execution times and errors for the single hump test function \eqref{GaussianTestFun} for $d=2, \ldots, 8$ and $\varepsilon_a=10^{-3}$, with the rest of the parameters as in Figure \ref{fig:GaussianTestFun}.\label{fig:GaussianTestFunHD}}
\end{figure}

Figure \ref{fig:GaussianTestFunHD} repeats the simulation shown in Figure \ref{fig:GaussianTestFun} for the same test function \eqref{GaussianTestFun}, but now with $d=2, \ldots, 8$ chosen randomly and uniformly.  For this case the univariate integration algorithms are inapplicable, but the multidimensional routines can be used. There are more cases where the {\tt cubMCabs\_g} tries to exceed the maximum sample size allowed, i.e., $(n_{\sigma}+n_\mu)d > N_{\max}:=10^{10}$, but the behavior seen for $d=1$ still generally applies.  


%\Subsection{Integrating the Keister Test function}\label{subsec:keitertestfunmeanMCg}
%
%Keister \cite{Keister96} considered the following multidimensional integral that has applications in physics:
%\begin{equation}\label{KeisterTestFun}
%\int_{R^d} \cos (\norm[2]{\vx})e^{-\norm[2]{\vx}^2} \dif \vx 
%\end{equation}
%This isotropic integral take the form
%\begin{align*}
% \int_{\reals^d} \cos (\norm[2]{\vx})e^{-\norm[2]{\vx}^2} \dif \vx &= 2^{-d/2}\int_{\reals^d} \cos(\norm[2]{\vy}/\sqrt{2}) e^{-\norm[2]{\vy}^2/2} \dif \vy \\
%&= \pi^{d/2} \int_{\reals^d} \cos(\norm[2]{\vy}/\sqrt{2}) \frac{e^{-\norm[2]{\vy}^2/2}}{(2\pi)^{d/2}} \dif \vy\\
% &= \pi^{s/2} \int_{[0,1)^d} \cos \left ( \sqrt{\sum_{j=1}^d \frac{[\Phi^{-1}(y_i)]^2}{2}}\right )\dif \vy,
%\end{align*}
%where $\Phi$ denotes the standard Gaussian distribution function defined as below:
%$$\Phi(x) = \frac{1}{\sqrt{2\pi}} \int_{-\infty}^x e^{-s^2/2}ds, x \in [-\infty, \infty].$$
%Let the integrand be 
%\begin{equation*}
%f(\vx) = \pi^{d/2} \cos \left( \sqrt{\frac{1}{2} \sum_{i=1}^d (\Phi^{-1} (x_i))^2}\right).
%\end{equation*}
%The integration problem can be written as
%\begin{equation*}
%\mu = \int_{[0,1]^d} f(\vx)\dif\vx.
%\end{equation*}
%By applying Monte Carlo methods, the integral can be estimated as follows:
%\begin{align}
%\hmu = \frac{\pi^{d/2}}{n}\sum_{i=1}^n \cos \left (\sqrt{\frac12 \sum_{j=1}^d (\Phi^{-1}(x_{i,j}))^2}\right),
%\end{align}
%where $\vx_i = (x_{i1},\cdots,x_{id}) \in \reals$ , $i=1,\cdots, n.$
%Keister \cite{Keister96} showed an exact formula for calculating the exact integral. By coding them into MATLAB, we compared the numerical results over three different algorithms with a specified absolute error tolerance.
%\begin{figure}
%\centering
%\begin{minipage}{9cm} \centering \includegraphics[width=9cm]{KeisteriidErrTime-2015-09-13-00-24-11.eps} \\ {\tt cubMCabs\_g} \end{minipage}
%\begin{minipage}{7cm} \centering \includegraphics[width=7cm]{KeistercubSobolErrTime-2015-09-13-00-24-12.eps} \\ {\tt cubSobol\_g} \end{minipage}
%\begin{minipage}{7cm} \centering \includegraphics[width=7cm]{KeistercubLatticeErrTime-2015-09-13-00-24-12.eps} \\ {\tt cubLattice\_g} \end{minipage}
%\caption{Execution times and errors for Keister test function \eqref{KeisterTestFun} for dimension $d$ random chosen from $1, 2, \ldots, 8$ and absolute error tolerance $\varepsilon_a=10^{-2}$. The points labeled as a diamond are those tend to exceed the budget.Those points to the left/right of the dashed vertical line represent successes/failures of the automatic algorithms.  The solid line shows that cumulative distribution of actual errors, and the dot-dashed line shows the cumulative distribution of execution times. \label{fig:keistertestfunabstol}}
%\end{figure}
%
%The 500 replications were done for numerical integration of the Keister test function \eqref{KeisterTestFun} with the dimension uniformly chosen between 1 and 8 over three guaranteed algorithms, i.e.: {\tt cubMCabs\_g}, {\tt cubSobol\_g} and {\tt cubLattice\_g}. The absolute error tolerance was set to be $10^{-2}$, the initial sample size is $2^{13}$ and the sample budget was set to be $10^{10}$.
%
%As we could see from Figure \ref{fig:keistertestfunabstol}, {\tt cubMCabs\_g} tends to use more time than both {\tt cubSobol\_g} and {\tt cubLattice\_g}. The one labeled with a diamond are those tends to exceed time budget, there are total 83 diamonds. The success rate for {\tt cubMCabs\_g} is $99.6\%$. If we excludes the 83 diamonds, the success rate is $100\%$ for {\tt cubMCabs\_g},which means all the errors are less than the given error tolerance $\varepsilon_a = 10^{-2}$. For {\tt cubSobol\_g} and {cubLattice\_g}, the success rates are $100\%$ and $88\%$ respectively.
%
%
%\Subsection{Asian geometric mean call option pricing}\label{subsec:asiancallopt}
%The next example involves pricing an Asian geometric mean call option.  Suppose that the price of a stock $S$ at time $t$ follows a geometric Brownian motion with constant interest rate, $r$, and constant volatility, $v$.  
%One may express the stock price in terms of the initial condition, $S(0)$, as 
%\[
%S(t)=S(0) \exp[(r-v^2/2)t + v B(t)], \qquad t \ge 0,
%\]
%where $B(\cdot)$ is a standard Brownian motion.  
%The discounted payoff of the Asian geometric mean call option with an expiry of $T$ years, a strike price of $K$, and assuming a discretization at $d$ times is 
%\begin{equation} \label{payoff}
%Y=\max\biggl([\sqrt{S(0)}S(T/d) S(2T/d)\cdots S(T(d-1)/d) \sqrt{S(T)}]^{1/d} - K,0 \biggr)\me^{-rT}.
%\end{equation}
%The fair price of this option is $\mu=\e(Y)$. One of our chief reasons for choosing this option for numerical experiments is that its price can be computed analytically, while the numerical computation is non-trivial.
%
%In our numerical experiments, the values of the Brownian motion at different times required for evaluating the stock price, $B(T/d), B(2T/d), \ldots,  B(T)$, are computed via a Brownian bridge construction.  This means that for one instance of the Brownian motion we first compute $B(T)$, then $B(T/2)$, etc., using independent Gaussian random variables $X_1, \ldots, X_d$, suitably scaled. 
%
%%The Brownian bridge accounts for more of the low frequency motion of the stock price by the $X_j$ with smaller $j$, which allows the Sobol' sampling algorithm to do a better job.  
%
%The option price, $\mu=\e(Y)$, is approximated by algorithm {\tt cubMCabs\_g}, {\tt cubSobol\_g} and {\tt cubLattice\_g} using an absolute error tolerance of $\varepsilon_a=0.05$, initial sample size $n_\sigma=1024$ and compared to the analytic value of $\mu$.  The result of $500$ replications is given in Figure \ref{fig:GeoMeanAsianOptionabstol}.  Some of the parameters are set to be fixed values, namely,
%\[
%S(0)=K=100, \qquad T=1, \qquad r=0.03.
%\]
%The volatility, $v$, is drawn uniformly between $0.1$ and $0.7$.  The number of time steps, $d$, is random chosen to be uniform over $\{1, 2, 4, 8, 16, 32\}$.  The true value of $\mu$ for these parameters is between about $2.8$ and $14$.
%
%\begin{figure}
%\centering
%\begin{minipage}{9cm} \centering \includegraphics[width=9cm]{geomeaniidErrTime-2015-09-13-15-39-00.eps} \\ {\tt cubMCabs\_g} \end{minipage}
%\begin{minipage}{7cm} \centering \includegraphics[width=7cm]{geomeancubSobolErrTime-2015-09-13-15-39-00.eps} \\ {\tt cubSobol\_g} \end{minipage}
%\begin{minipage}{7cm} \centering \includegraphics[width=7cm]{geomeancubLatticeErrTime-2015-09-13-15-39-01.eps} \\ {\tt cubLattice\_g} \end{minipage}
%\caption{Execution times and errors for the Asian geometric mean call option for $d=1, 2, 4, 8, 16, 32$ and $\varepsilon_a=0.05$.\label{fig:GeoMeanAsianOptionabstol}}
%\end{figure}
%
%For this example the true kurtosis of $Y$ is unknown.  
%All three algorithms compute the option price to the desired error tolerance with high reliability.  the IID sampling using {\tt cubMCabs\_g} costs more time than other two guaranteed algorithms.
%
%%and the ordinary Sobol' sampling algorithm it can be seen that some of the errors are barely under the error tolerance, meaning that the sample size is not chosen too conservatively.  For the heavy duty Sobol' algorithm, the high initial sample size seems to lead to smaller than expected errors and larger than necessary computation times.

\Chapter{Guaranteed Monte Carlo method with a generalized error tolerance} \label{chapter:meanMCg}

In this chapter, we will illustrate another two algorithms, {\tt meanMC\_g} and {\tt cubMC\_g}. They are used to estimate the mean $\mu$ of a random variable $Y$ and perform numerical integration under some generalized error tolerance criterion. This is a hybrid error criterion that combines the absolute error tolerance $\varepsilon_a$ and relative error tolerance $\varepsilon_r$. For example, we can take the tolerance $\max(\varepsilon_a, \varepsilon_r \abs{\mu})$. These two algorithms do not default to the previous two algorithms described in Chapter \ref{chapter:meanMCabsg} if the absolute error tolerance is quite small but nonzero. The algorithms introduced in this chapter will proceed iteratively until the stopping criterion is met. The details of the algorithms are described in Section \ref{sec:algmeanMCg} and Section \ref{sec:cubmcgalg}. The computational cost bound is derived in Section \ref{sec:meanmcgcost} and several numerical examples are given in Section \ref{sec:cubmcgnumericalexample}.

\Section{A generalized error criterion}\label{sec:generalerrorcriterion}

 Suppose $Y$ is a random variable, one wants to estimate its mean, i.e. $\mu=\e(Y)$. One way to do it is to sample $n$ values of $Y_i$, and let $\hmu=\sum_{i=1}^n Y_i/n$, when $n$ is large enough, $\hmu$ may be a good estimator of $\mu$. In some practical situations, one may seek to approximate the answer with 
%How large should $n$ be? one way to determine sample size $n$ is by Central Limit Theorem \ref{clt}. As CLT is an asymptotic results and is true only when $n \to \infty$, we need a probabilistic bound on error when $n$ is finite. In  Chapter \ref{chapter:meanMCabsg}, we proposed a two stage algorithm used to estimate the mean of a random variable to a specified absolute error tolerance with a high confidence level, i.e., we want our estimator $\hmu_n$ satisfies this fixed width confidence interval condition:
%\begin{align*}
%\Pr(\abs{\mu-\hmu_n} \leq \varepsilon_a) \geq 1-\alpha.
%\end{align*} 
%The procedure is done by bound the variance in the first step by applying Cantelli's inequality \eqref{CanThm} and bound the error of mean estimation in the second step by applying the Berry-Esseen inequality \eqref{BEThm}, as long as the modified kurtosis $\kappa$ is less than $\kmax$.
%
%In some practical situations, one may seek to approximate the answer with a certain relative accuracy, e.g. correct to three significant digits. Let $\hmu_n$ be the estimator of the true mean $\mu$, $\varepsilon_r$ stand for the relative error tolerance and $\varepsilon_a$ denote the absolute error tolerance. In this case, one may seek 
%$$\Pr(\abs{\mu-\hmu_n}\leq \varepsilon_r) \geq 1-\alpha.$$
%This is a global relative error criterion, rather than a point-wise relative error criterion.
%One may generalize the pure absolute and pure relative error criteria as follows:
%\begin{align}\label{hybridCI}
%\Pr(\abs{\mu-\hmu_n}\leq \tol(\varepsilon_a, \varepsilon_r\abs{\mu})) \geq 1-\alpha.
%\end{align}
%Here $\tol: [0, \infty) \times [0, \infty) \to [0,\infty)$ is non-decreasing in each of its arguments and satisfies a Lipschitz condition in terms of its second argument,
%\begin{align}
%|\text{tol}(a,b)-\text{tol}(a,b')| \leq |b-b'| \quad \forall a,b,b' \geq 0.
%\end{align}
%Two examples that one may choose are
%\begin{align}
%\text{tol}(a,b) = \text{max} (a,b),
%\end{align}
%\begin{align}
%\text{tol}(a,b) = (1- \theta) a + \theta b, \quad 0 \leq \theta \leq 1
%\end{align}
%Both of these examples include absolute error and relative error as special cases.
%Letting $\hat{\varepsilon}_n$ be the reliable upper bounds on $|\mu-\hat{\mu}_n|$, the aim is to take enough samples so that the generalized error criterion can be satisfied. but not too many.
a generalized error criterion of the form:
\begin{align}\abs{\mu -\tmu} \leq \tol \left(\varepsilon_a,\varepsilon_r \abs{\mu}\right),\label{pointwisegeneralizederror} 
\end{align}
where $\tol$ is defined in Definition \ref{def:tolfun}. Here $\tmu$ might not be the same as $\hmu$, but is defined in terms of $\hmu$. Suppose that one has a reliable upper bound on the error of a non-adaptive Monte Carlo estimate $\hmu$, which satisfies the following inequality:
\begin{align}
|\mu-\hmu| \leq \heps.  \label{pointwiseerrorbound} 
\end{align} We will have the proposition below.

\begin{proposition}\label{meanMCgProp}
Given an absolute error tolerance, $\varepsilon_a\geq 0$, a relative error tolerance, $0 \leq \varepsilon_r\le 1$, and some $\hmu$ and $\heps\geq 0$, define $\Delta_\pm$ by
\begin{align}\Delta_{\pm}(\hmu,\heps) = \frac{1}{2} [\tol(\varepsilon_a, \varepsilon_r |\hmu-\heps|) \pm \tol(\varepsilon_a, \varepsilon_r |\hmu+\heps|)] \label{deltadef}
\end{align}
and the approximation $\tmu$ by
\begin{align}
\tmu = \hmu + \Delta_{-}. \label{appxsol}
\end{align}
If error bound \eqref{pointwiseerrorbound} holds, and $\heps$ satisfies the following condition,
\begin{align}
\heps\leq \Delta_{+}(\hmu,\heps),\label{deltacondition}
\end{align}
 then the generalized error criterion for $\tmu$, \eqref{pointwisegeneralizederror}, is satisfied. 
 \end{proposition}
 \begin{proof}
 This proof follows by applying condition \eqref{pointwiseerrorbound} that bounds the absolute error in terms of $\heps$, upper bound condition \eqref{deltacondition} on $\heps$, the definition of the approximate solution \eqref{appxsol}, and definition \eqref{deltadef}:
\begin{align*}
\MoveEqLeft
|\tmu -\hmu - \Delta_{-}| = 0 \leq \Delta_{+}- \hat{\varepsilon}  \text{ by } \eqref{appxsol} \text{ and } \eqref{deltacondition} \\ 
\Leftrightarrow 
&\hmu+\Delta_- - \Delta_{+} + \hat{\varepsilon} \leq \tilde{\mu}  \leq \hat{\mu} +\Delta_{-}+\Delta_{+}-\hat{\varepsilon} \\ \Leftrightarrow 
&\hat{\mu} -\text{tol} (\varepsilon_a, \varepsilon_r |\hat{\mu} +\hat{\varepsilon}|)+ \hat{\varepsilon} \leq \tilde{\mu}  \leq \hat{\mu}  +\text{tol} (\varepsilon_a, \varepsilon_r |\hat{\mu} -\hat{\varepsilon} |)-\hat{\varepsilon}  \text{ by } \eqref{deltadef}\\
\Leftrightarrow 
&\hat{\mu} + \hat{\varepsilon}  -\text{tol} (\varepsilon_a, \varepsilon_r |\hat{\mu} +\hat{\varepsilon} |) \leq \tilde{\mu}  \leq \hat{\mu} -\hat{\varepsilon} +\text{tol} (\varepsilon_a, \varepsilon_r |\hat{\mu} -\hat{\varepsilon} |) \\
\Rightarrow &
\mu -\text{tol} (\varepsilon_a, \varepsilon_r |\mu|) \leq\hat{\mu} + \hat{\varepsilon}  -\text{tol} (\varepsilon_a, \varepsilon_r |\hat{\mu} +\hat{\varepsilon} |) \leq \tilde{\mu}  \leq \\
& \hat{\mu} -\hat{\varepsilon} +\text{tol} (\varepsilon_a, \varepsilon_r |\hat{\mu} -\hat{\varepsilon} |)  \leq \mu+\text{tol} (\varepsilon_a, \varepsilon_r |\mu|) \\
& \text{since } b \to b \pm \tol (\varepsilon_a,\varepsilon_r |b|) \text{ is nondecreasing defined in Definition \ref{def:tolfun}}\\ 
\Leftrightarrow &
\abs{\mu-\tmu} \leq \tol(\varepsilon_a, \varepsilon_r \abs{\mu}) 
\end{align*}
\end{proof}

If $\hmu >0$, $\Delta_{-}(\hmu,\heps)$ will be negative according to the definition of $\tol$ and $\Delta_{-}$. Similarly, if $\hmu <0$, $\Delta_{-}(\hmu,\heps)$ will be positive. Thus, $\tmu$ is a shrinkage estimator that is smaller than $\hat{\mu}$ in magnitude only if $\varepsilon_r > 0$. We give the unbiased estimator $\hmu$ a little shift towards 0 to make the new biased estimator $\tmu$ more easy to satisfy the generalized error criterion. This little shift, $\Delta_-$, will offset the scenario that we over estimate $\hmu$. Suppose our estimator $\hmu>\mu>0$, without the shift, we can only guarantee that $\abs{\mu-\hmu}\leq \tol(\varepsilon_a,\varepsilon_r\abs{\hmu})$. In order to make the error less than $\tol(\varepsilon_a,\varepsilon_r\abs{\mu})$, we give the estimator $\hmu$ a little shift towards 0, which will make the new estimator $\tmu$ satisfy the error condition.

Let's consider the pure relative error case, which is, $\varepsilon_a=0$. Then the $\tol$ and $\Delta$ functions become $\tol(\varepsilon_a,\varepsilon_r\abs{\mu}) = \varepsilon_r\abs{\mu}$, $\Delta_{\pm}(\hmu,\heps) = \varepsilon_r [ |\hmu-\heps| \pm |\hmu+\heps|]/2$. 
Suppose $\hmu>\heps>0$, the stopping condition will be $\hmu\varepsilon_r\geq \heps$. The estimator $\tmu$ is $\tmu = \hmu-\heps\varepsilon_r$.  As been proved in Proposition \ref{meanMCgProp}, if $\abs{\hmu-\mu}\leq \heps$ and $\hmu\varepsilon_r\geq \heps$, then the shrinkage estimator $\tmu$ will satisfy the condition $\abs{\tmu -\mu} \leq \varepsilon_r\abs{\mu}$. 

\Section{The Algorithm {\tt meanMC\_g}}\label{sec:algmeanMCg}

In this section, we introduce an algorithm that estimates the mean of a random variable to a specified generalized error tolerance $\tol(\varepsilon_a, \varepsilon_r\abs{\mu})$ defined in Definition \ref{def:tolfun}. Before presenting our algorithm, we first introduce some notations. 
For any fixed $n \in \reals$, $\alpha \in (0,1)$, and $M>0$, define the inverse of the functions $N_{\Cheb}(\cdot,\alpha)$, $N_{\BE}(\cdot,\alpha,M)$, and $N_{\CB}(\cdot,\alpha,M)$ defined in \eqref{NCheb}, \eqref{NBE} and \eqref{NCBmeanMCabsg}:
%\begin{subequations} \label{probadapterrcritBE}
\begin{gather}\label{NCinv*}
N_{\Cheb}^{-1}(n;\alpha) := \sqrt{n \alpha}, \\
\label{NBinv*}
N_{\BE}^{-1}(n;\alpha,M) := \min \left \{ b>0 : \Phi\left(-\sqrt{n}/b  \right)+\delta_n(\sqrt{n}/b,M)
\le \frac{\alpha}{2} \right \}, \\
\label{NCBinv}
N_{\CB}^{-1}(n;\alpha,M) := \min\left(N_{\Cheb}^{-1}(n,\alpha),N_{\BE}^{-1}(n,\alpha,M)\right),
\end{gather}
where $\Phi$ is the standard Gaussian cumulative distribution function and $\delta_n$ is defined in \eqref{deltaBEconstant}. It then follows by Chebyshev's Inequality and the Berry-Esseen Inequality that 
\begin{equation*}
\Pr[\abs{\hmu_n -\mu}<\hvareps] \geq 1-\alpha, \quad \text{provided } \kappa\leq\kmax, \text{ where }\hvareps=\sigma/N_{\CB}^{-1}\left(n,\alpha,\kappa_{\max}^{3/4}\right), 
\end{equation*} 
and $\sigma$ is the standard deviation of the random variable $Y$ defined in \eqref{truevar}.  

To prepare the algorithm, given the uncertainty $\alpha \in (0,1)$, let $\alpha_{\sigma}, \alpha_1,  \alpha_2, \ldots$ be an infinite sequence of positive numbers all less than one, such that 
\begin{equation} \label{alphaseq}
(1-\alpha_{\sigma})\left [1-(\alpha_1+\alpha_2+\cdots)\right] = 1-\alpha.
\end{equation}
For example, one might choose $\alpha_{\sigma}$ and 
\begin{equation} \label{alphaseqex}
\alpha_{t} = \frac{1-\alpha_\sigma}{\alpha-\alpha_\sigma} 2^{-t}, \ t\in \naturals, \quad \text{where} \  a \in (1,\infty).
\end{equation}
\begin{table}[ht]
\caption{Parameters specified in Algorithm \ref{alg:meanMCg} and \ref{alg:cubMCg} .\label{table:meanMCgparam}}
\begin{tabular}{c|c|c}
      \hline
      \hline
      \text{parameters} & description & default value\\
      \hline 
     $\alpha$ &  the uncertainty level, $1<\alpha<1$ & $1\%$\\
      $\alpha_\sigma$& the uncertainty level for variance estimation, $1<\alpha_\sigma<\alpha$ & $\alpha/2$\\
             $\alpha_t$ &  the uncertainty level satisfying  condition \eqref{alphaseq} & $\frac{1-\alpha_\sigma}{\alpha-\alpha_\sigma} 2^{-t}$ \\
       $n_{\sigma}$ &  the sample size for variance estimation, $n_\sigma\in \naturals$ & $10^4$\\
       $n_1$ & initial sample size for mean estimation, $n_1\in \naturals$ & $10^4$\\
       $\fudge$ & standard deviation inflation factor (fudge factor), $\fudge>1$ & $1.2$\\
       $\theta$ & parameter used to choose next tolerance, $0<\theta<1$ & $0.95$\\
       $\tol$ & a generalized error criterion in Definition \ref{def:tolfun} & $\max$ \\
      \hline
    \end{tabular}
\end{table}

\begin{algorithm}\label{alg:meanMCg} 
This algorithm requires an user to provide several inputs:
\begin{itemize}
\item a random number generator $\Yrand$ that takes the sample size $n$ as an input. It generates $n$ IID random numbers from the distribution of $Y$,
\item the absolute error tolerance $\varepsilon_a \geq 0$,
\item the relative error tolerance $0 \leq \varepsilon_r <1$ ( we require $\varepsilon_a+\varepsilon_r >0$),
%\item a generalized error tolerance function $\tol(\varepsilon_a,\varepsilon_r\abs{\mu})$.
\end{itemize}
With the parameters given in Table \ref{table:meanMCgparam}, we first calculate $\kmax=\kappa_{\max}(n_\sigma,\alpha_\sigma,\fudge)$ as defined in \eqref{def:kmax}, then do the following:
\begin{enumerate}
\item Compute the sample variance, $s^2_{n_{\sigma}}$, using $n_\sigma$ IID samples from the distribution of $Y$. Then approximate the variance of $Y$ by $\hsigma^2 = \fudge^2 s^2_{n_\sigma}$. 
\item If the relative error tolerance $\varepsilon_r=0$, the algorithm becomes a two stage one instead of multiple stage one. Then calculate the uncertainty level for the second stage estimation, $\alpha_1 = 1-(1-\alpha)/(1-\alpha_\sigma)$, and the sample size for mean estimation,
$$n_1 = N_{\CB}\left(\hsigma/\varepsilon_a,\alpha_1,\kappa_{\max}^{3/4}\right).$$ 
Sample $n$ IID random numbers from distribution of $Y$ and compute $$\tmu =\hmu_1 = \frac1n\sum_{i=1}^nY_i.$$
Terminate the algorithm.
\item Else, calculate the width of the initial confidence interval for the mean estimation,
\begin{align}\label{eps1def}
\heps_1=\hsigma /N_{\CB}^{-1}\left(n_1,\alpha_1,\kappa_{\max}^{3/4}\right).
\end{align}
For $t = 1,2,\cdots$, do the following:
\begin{enumerate}
\item \label{deltamu}Compute $\hmu_t$ and $\Delta_{+}(\hmu_t,\heps_t)$ using sample size $n_t$ and tolerance $\heps_t$, where $\Delta_{+}$ is defined in \eqref{deltadef}.
\item If $\Delta_{+}(\hmu_t,\heps_t) \geq  \heps_t$, then $\Delta_{+}$ is large enough. Set stopping time $\tau = t$ and Let $\tmu = \hmu_{\tau}+\Delta_{-}(\hmu_\tau,\heps_\tau)$. Terminate the algorithm.
\item Else, define the next tolerance, $$\heps_{t+1} = \max \left ( \heps_{t-1}/10,\min\left(\heps_t/2, \max(\varepsilon_a,\theta\varepsilon_r\abs{\hmu_t})\right)\right)$$ and the next sample size, $$n_{t+1} = N_{\CB}\left(\hsigma/\hvareps_{t+1},\alpha_{t+1},\kappa_{\max}^{3/4}\right).$$ Increase $t$ by one and go back to step \eqref{deltamu}. 
\end{enumerate}
\end{enumerate}
\end{algorithm}
The success of Algorithm \ref{alg:meanMCg} has been proved in several steps stated below
\begin{itemize}
\item Algorithm \ref{alg:meanMCg} terminates in a finite step almost surely is proved in Lemma \ref{taufinite}.
\item The generalized error criterion, \eqref{hybridCI}, is satisfied when the algorithm terminates is proved in Theorem \ref{thm:meanMCg}.
\item The cost upper bound of Algorithm \ref{alg:meanMCg} is derived in Theorem \ref{cost:meanMCg}.
\end{itemize}
Before proving the success of the algorithm, we need the following lemmas.

\begin{lemma}\label{cost1}
If the function $\tol: [0, \infty) \times [0, \infty) \to [0,\infty)$ is non-decreasing in each of its arguments and satisfies a Lipschitz condition in terms of its second argument, i.e.:
\begin{align}
|\tol (a,b)-\tol (a,b')| \leq |b-b'| \quad \forall a,b,b' \geq 0.
\end{align}
Then, for $\varepsilon_a \geq 0$, $\varepsilon_r\geq 0$ and $\varepsilon' \geq 0$, the following inequality must be true:
\begin{equation}\label{lipstol}
\tol (\varepsilon_a, \varepsilon_r \abs{\hmu \pm\heps }) \geq \tol(\varepsilon_a,\varepsilon_r \abs{\mu})-\varepsilon_r (\abs{\mu-\hmu} + \heps).
\end{equation}
\end{lemma}
\begin{proof}
By triangle inequality and Lipschitz condition of $\tol$, we have the inequality below
 $$\tol(a,b) -\tol(a,b') \leq \abs{\tol(a,b) -\tol(a,b')} \leq \abs{b-b'},$$
which is equivalent to:
\begin{align}\label{tolineq}
\tol(a,b') \geq \tol(a,b)-\abs{b-b'}.
\end{align}
Then we choose $a = \varepsilon_a$, $b =\varepsilon_r\abs{\mu}$ and $b'=\varepsilon_r\abs{\heps \pm \hmu}$. By applying inequality \eqref{tolineq} and the triangle inequality, the following inequality must be true:
\begin{align*}
\tol (\varepsilon_a, \varepsilon_r \abs{\hmu \pm\heps}) &
 \geq \tol(\varepsilon_a,\varepsilon_r \abs{\mu})-\varepsilon_r \abs{\abs{\mu}-\abs{\hmu  \pm \heps}} \\&\geq \tol(\varepsilon_a,\varepsilon_r \abs{\mu})-\varepsilon_r \abs{-\mu+\hmu \pm \heps} \\&
 \geq \tol(\varepsilon_a,\varepsilon_r \abs{\mu})-\varepsilon_r (\abs{\mu-\hmu} + \heps).
\end{align*}
\end{proof}

\begin{lemma}\label{cost2}
Given an absolute error tolerance $\varepsilon_a \geq 0$, a relative error tolerance $\varepsilon_r \geq 0$ and an error tolerance $\heps$. If $\heps$ satisfies this condition: $$ \heps \leq \frac{\tol(\varepsilon_a, \varepsilon_r\abs{\mu})}{1+\varepsilon_r}, $$ and condition \eqref{pointwiseerrorbound}
%\begin{equation}\label{hmuboundheps}
%\abs{\mu -\hmu}\leq \heps.
%\end{equation}
Then the inequality $$\heps \leq \Delta_{+}(\hmu,\heps)$$ must be satisfied.
\end{lemma}
\begin{proof}
Since we assume $\abs{\mu -\hmu}\leq \heps$, it follows by the triangle inequality that,
 $$\abs{\mu} \leq\abs{\hmu}+\heps.$$
Since $\tol$ is non-decreasing on its second argument, we have the following inequality
\begin{align}\label{tolineq1}
\tol(\varepsilon_a, \varepsilon_r (\abs{\hmu}+\heps))  \geq \tol(\varepsilon_a, \varepsilon_r \abs{\mu}).
\end{align}
Next, applying \eqref{lipstol} and \eqref{pointwiseerrorbound} yields:
\begin{align}
\tol\left(\varepsilon_a, \varepsilon_r \abs{\abs{\hmu}-\heps}\right)  
 &\geq \tol\left(\varepsilon_a, \varepsilon_r \abs{\mu}\right) - \varepsilon_r\left(\abs{\mu-\hmu}+\heps\right) \nonumber\\&
 \geq  \tol\left(\varepsilon_a, \varepsilon_r \abs{\mu}\right) - 2\varepsilon_r\heps. \label{tolineq2}
\end{align}
By the definition of $\Delta_{+}$ in \eqref{deltadef} and the inequality \eqref{tolineq1} and \eqref{tolineq2},
 \begin{align}
\Delta_{+}(\hmu, \heps) &= \frac{1}{2} \left ( \tol\left(\varepsilon_a, \varepsilon_r \left(\abs{\hmu}+\heps\right)\right) +\tol\left(\varepsilon_a, \varepsilon_r \abs{\abs{\hmu}-\heps}\right) \right) \nonumber\\&
\geq \frac{1}{2} \left( \tol\left(\varepsilon_a,\varepsilon_r \abs{\mu}\right) + \tol\left(\varepsilon_a, \varepsilon_r \abs{\mu}\right) - 2\varepsilon_r\heps \right)\nonumber\\& =  \tol\left(\varepsilon_a, \varepsilon_r \abs{\mu}\right) - \varepsilon_r\heps.\label{deltatolrelationship}
\end{align}
If we subtract $\heps$ from both side of the inequality \eqref{deltatolrelationship}, it becomes:
$$\Delta_{+}(\hmu, \heps) -\heps \geq \tol(\varepsilon_a,\varepsilon_r \abs{\mu})-(1+\varepsilon_r)\heps \geq 0.$$
By the hypothesis of the theorem, this completes the proof.
\end{proof}
With the preparation of Lemma \ref{cost1} and Lemma \ref{cost2}, we are going to prove the stopping time $\tau$ in Algorithm \ref{alg:meanMCg} has an upper bound, hence, it is finite almost surely.
% this is the lemma to bound the stopping time
\begin{lemma}\label{tauprobbound}
Given $\varepsilon_a$, $\varepsilon_r$, $n_1$, $\alpha_1$, and $\kmax(n_\sigma, \alpha_\sigma, \fudge)$ defined in \eqref{kmaxinalg}, which are parameters given in Algorithm \ref{alg:meanMCg}. Let $\beta$ be an uncertainty level and define the upper bound on tolerance $\heps_1$ as:
\begin{align}\label{eps1updef}
\hvareps_{1\up}(\beta)&:=\hsigma_{\up}(\beta)/N_{\CB}^{-1}\left(n_1,\alpha_1,\kappa_{\max}^{3/4}\right),
\end{align}
where $\hsigma_{\up}(\beta)$ is defined in \eqref{sigup}. The stopping time $\tau$ for Algorithm \ref{alg:meanMCg}, has a probabilistic upper bound
\begin{align}
\bar{\tau}(\beta)&:= \left \lceil 1+\log_2 \left(\frac{\heps_{1\up}(\beta)(1+\varepsilon_r)}{ \tol(\varepsilon_a,\varepsilon_r \abs{\mu})}\right) \right \rceil \label{taubardef} \\
&=1+\left \lceil \log_2 \left(\frac{\sqrt{\left(\fudge^2+(\fudge^2-1)\sqrt{\frac{\alpha_{\sigma}(1-\beta)}{(1-\alpha_{\sigma})\beta}}\right)}\sigma(1+\varepsilon_r)}{ N_{\CB}^{-1}\left(n_1,\alpha_1,\kappa_{\max}^{3/4}\right)\tol\left(\varepsilon_a,\varepsilon_r \abs{\mu}\right)}\right)\right \rceil \label{taubarexplicitdef}
\end{align}
with confidence level $1-\beta$, i.e.:
\begin{align}\label{ineq:taufinite}
\Pr(\tau <\bar{\tau}(\beta)) \geq 1-\beta.
\end{align}
\end{lemma}
\begin{proof}
The stopping time $\tau$ is defined as:
 \begin{align}\label{st}
\tau = \min \{ t: \Delta_{+}(\hmu_t, \heps_t) \geq \heps_t \}
\end{align}
 and the tolerance in each step of iteration is defined as:
 $$\heps_{t+1} = \min(\heps_t/2, \tol(\varepsilon_a,\theta\varepsilon_r\abs{\hmu_t})).$$ 
The new tolerance must be no greater than half of the previous tolerance, i.e. $\heps_{t+1} \leq\heps_t/2$, which yields: 
\begin{align}\label{eps1niRelation}
\heps_t\le\heps_1/2^{t-1}.
\end{align} 
If the tolerance $\varepsilon_t$ satisfies the upper bound in Lemma \ref{cost2}, which is, $$\varepsilon_t \leq \frac{\tol(\varepsilon_a, \varepsilon_r\abs{\mu})}{1+\varepsilon_r},$$ then it must also satisfy
$$\varepsilon_t \leq \Delta_{+}(\hmu_t, \heps_t).$$ 
Define an upper bound on stopping time $\tau$ in terms of the random $\heps_1$ as follows
$$T := 1+\left \lceil \log_{2} \left(\frac{\heps_{1}(1+\varepsilon_r)}{ \tol(\varepsilon_a,\varepsilon_r \abs{\mu})}\right) \right \rceil. $$
This definition of $T$ then implies that:
\begin{align*}
& T \geq 1+\log_{2} \left(\frac{\heps_{1}(1+\varepsilon_r)}{ \tol(\varepsilon_a,\varepsilon_r \abs{\mu})}\right)\\&
\Leftrightarrow  \frac{\tol(\varepsilon_a, \varepsilon_r\abs{\mu})}{1+\varepsilon_r} \ge \heps_1/2^{T-1}\\
& \Rightarrow \frac{\tol(\varepsilon_a, \varepsilon_r\abs{\mu})}{1+\varepsilon_r} \ge \heps_\tau  \text{ \quad by inequality \eqref{eps1niRelation}}\\&
\Rightarrow \Delta_{+,T} \ge \heps_{T} \text{\quad by Lemma \ref{cost2}}.
\end{align*}
Since the first occurrence $\heps_t \leq \Delta_+(\heps_t,\heps_t)$ defines the stopping time $\tau$, we must have
\begin{align}\label{taurel}
\tau \le T= 1+\left \lceil \log_{2} \left(\frac{\heps_{1}(1+\varepsilon_r)}{ \tol(\varepsilon_a,\varepsilon_r \abs{\mu})}\right) \right \rceil.
\end{align}
Then by \eqref{taurel} and \eqref{taubardef}, we have the relationship below:
\begin{align} \label{epstau}
 \heps_1 <\heps_{1\up}(\beta) \Rightarrow \tau \le T<\bar{\tau}(\beta).
\end{align}
Also, according to the definition of $\heps_1$ in \eqref{eps1def} and $\heps_{1\up}(\beta)$ in \eqref{eps1updef}, which are functions of $\hsigma$ and $\hsigma_{\up}(\beta)$ respectively, we have the relationship below
\begin{align}\label{sigmaeps}
 \hsigma < \hsigma_{\up}(\beta) \Rightarrow \heps_1 < \heps_{1\up}(\beta).
\end{align}
Combining \eqref{epstau} and \eqref{sigmaeps} yields
$$\hsigma < \hsigma_{\up}(\beta) \Rightarrow\tau <\bar{\tau}(\beta).$$
Apply Lemma \ref{upperboundhsigma}, we have the probability relationship as below: 
\begin{align*}
\Pr(\tau < \bar{\tau}(\beta)) \geq  \Pr(\hsigma < \hsigma_{\up}(\beta)) \geq 1-\beta.
\end{align*}
\end{proof}
Equation \ref{taubarexplicitdef} gives an explicit form of the upper bound on the stopping time $\tau$, i.e. $\bar{\tau}(\beta)$. As we could see, when the tolerance $\tol$ becomes small, the stopping time becomes large, which means we need more iteration steps. When $\beta$ becomes small, the stopping time becomes large, and $\lim_{\beta \to 0}\bar{\tau}(\beta) = \infty$. Also, the stopping time is a function of the true standard deviation $\sigma$, when $\sigma$ becomes large, we need more iteration steps. Next, we will prove that the stopping time is finite almost surely.
\begin{lemma}\label{taufinite}
The stopping time $\tau$ is finite almost surely, i.e.:
$$\Pr(\tau < \infty) = 1.$$
\end{lemma}
\begin{proof}
By Lemma \ref{tauprobbound}, the stopping time $\tau$ has a probabilistic upper bound $\bar{\tau}(\beta)$ defined in \eqref{ineq:taufinite}. As $\bar{\tau}(\beta)$ is a function in terms of $\beta$, taking the limit that $\beta \to 0$ yields $\lim_{\beta \to 0} \bar{\tau}(\beta) = \infty$, which is
\begin{align}\label{betagoesto0}
\Pr(\tau < \infty) = \Pr(\tau  <\lim_{\beta \to 0}\bar{\tau}(\beta)).
\end{align}
By the continuity of probability in Lemma \ref{contiofprob}, 
\begin{align}\label{contofprob}
 \Pr(\tau  <\lim_{\beta \to 0}\bar{\tau}(\beta)) =\lim_{\beta \to 0} \Pr(\tau  <\bar{\tau}(\beta)),
\end{align}
and preservation of inequality in Lemma \ref{presofineq}, we have
\begin{align}\label{preofineq}
\lim_{\beta \to 0} \Pr(\tau  <\bar{\tau}(\beta)) 
\geq \lim_{\beta \to 0}(1-\beta) = 1.
\end{align}
By inequality \eqref{betagoesto0}, \eqref{contofprob} and \eqref{preofineq}, the proof proceeds.
\end{proof}

\begin{lemma}\label{thm:fixedwidthrandomeps}
Let $Y$ be a random variable with mean $\mu$, and either zero variance or positive variance with modified kurtosis $\kappa \leq \kmax(\alpha_\sigma, n_\sigma, \fudge)$. Suppose that the width of the confidence interval, $\heps$, is a random number. It follows that Algorithm \ref{alg:meanMCabsg} above yields an estimate $\hmu$ given by \eqref{def:meanmcabsghmu} that satisfies the following condition
\begin{align}\label{eq:fixedwidthrandomeps}
\Pr\left( \abs{\mu-\hmu} > \heps |\hsigma \geq \sigma \right) <\alpha_\mu.
\end{align}
Here, the samples of $Y$ used to calculate $\hmu$ are independent of $\heps$, although the number of samples, $n_\mu$, depends on $\heps$.
\end{lemma}
\begin{proof}
This result could be derived from the proof of the Theorem \ref{thm:meanMCabsg}, by applying the conditional expectation on $\heps$.
Let $\alpha_\sigma$, $n_\sigma$, $\fudge$ be the parameters and $\hsigma^2$ be a random variable depending on the random samples $Y_i$, $i = 1,2,\cdots, n_{\sigma}$ as given in Algorithm \ref{alg:meanMCabsg}. Also, let the tolerance $\heps$ be a random variable depending on $\hsigma^2$. 
The probability \eqref{eq:fixedwidthrandomeps} can be written as:
\begin{align}
&\Pr(\abs{\mu-\hmu} > \heps|\hsigma \geq \sigma) = \e_{\heps}\left[\Pr({\abs{\mu-\hmu}  > \heps}| \hsigma\geq\sigma , \heps)\right].
\end{align}

We choose the random sample size, $n_\mu$, to calculate $\hmu$ according to \eqref{BEresult}, namely $n_\mu= N_{\CB}\left(\hsigma/\heps,\alpha_\mu, \kmax^{3/4}\right)$. 
%Again, the $Y_i$ used to calculate $\hmu$ is independent of the $Y_i$ used to calculate $\hsigma$.  
The following inequality holds:
\begin{align*}
&\e_{\heps}\left[\Pr({\abs{\mu-\hmu}  > \heps}| \hsigma\geq\sigma , \heps)\right]\\
&\geq \e_{\heps}\left[\Pr\left({\abs{\mu-\frac{1}{n_\mu}\sum_{i=1}^{n_\mu}Y_i} > \heps}\bigg \vert \hsigma\geq\sigma , \heps, n_\mu= N_{\CB}\left(\hsigma/\heps,\alpha_\mu, \kmax^{3/4}\right)\right)\right]\\
&<\alpha_\mu
\end{align*}
%\begin{align}
%&\Pr\left(\abs{\mu-\hmu} >\heps | \hsigma \geq \sigma,\heps = \varepsilon \right) \\
%&=\Pr\left( \left({\abs{\mu-\sum_{i=1}^{n_\mu} Y_i} > \heps},{n_\mu= N_{CB}\left(\heps/\hsigma,\alpha_\mu, \kmax^{3/4}\right)}\right)\,\bigg\vert \,\hsigma>\sigma, \heps = \varepsilon \right)\\
%&<\alpha_\mu
%\end{align}
\end{proof}
%\begin{lemma}\label{step3} 
%In Algorithm \ref{meanMCg}, suppose $\hsigma$, $\tau$, $\heps_i$, $\hmu_i$, $n_i$, $i = 1,2,\cdots,\tau$ and are random, all other parameters are given as constants in the Algorithm \ref{meanMCg}, then the generalized error criterion \eqref{hybridCI} would be satisfied.
%\end{lemma}
\begin{theorem}\label{thm:meanMCg}
Let $Y$ be a random variable with mean $\mu$, and nonnegative variance with modified kurtosis $\kappa \leq \kmax(\alpha_\sigma, n_\sigma, \fudge)$. It follows that Algorithm \ref{alg:meanMCg} terminates in a finite time step almost surely. If the algorithm terminates, then the general error criterion, \eqref{hybridCI}, is satisfied.
\end{theorem}
\begin{proof}
The stopping time is finite is proved in Lemma \ref{taufinite}. Next, we will prove the success of the algorithm.
In Algorithm \ref{alg:meanMCg}, several quantities are random: the random variable $Y$, the stopping time $\tau$, the sample size $n_t$,  the confidence interval width $\heps_t$ in each iteration, for $t = 1,2,\cdots, \tau$. 
%By Proposition \ref{meanMCgProp}, at step $\tau$, we know that
%\begin{align}
%\Pr \left( \abs{\mu-\tmu} \leq \tol(\varepsilon_a, \varepsilon_r \abs{\mu}) \right) \geq \Pr\left(\Delta_{+,\tau} \geq \heps_\tau, \abs{\mu-\hmu_\tau} \leq \heps_\tau\right).
%\end{align}
$\tau$ is the first $t$ for which the stopping criterion $\Delta_{+, t} \geq \heps_t$ is satisfied. Hence, we know the following events are equivalent
\begin{align}\label{STrelationshop}
\{\tau = s\} \Leftrightarrow \{\Delta_{+}(\hmu_s,\heps_s)\geq \heps_s\} \& \{\Delta_{+}(\hmu_t,\heps_t) \leq \heps_t, \text{for }  t = 1,2,\cdots, s-1 \}.
\end{align}
Now let's consider the worst case. Expand the definition of $\hmu_t$ and $\heps_t$ given in Algorithm \ref{alg:meanMCg} for all $t = \tau+1, \tau+2, \cdots$ by defining $\heps_t = \heps_\tau$ for $t >\tau$. And let $\hmu_t$ be defined according to step \ref{deltamu} in Algorithm \ref{alg:meanMCg}. Suppose that at stopping time $\tau$, the desired error tolerance is not obtained, the probability of this event happens is
\begin{align}\label{439}
&\Pr \left( \abs{\mu-\tmu}> \tol(\varepsilon_a,\varepsilon_r\abs{\mu})\right) \nonumber\\
&=\sum_{t=1}^\infty\Pr \left( \abs{\mu-(\hmu_t+\Delta_{-}(\hmu_t,\heps_t))}> \tol(\varepsilon_a,\varepsilon_r\abs{\mu}), \tau = t \right)+\Pr(\tau = \infty)\nonumber\\
&=\sum_{t=1}^\infty\Pr \left( \abs{\mu-(\hmu_t+\Delta_{-}(\hmu_t,\heps_t))}> \tol(\varepsilon_a,\varepsilon_r\abs{\mu}), \tau = t \right).
\end{align}
By Proposition \ref{meanMCgProp}, we know
\begin{align*}
 &\Pr \left( \abs{\mu-(\hmu_t+\Delta_{-}(\hmu_t,\heps_t))}> \tol(\varepsilon_a,\varepsilon_r\abs{\mu}), \tau = t  \right)\\
&\leq
\Pr \left( \abs{\mu-\hmu_t}>\heps_t  \text{ or } \Delta_{+}(\hmu_t,\heps_t)<\heps_t , \tau = t \right).
\end{align*}
The definition of stopping time in \eqref{STrelationshop} implied that $\tau = t$ is incompatible with $ \Delta_{+}(\hmu_t,\heps_t)<\heps_t$. Then the inequality is equal to
\begin{align*}
\Pr \left( \abs{\mu-\hmu_t}> \heps_t  \text{ or } \Delta_{+}(\hmu_t,\heps_t)<\heps_t , \tau = t\right)= \Pr \left( \abs{\mu-\hmu_t}> \heps_t, \tau = t \right).
\end{align*}
Adding extra condition of $\hsigma$ we have:
\begin{align*}
& \Pr \left( \abs{\mu-\hmu_t}> \heps_t, \tau = t \right) \\
 &=  \Pr \left( \abs{\mu-\hmu_t}> \heps_t, \tau = t , \hsigma\geq\sigma\right) + \Pr \left( \abs{\mu-\hmu_t}> \heps_t, \tau = t,\hsigma<\sigma \right) \\
 & \leq \Pr \left( \abs{\mu-\hmu_t}> \heps_t, \hsigma\geq\sigma\right) + \Pr \left(  \tau = t,\hsigma<\sigma \right)
\end{align*}
By Lemma \ref{thm:fixedwidthrandomeps}, the inequality has the form
\begin{equation}\label{440}
\Pr \left( \abs{\mu-\hmu_t}> \heps_t, \hsigma\geq\sigma\right) + \Pr \left(  \tau = t,\hsigma<\sigma \right)\nonumber
<\alpha_t \Pr(\hsigma \geq \sigma) +\Pr \left(  \tau = t,\hsigma<\sigma \right).
\end{equation}
Thus, by \eqref{439} and \eqref{440}
\begin{align*}
\Pr \left( \abs{\mu-\tmu} > \tol(\varepsilon_a,\varepsilon_r\abs{\mu})\right) 
&\leq \Pr(\hsigma \geq \sigma) \sum_{t=1}^\infty \alpha_t+\Pr(\hsigma<\sigma)\\
&=1-\Pr(\hsigma \geq \sigma)\left(1-\sum_{t=1}^\infty \alpha_t\right)
\end{align*}
which is 
\begin{align*}
\Pr \left( \abs{\mu-\tmu} \leq \tol(\varepsilon_a,\varepsilon_r\abs{\mu})\right) &\geq \Pr(\hsigma \geq \sigma)\left(1-\sum_{t=1}^\infty \alpha_t\right) \\&\geq \left(1-\sum_{t=1}^\infty \alpha_t\right) (1-\alpha_\sigma) \\&= 1-\alpha
\end{align*}

Figure \ref{WorstcasemeanMCgproof} illustrates the worst case when estimating $\tmu$. Summing up all the 'bad' scenarios ends up with a worst case error bound.
\begin{figure} 
\centering
\includegraphics[width=6in]
{plottree3.pdf} 
\caption{The red boxes are those 'bad' scenarios that the algorithm terminated with the estimator $\hmu$ not satisfying the fixed width confidence interval condition.\label{WorstcasemeanMCgproof}}
\end{figure}
This completes the proof.
\end{proof}
\Section{The upper bound on cost of Algorithm {\tt meanMC\_g}}\label{sec:meanmcgcost}

\begin{theorem}\label{cost:meanMCg}
The cost of Algorithm \ref{alg:meanMCg} is defined as $$n_{\tot}= n_{\sigma}+\sum_{t=1}^\tau n_t,$$ 
where $\tau$ is the stopping time, $n_\sigma$ is the sample size used to calculate the sample variance $s_{n_\sigma}^2$, and $n_t$ is the sample size used to estimate $\hmu_t$ in each iteration. It has a probabilistic upper bound  $$n_{\up}(\beta)=n_\sigma+ n_1+\sum_{t=2}^{\bar{\tau}(\beta)} N_{\CB}\left(10^{t-1}N_{\CB}^{-1}\left(n_1,\alpha_1,\kappa_{\max}^{3/4}\right),\alpha_t,\kmax^{3/4}\right),$$
where $N_{\CB}$, $N_{\CB}^{-1}$ and $\bar{\tau}(\beta)$ are defined in \eqref{NCBmeanMCabsg}, \eqref{NCBinv} and \eqref{taubardef}.
It satisfies the probability inequality:
\begin{equation}
\Pr \left(n_{\tot}\leq n_{\up} (\beta)\right) \geq 1-\beta.
\end{equation}
\end{theorem}
\begin{proof}
The number of samples required by the Algorithm \ref{alg:meanMCg} is $n_{\sigma}+\sum_{t=1}^\tau n_t$. Although $n_{\sigma}$ and $n_1$ are deterministic, the $n_2, n_3,\cdots, n_\tau$ and $\tau$ are random variables. The cost of this algorithm may be defined probabilistically. The dependency of the random variables is as follows:
 \begin{itemize}
 \item $\hsigma$ depends on the random samples $Y_1,\cdots, Y_{n_\sigma}$;
 \item $n_1$ is given; $\heps_1$ depends on $\hsigma$ and $n_1$; 
 \item $\hmu_t$ depends on $n_t$ and random samples $Y_1, \cdots, Y_{n_t}$ where $t = 1,2,\cdots,\tau$;
 \item $\heps_t$ depends on $\heps_{t-1}$, $\hmu_{t-1}$, where $t= 2,3,\cdots,\tau$;
 \item $n_t$ depends on  $\hsigma$ and $\heps_t$, where  $t = 2,3,\cdots,\tau$;
 \end{itemize}
 
 In order to bound the total cost, we need to bound the stopping time $\tau$ and also the sample size needed in each iteration. The stopping time $\tau$ has a probabilistic bound by $\bar{\tau}$ defined in \eqref{taubarexplicitdef} as been proved in Lemma \ref{tauprobbound}. In each iteration, the sample size $n_t$ is defined as:
$$n_t = N_{\CB}\left(\hsigma/\heps_t,\alpha_t,\kappa_{\max}^{3/4}\right).$$
Since $\heps_t$ has the form
\begin{align*}
\heps_t= \max \left( \heps_{t-1}/10, \min(\heps_{t-1}/2, \max(\varepsilon_a,\theta\varepsilon_r\abs{\hmu_{t-1}})) \right),
\end{align*}
we have $\heps_t \geq \heps_{t-1}/10,$ which is also equivalent to $\heps_t \geq 10^{-t+1} \heps_1.$
Define the lower bound of $\heps_t$ as
$$\underline{\varepsilon_t} = 10^{-t+1}\heps_1 =10^{-t+1} \hsigma /N_{\CB}^{-1}\left(n_1,\alpha_1,\kappa_{\max}^{3/4}\right),$$
then we have 
\begin{equation}\label{ntbound}
n_t=N_{\CB}\left(\hsigma/ \heps_t, \alpha_i,\kmax^{3/4}\right) \leq N_{\CB}\left(10^{t-1}N_{\CB}^{-1}\left(n_1,\alpha_1,\kappa_{\max}^{3/4}\right),\alpha_t,\kappa_{\max}^{3/4}\right). 
\end{equation}
Thus, the total cost can be bounded as follows:
\begin{align*}
&\Pr(n_{\tot} \leq n_{\up}(\beta))\\
 &= \Pr\left(\sum_{t=2}^\tau  n_t  \leq \sum_{t=2}^{\bar{\tau}(\beta)} N_{\CB}\left(10^{t-1}N_{\CB}^{-1}\left(n_1,\alpha_1,\kappa_{\max}^{3/4}\right),\alpha_t,\kappa_{\max}^{3/4}\right) \right)\\
&\geq \Pr\left( n_t \leq  N_{\CB}\left(10^{-t+1}N_{\CB}^{-1}\left(n_1,\alpha_1,\kappa_{\max}^{3/4}\right),\alpha_t,\kappa_{\max}^{3/4}\right),t=1,\cdots, \bar{\tau}(\beta), \tau <\bar{\tau}(\beta) \right).
\intertext{Since the first condition inside the probability is true for sure as been derived in \eqref{ntbound}, we have:}
&\Pr\left(n_{\tot} \leq n_{\up}(\beta)\right) \geq \Pr \left(\tau <\bar{\tau}(\beta)\right) \geq 1-\beta.
\end{align*}
\end{proof}
Note that, this bound may be loose, finding a tighter bound could be a possible future work.

\bigskip

\Section{The Algorithm {\tt cubMC\_g }}\label{sec:cubmcgalg}

An algorithm that performs numerical integration via Monte Carlo sampling with a generalized error tolerance will be presented here. The integrand and parameters are same as in Section \ref{sec:numericalintegrationviaMC}, but with a generalized error tolerance $\tol(\varepsilon_a, \varepsilon_r\abs{\mu})$ as given in Definition \ref{def:tolfun}. The algorithm takes inputs: the integrand $f$, the integration hyperbox $(\va, \vb)$, and the probability density function $\rho$, the absolute error tolerance $\varepsilon_a$ and relative error tolerance $\varepsilon_r$, and does the iteration until the estimator $\tilde{I}$ satisfies the fixed width confidence interval condition
\begin{align}\label{cubMCgCI}
\Pr \left(\abs{I-\tilde{I}} \leq \tol\left(\varepsilon_a, \varepsilon_r\abs{I}\right)\right) \geq 1-\alpha.
\end{align}

The detailed algorithm is explained in Algorithm \ref{alg:cubMCg}.
\begin{algorithm}\label{alg:cubMCg} 
The algorithm requires the user to provide the following inputs:
\begin{itemize}
\item the integrand $f$,
\item the integration interval $(\va, \vb)$,
\item the probability density function $\rho$,
\item the absolute error tolerance $\varepsilon_a \geq 0$, and the relative error tolerance $0 \leq \varepsilon_r <1$, that satisfy the condition $\varepsilon_a+\varepsilon_r >0$,
%\item the hybrid error tolerance function $\tol(\varepsilon_a,\varepsilon_r)$,
%\item the uncertainty level, $\alpha\in (0,1)$, and a sequence of uncertainty $ \alpha_\sigma, \alpha_1,  \alpha_2, \ldots$ satisfying the condition \eqref{alphaseq}, 
%\item the variance inflation factor, $\fudge \in (1,\infty)$, 
%\item the initial sample size for variance estimation, $n_{\sigma} \in \naturals$,
%\item the initial sample size for mean estimation, $n_1 \in \naturals$,
\end{itemize} 
With the given parameters given in Table \ref{table:meanMCgparam}, we first calculate $\kmax=\kappa_{\max}(n_\sigma,\alpha_\sigma,\fudge)$ as defined in \eqref{def:kmax}, then do the following:
%Let $\kmax=\kappa_{\max}(n_\sigma,\alpha_\sigma,\fudge)$ be as defined in \eqref{kappamaxdef}, for any random variable $Y$ lying in the cone of functions with bounded kurtosis, $\cc_{\kappa_{\max}}$, do the following:
\begin{enumerate}
%\renewcommand{\labelenumi}{\alph{enumi})}
\item Sample $n_\sigma$ values of random vector $\vX_i$ from the distribution of $\vX$ with probability density function $\rho(\vX)$ within the integration interval $(\va, \vb)$ and calculate the function values of $f(\vX_i)$. Use this to calculate the sample mean $$\hat{I}_{n_\sigma} = \frac{1}{n_\sigma}\sum_{i=1}^{n_\sigma} f(\vX_i),$$ and sample variance $$s_{n\sigma}^2 = \frac{1}{n_\sigma-1}\sum_{i=1}^{n_\sigma} \left(f(\vX_i)-\hat{I}_{n_\sigma}\right)^2.$$ Also approximate the upper bound on variance by $\hsigma^2 = \fudge^2 s^2_{n_\sigma}$. 

\item If the relative error tolerance $\varepsilon_r=0$, then the algorithm becomes a two stage one instead of multiple stage one. Then calculate the uncertainty level for the second stage estimation, $\alpha_1 = 1-(1-\alpha)/(1-\alpha_\sigma)$, and the sample size for mean estimation, $$n_1 = N_{\CB}\left(\hsigma/\varepsilon_a,\alpha_1,\kappa_{\max}^{3/4}\right),$$where $N_{\CB}$ is defined in \eqref{NCBmeanMCabsg}. Sample $n_1$ values of random vector $\vX_i$ that are independent of those used to calculate $\hsigma^2$ from the distribution of $\vX$ with probability density function $\rho(\vX)$, then calculate the $n_1$ function values of $f(\vX_i)$. Use this to calculate the sample mean 
\begin{align}\label{hatIcubMCg}
\tilde{I}= \frac{1}{n_1}\sum_{i = n_\sigma+1}^{n_\sigma+n_1}f(\vX_i).
\end{align}
Terminate the algorithm.
%Then, compute $\hat{I}$ using $n$ samples, terminate the algorithm.
\item Else, calculate the width of the initial confidence interval for the integral estimation,
\begin{align}
\heps_1=\hsigma N_{\CB}^{-1}\left(n_1,\alpha_1,\kappa_{\max}^{3/4}\right),
\end{align}
where $N_{\CB}^{-1}$ is defined in \eqref{NCBinv}. For $t = 1,2,\cdots$, do the following:
\begin{enumerate}
\item  \label{deltamucubMCg}Compute $\hat{I}_t$ and $\Delta_{+}(\hat{I}_t,\heps_t)$ using sample size $n_t$ and tolerance $\heps_t$, where $\Delta_{+}$ is defined in \eqref{deltadef}.
\item If $\Delta_{+}(\hat{I}_t,\heps_t)\geq  \heps_t$, then $\Delta_{+}$ is large enough. Set the stopping time $\tau = t$ and let $\tilde{I} = \hat{I}_{\tau}+\Delta_{-}(\hat{I}_t,\heps_t)$. Terminate the algorithm.
\item Else, define the next tolerance, $$\heps_{t+1} = \max \left ( \heps_{t-1}/10,\min\left(\heps_t/2, \max\left(\varepsilon_a,\theta\varepsilon_r\abs{\hat{I}_t}\right)\right)\right),$$ $$n_{t+1} = N_{\CB}\left(\hsigma/\hvareps_{t+1},\alpha_{t+1},\kappa_{\max}^{3/4}\right).$$ Increase $t$ by one and go back to step \eqref{deltamu}. 
\end{enumerate}
\end{enumerate}
If this algorithm terminates, then the general error criterion, \eqref{cubMCgCI}, is satisfied.
\end{algorithm}
As {\tt cubMC\_g} applies the same argument as {\tt meanMC\_g}, hence, the proof is the same. In the next section, several numerical examples are implemented to test and compare different algorithms.

\Section{Numerical examples for {\tt cubMC\_g}}\label{sec:cubmcgnumericalexample}

In this section, several integrands are used to test the timing and accuracy results for integrating problems over different algorithms such as {\tt cubMC\_g}, {\tt cubSobol\_g}, {\tt cubLattice\_g} and {\tt integral} in MATLAB, The first three algorithms are part of Guaranteed Automatic Integration Library(GAIL) \cite{GAIL_2_1}, which includes several algorithms that do numerical integration and function approximation with guarantee.

\Subsection{Integrating a product function}

% mat file used to plot TestcubMCgFixedCovon-product-N500d17abstol0.001rel0.001-2015-11-26-00-53-24.mat
\begin{figure}
\centering
\begin{minipage}{9cm} \centering \includegraphics[width=9cm]
{fig_productfun/productiidErrTime-2015-11-26-01-00-46.eps} \\ {\tt cubMC\_g}  \end{minipage}
\begin{minipage}{7cm} \centering \includegraphics[width=7cm]{fig_productfun/productcubSobolErrTime-2015-11-26-01-00-47.eps} \\  {\tt cubSobol\_g}\end{minipage}
\begin{minipage}{7cm} \centering \includegraphics[width=7cm]{fig_productfun/productcubLatticeErrTime-2015-11-26-01-00-48.eps} \\ {\tt cubLattice\_g} \end{minipage}
\caption{Execution times and errors for the prodcut test function \eqref{productfun} for $d=2,3,\cdots,20$ and error tolerance $\max\left(10^{-3},10^{-3}\abs{I}\right)$. The horizontal axis denotes the error/tolerance. The points lies on the left of the red vertical line means the tolerance was met. The points labeled with a diamond are those for which {\tt cubMC\_g} attempts to exceed the cost budget. The solid green curve denotes the empirical distribution function of the error, and the dot-dashed pink curve denotes the empirical distribution function of the time.\label{fig:productfunreltol} }
\end{figure}

A numerical integration problem with integrand \eqref{productfun} and the generalized error tolerance $\max\left(10^{-3},10^{-3}\abs{I}\right)$ has been tested. Time and accuracy are recorded in Figure \ref{fig:productfunreltol}. The initial sample size is $2^{13}$ and the sample budget is $10^{10}$ for all of the algorithms. As we can see, {\tt cubMC\_g} has an accuracy of $100\%$, which is higher than the nominal value of $99\%$. As discussed before, {\tt cubMC\_g} uses a conservative way to construct the fixed width confidence interval for the numerical integration, which may take more time and cost more samples. It, however, gives a more reliable solution. On the other hand, {\tt cubSobol\_g} has 455 out of 500 samples that reach the desired accuracy, whereas, {\tt cubLattice\_g} performs poorly with this test function, only 328 out of 500 samples have met the tolerance criterion, which means the accuracy is only $65.6\%$.

%\Subsection{Integrating a single hump}
%
%In figure \ref{fig:GaussiankerTestFun}, accuracy and timing results have been recorded for the integration problem 
%$\mu=\int_{[0,1]^d} f(\vx) \, \dif \vx$ for a single hump test integrand
%\begin{align}\label{GaussiankerTestFun}
%f(x) = b_0\prod_{j=1}^d\left[ 1 +b_j \exp \left(-\frac{(x_j-h_j)^2}{c_j^2}\right) \right].
%\end{align}
%Here $\vx$ is a $d$ dimensional vector, and $ b_0, \ldots, b_d, c_1, \ldots, c_d, h_1, \ldots, h_d$ are parameters. Figure \ref{fig:GaussiankerTestFun} shows the results of different algorithms being used to integrate 500 different instances of $f$.  For each instance of $f$, the parameters are chosen as follows:
%\begin{itemize} 
%\item $b_1, \ldots, b_d \in [0.1,10]$ with $\log(b_j)$ being IID uniform,
%\item $c_1, \ldots, c_d \in [10^{-6},1]$ with $\log(c_j)$ being IID uniform,
%\item $h_1, \ldots, h_d \in [0,1]$ with $h_j$ being IID uniform,
%\item $b_0$ chosen in terms of the $b_1, \ldots, b_d, c_1, \ldots, c_d, h_1, \ldots, h_d$ to make $\sigma^2 =\norm[2]{f-\mu}^2 \in [10^{-2}, 10^2]$, with $\log(\sigma)$ being IID uniform for each instance.
%\end{itemize}
%These ranges of parameters are chosen so that the algorithms being tested fail to meet the error tolerance a significant number of times.
%
%\begin{figure}
%\centering
%\begin{minipage}{7cm} \centering \includegraphics[width=7cm]
%{gaussiankerd=1iidErrTime-2015-08-19-00-21-30.eps} \\ {\tt cubMC\_g}  \end{minipage}
%\begin{minipage}{7cm} \centering \includegraphics[width=7cm]{gaussiankerd=1cubSobolErrTime-2015-08-19-00-21-31.eps} \\  {\tt cubSobol\_g}\end{minipage}
%\begin{minipage}{7cm} \centering \includegraphics[width=7cm]{gaussiankerd=1cubLatticeErrTime-2015-08-19-00-21-31.eps} \\ {\tt cubLattice\_g} \end{minipage}
%\begin{minipage}{7cm} \centering \includegraphics[width=7cm]{gaussiankerd=1integralErrTime-2015-08-19-00-21-32.eps} \\ {\tt integral } \end{minipage}
%\caption{Execution times and errors for the single hump test function \eqref{GaussianTestFun} with $d=1$ and error tolerance $\varepsilon_a=10^{-8}$ and $\varepsilon_r=10^{-3}$, and a variety of parameters giving a range of $\sigma$ and $\kappa$. Those points to the left/right of the dashed vertical line represent successes/failures of the automatic algorithms.  The solid line shows that cumulative distribution of actual errors, and the dot-dashed line shows the cumulative distribution of execution times.  For the Algorithm {\tt cubMC\_g} the points labeled * are those for which the Theorem \ref{thm:meanMCg} guarantees the error tolerance.\label{fig:GaussiankerTestFun} }
%\end{figure}
%
%These 500 random constructions of $f$ with $d=1$ are integrated using  {\tt cubMC\_g}, {\tt cubSobol\_g}, {\tt cubLattice\_g} and {\tt integral} in MATLAB with absolute tolerance $\varepsilon_a = 10^{-8}$ and $\varepsilon_r=10^{-3}$ and the tolerance function is $\tol(\varepsilon_a, \varepsilon_r) = \max \{ \varepsilon_a, \varepsilon_r \abs{\mu}\}$. 
%
%Figure \ref{fig:GaussiankerTestFun} shows that the {\tt integral} are quite fast, but the successful rate is only a little over $95\%$ which is less than our uncertainty rate $1\%$. The three algorithms in GAIL performs better than {\tt integral}, they all have almost $100\%$ success rate.
%
%In the plots for {\tt cubMC\_g} , an asterisk is used to label those points satisfying $\kappa \le \kmax$, where $\kappa$ is defined in \eqref{kurtosis} and $\kmax$ is defined in \eqref{def:kmax}. All such points fall within the prescribed error tolerance, which is even better than the guaranteed confidence of $99\%$.Those points labeled with a dot, are those for which $\kappa > \kmax$, and so no guarantee holds. The points labeled with a diamond are those for which  {\tt cubMC\_g}  attempts to exceed the cost budget, which is set to be $10^{9}$.
%
%\begin{figure}
%\centering
%\begin{minipage}{9cm} \centering \includegraphics[width=9cm]
%{gaussiankerd=3iidErrTime-2015-08-19-00-11-43.eps} \\ {\tt cubMC\_g}  \end{minipage}
%\begin{minipage}{7cm} \centering \includegraphics[width=7cm]{gaussiankerd=3cubSobolErrTime-2015-08-19-00-11-44.eps} \\  {\tt cubSobol\_g}\end{minipage}
%\begin{minipage}{7cm} \centering \includegraphics[width=7cm]{gaussiankerd=3cubLatticeErrTime-2015-08-19-00-11-44.eps} \\ {\tt cubLattice\_g} \end{minipage}
%\caption{Execution times and errors for the single hump test function \eqref{GaussiankerTestFun} for $d=2, \ldots, 8$, with the rest of the parameters as in Figure \ref{fig:GaussiankerTestFun}.\label{fig:GaussiankerHDTestFun} }
%\end{figure}
%
%Figure \ref{fig:GaussiankerHDTestFun} repeats the simulation shown in Figure \ref{fig:GaussiankerTestFun} for the same test function \eqref{GaussiankerTestFun}, but now with $d=2, \ldots, 8$ chosen randomly and uniformly.  For this case the univariate integration algorithms are inapplicable, but the multidimensional routines can be used.  There are more cases where the  {\tt cubMC\_g}  tries to exceed the maximum sample size allowed, but the behavior seen for $d=1$ still generally applies.  

\Subsection{Integrating Keister's test function}
% mat TestcubMCon-Keister-uniform-N500d18abstol0.001rel0.001-2015-08-24-21-47-53.mat
\begin{figure}
\centering
\begin{minipage}{9cm} \centering \includegraphics[width=9cm]
{PlotKeisterFun/gaussianiidErrTime-2015-11-29-01-06-32.eps} \\ {\tt meanMC\_g}  \end{minipage}
\begin{minipage}{7cm} \centering \includegraphics[width=7cm]{PlotKeisterFun/gaussiancubSobolErrTime-2015-11-29-01-06-33.eps} \\  {\tt cubSobol\_g}\end{minipage}
\begin{minipage}{7cm} \centering \includegraphics[width=7cm]{PlotKeisterFun/gaussiancubLatticeErrTime-2015-11-29-01-06-33.eps} \\ {\tt cubLattice\_g} \end{minipage}
\caption{Execution times and errors for the Keister test function \eqref{KeisterTestFun} with the dimension uniformly random chosen between 1 and 20. The tolerance is $\max\left(10^{-3},10^{-3} \abs{I}\right)$. All other settings are same as in Figure \ref{fig:productfunreltol}. \label{fig:KeisterTestFun} }
\end{figure}
Keister \cite{Keister96} considered the following multidimensional integral that has applications in physics
\begin{align} \label{KeisterTestFun}
I= \int_{\reals^d} \cos (\norm[2]{\vx})e^{-\norm[2]{\vx}^2} \dif \vx &= 2^{-d/2}\int_{\reals^d} \cos(\norm[2]{\vy}/\sqrt{2}) e^{-\norm[2]{\vy}^2/2} \dif \vy \nonumber \\
&= \pi^{d/2} \int_{\reals^d} \cos(\norm[2]{\vy}/\sqrt{2}) \frac{e^{-\norm[2]{\vy}^2/2}}{(2\pi)^{d/2}} \dif \vy \nonumber \\
 &= \pi^{s/2} \int_{[0,1)^d} \cos \left ( \sqrt{\sum_{j=1}^d \frac{[\Phi^{-1}(y_i)]^2}{2}}\right )\dif \vy,
\end{align}
where $\Phi$ denotes the standard Gaussian cumulative distribution function defined as below:
$$\Phi(x) = \frac{1}{\sqrt{2\pi}} \int_{-\infty}^x e^{-s^2/2}ds, x \in [-\infty, \infty].$$
Let the integrand be 
\begin{equation*}
f(\vx) = \pi^{d/2} \cos \left( \sqrt{\frac{1}{2} \sum_{i=1}^d (\Phi^{-1} (x_i))^2}\right).
\end{equation*}
The integration problem can be written as
\begin{equation*}
I = \int_{[0,1]^d} f(\vx)\dif\vx.
\end{equation*}
By applying the Monte Carlo method, the integral can be estimated as follows:
\begin{align}
\hIn = \frac{\pi^{d/2}}{n}\sum_{i=1}^n \cos \left (\sqrt{\frac12 \sum_{j=1}^d (\Phi^{-1}(x_{i,j}))^2}\right),
\end{align}
where $\vx_i = (x_{i1},\cdots,x_{id}) \in \reals$ , $i=1,\cdots, n.$
Keister \cite{Keister96} showed a formula for calculating the exact integral. With the analytical solution being  computed, we compared the numerical results over three different algorithms with a generalized error criterion. Timing and accuracy were recorded in Figure \ref{fig:KeisterTestFun}.

%Keister  considered the following multidimensional integral that has applications in physics:
%\begin{equation}\label{KeisterTestFun}
%\int_{R^d} \cos (\norm{x}_2)e^{-\norm{x}_2^2} dx 
%\end{equation}
%This isotropic integral take the form
%\begin{align}
% \int_{\reals^d} \cos (\norm{x}_2)e^{-\norm{x}_2^2} dx &= 2^{-d/2}\int_{\reals^d} \cos(\norm{y}/\sqrt{2}) e^{-\norm{y}^2/2} dy \\
%&= \pi^{d/2} \int_{\reals^d} \cos(\norm{y}/\sqrt{2}) \frac{e^{-\norm{y}^2/2}}{(2\pi)^{d/2}} dy\\
% &= \pi^{s/2} \int_{[0,1)^d} \cos \left ( \sqrt{\sum_{j=1}^d \frac{[\Phi^{-1}(y_i)]^2}{2}}\right )dy,
%\end{align}
%where $\Phi$ denotes the standard Gaussian distribution function defined as below:
%\begin{align}\label{Def:PhiCDF}
%\Phi(x) = \frac{1}{\sqrt{2\pi}} \int_{-\infty}^x e^{-s^2/2}ds, x \in [-\infty, \infty].
%\end{align}
%
%Letting the integrand be 
%\begin{equation}
%f(\vx) = \pi^{d/2} \cos \left( \sqrt{\frac{1}{2} \sum_{i=1}^d (\Phi^{-1} (x_i))^2}\right).
%\end{equation}
%The integration problem could be written as
%\begin{equation}
%\mu = \int_{[0,1]^d} f(\vx)d\vx.
%\end{equation}
%By applying Monte Carlo methods, the integral could be estimated as follows:
%\begin{align}
%\hmu = \frac{\pi^{d/2}}{n}\sum_{i=1}^n \cos \left (\sqrt{\frac12 \sum_{j=1}^d (\phi^{-1}(x_{i,j}))^2}\right )
%\end{align}
%where $\vx_i = (x_{i1},\cdots,x_{id}) \in \reals$ , $i=1,\cdots, n.$
%As Keister [cite] showed an exact formula for calculating the exact integral, by coding them into MATLAB, we compared the numerical results over different algorithms with the generalized error tolerance criterion.
500 replications are performed for dimension $d$ IID uniform in $[1,20]$, with the integration interval $[0,1]^d$. The analytic solution, $I$, is calculated and the true error is recorded. The generalized error tolerance is $\max\left(10^{-3},10^{-3}\abs{I}\right)$. The initial sample size used to estimate the variance in {\tt cubMC\_g} is $2^{13}$ whereas the initial Sobol's and Lattice points taken in {\tt cubSobol\_g} and {\tt cubLattice\_g} are $2^{13}$. The sample budget is $10^{10}$. 

As we can see,  {\tt cubMC\_g} takes significantly more time than {\tt cubSobol\_g} and {\tt cubLattice\_g}. The accuracy is $98.6\%$, which is a bit less than our confidence level, $99\%$. The reason is that the points labeled as a diamond are those attempt to exceed the sample budget $10^{10}$,  the guarantees do not hold for those points. If we exclude those 50 points labeled as a diamond and calculate the success rate again, it is $100\%$ of the time. Although we have not calculated the exact kurtosis, which has a very complicated form, nor we check if it is less than the kurtosis maximum, our numerical result using {\tt cubMC\_g} is still better than other two quasi-Monte Carlo methods in the sense of the error criterion. For {\tt cubSobol\_g}, the error meets tolerance $98\%$ of the time, whereas, for  {\tt cubLattice\_g}, the success rate is only $71\%$.  For this test function, we do not see the significant advantage of {\tt cubMC\_g}, the reason is that the test function is smooth and quasi-Monte Carlo methods perform well for smooth integrand. {\tt cubMC\_g} is more robust and does not assume any smoothness of the integrand. In this case {\tt cubMC\_g} does not take the advantage of the robustness and the mild assumption for the integrand, hence, it costs more time  than {\tt cubSobol\_g} and {\tt cubLattice\_g}.

\Subsection{Integrating Multivariate Normal Probability with a deterministic covariance matrix}
\begin{figure}
\centering
\begin{minipage}{9cm} \centering \includegraphics[width=9cm]
{PlotMVNPFun/MVNPiidErrTime-2015-11-29-01-36-25.eps} \\ {\tt cubMC\_g}  \end{minipage}
\begin{minipage}{7cm} \centering \includegraphics[width=7cm]{PlotMVNPFun/MVNPcubSobolErrTime-2015-11-29-01-36-25.eps} \\  {\tt cubSobol\_g}\end{minipage}
\begin{minipage}{7cm} \centering \includegraphics[width=7cm]{PlotMVNPFun/MVNPcubLatticeErrTime-2015-11-29-01-36-26.eps} \\ {\tt cubLattice\_g} \end{minipage}
\caption{Execution times and errors for Multivariate Normal Probability test function \eqref{MVNP} with the dimension $d$ IID uniform in $[2,8]$. The tolerance is $\max\left(10^{-3},10^{-4} \abs{I}\right)$. All other settings are same as in Figure \ref{fig:productfunreltol}. \label{fig:MVNPTestFun1} }
\end{figure}
A function that has been used in many statistics applications is the numerical computation of Multivariate Normal Probability function 
\begin{equation}\label{MVNP}
F(\va,\vb) = \frac{1}{\sqrt{\abs{\Sigma}(2\pi)^d}}\int_{a_1}^{b_1}\cdots\int_{a_d}^{b_d} e^{-\frac{1}{2} \boldsymbol{\theta}' \Sigma^{-1}\boldsymbol{\theta} }\dif \boldsymbol{\theta},
\end{equation}
where $\va$ and $\vb$ are $d \times 1$ vectors, $\boldsymbol{\theta} = (\theta_1,\cdots,\theta_d)'$ and $\Sigma$ is given as $d \times d$ symmetric positive definite covariance matrix. Genz \cite{Genz92} points that if $a_i$ is $-\infty$ and $b_i$ is $\infty$ for some $i$, an appropriate transformation allows the $i$th variable to be integrated explicitly, which reduces the number of variables in the problem. Thus, for each $i$, we assume at most one of $a_i$ and $b_i$ be infinite. 

One way to compute $F$ is by Monte Carlo. However, the infinite integration limits need to be handled either by some type of transformation to a finite region or by using some carefully selected cutoff values. Genz \cite{Genz92, GenzBretz02} proposed a transformation that used to transform the original $d$ dimensional integral into a new $d-1$ dimensional integral over a unit cube. The transformation was done by three steps:
\begin{enumerate}
\item First, apply Cholesky decomposition to the covariance matrix $\Sigma$, i.e. $\Sigma = CC'$, where $C$ is a lower triangular $d \times d$ matrix. Let $\boldsymbol{\theta}=C \vy$, so that $\boldsymbol{\theta}'\Sigma^{-1}\boldsymbol{\theta} = \vy' C'(C')^{-1} C^{-1}C\vy =y'y$, and $\dif\boldsymbol{\theta}=\abs{C} \dif \vy = \abs{\Sigma}^{1/2} \dif \vy$.
\item Second, let
$$a_1'=a_1/c_{11}, \quad b_1'=b_1/c_{11}, \quad f_1=\Phi(a'), \quad  e_1 = \Phi(b'),$$
where $\Phi$ is the standard Gaussian cumulative distribution function. 
Thus, for $i=1,2,\cdots,d$, recursively define
$$y_{i}(w_1,\cdots,w_{i-1}) = \Phi^{-1} (d_i+w_i(e_i-f_i)),$$
$$a_i'(w_1,\cdots,w_{i-1}) = \frac{a_i-\sum_{j=1}^{i-1} c_{ij}y_j}{c_{ii}},$$
$$b_i'(w_1,\cdots,w_{i-1}) = \frac{b_i-\sum_{j=1}^{i-1} c_{ij}y_j}{c_{ii}},$$
$$f_i(w_1,\cdots,w_{i-1}) = \Phi(a_i'),$$
$$e_i(w_1,\cdots,w_{i-1}) = \Phi(b_i').$$
\item Last, the multivariate normal probability in \eqref{MVNP} may be written as 
\begin{equation}\label{MVNP2}
F(\va,\vb) = (e_1-f_1)\int_0^1 (e_2-f_2) \cdots \int_0^1(e_d-f_d)\dif \vw.
\end{equation}
\end{enumerate}
The integral \eqref{MVNP2} can be evaluated using different techniques, including Monte Carlo, quasi-Monte Carlo and adaptive methods. Two test problems are shown in the next section to test the time and error when evaluating the integral with different algorithms.

Now, consider the parameters defined as follows 
\begin{subequations} \label{MVNPexp1param}
\begin{gather}
a_1=\cdots=a_d =-\infty,\\
b_j \text{ IID uniformly on } [0,\sqrt{d}],\\
\Sigma = (\sigma_{ij})_{d\times d}, \text{where } \sigma_{ij} =  
  \begin{cases}1 \quad i=j \\
   \sigma \quad i \neq j 
  \end{cases},\\
\sigma \text{ distributed uniformly on } [0,1].
\end{gather}
\end{subequations}
As the off diagonal elements of the covariance matrix are all $\sigma$ and the lower limits of the integration are all $-\infty$, the analytic solution has the form,
\begin{align}\label{truesolMVNP}
F(-\infty, \vb,\Sigma) = \frac{1}{\sqrt{2\pi}}\int_{-\infty}^{\infty} e^{-\frac{t^2}{2}} \prod_{j=1}^d \left(\Phi\left(\left(b_j+\sqrt{\sigma}t\right)/\sqrt{1-\sigma}\right)\right)dt,
\end{align}
which could be evaluated by univariate quadrature techniques.

Numerical experiments are performed using 500 replications with dimension $d$ IID uniformly chosen from 2 to 8, all other parameters are chosen according to \eqref{MVNPexp1param}. The error tolerance is $\max\left(10^{-3},10^{-4} \abs{\mu}\right)$, where $\mu$ is computed by \eqref{truesolMVNP} using univariate quadrature function {\tt integral} in MATLAB. The initial sample size for estimating the variance is $2^{10}$, while the initial sample size using {\tt cubSobol\_g} and {\tt cubLattice\_g} are $2^{10}$ also. Time and error are recorded and plotted in Figure \ref{fig:MVNPTestFun1} using these three algorithms.

As we could see from Figure \ref{fig:MVNPTestFun1}, {\tt cubMC\_g} used significantly more time than {\tt cubSobol\_g} and {\tt cubLattice\_g}, while $100\%$ of the points satisfy the error tolerance. For {\tt cubSobol\_g}, $100\%$ of the points satisfy the tolerance whereas $99.2\%$ of the points satisfy the error tolerance for {\tt cubLattice\_g}.

%the parameters random chosen as follows:
%\begin{itemize}
%\item $d$ is IID uniformly random chosen on $[1,8]$,
%\item $\sigma$ is IID uniformly random chosen on $[0,1]$,
%\item $b_j$ is IID uniformly random chosen on $[0,\sqrt{d}]$.
%\end{itemize}

%In the second step, each of the $y_i$'s could be transformed separately using $y_i = \Phi^{-1}(z_i)$, where $\Phi$ is the standard univariate normal distribution function. Then $F(\va,\vb)$ becomes
%$$F(\va,\vb) = \int$$
%
%Unfortunately, the original form is not well-suited for numerical quadrature. Therefore, Alan Genz [cite] proposed a transformation of variables that results in an integral over an s-1-dimensional unit cube. See [12, 13] for the details of the transformation, and see [13, 15] for comparisons of different methods for calculating multivariate normal probabilities.
%The particular test problem is one considered by [13, 24] and may be described as follows:


%\begin{subequations}
%\begin{gather}
%a_1=\cdots=a_d =-\infty,\\
%b_j \text{ IID uniformly on } [0,\sqrt{s}],\\
%\Sigma \text{ generated randomly according to } [13,39].
%\end{gather}
%\end{subequations}
%Time and error were recorded and plotted in figure [cite].
\Subsection{Integrating Multivariate Normal Probability with a random covariance matrix}
\begin{figure}
\centering
 \includegraphics[width=9cm]{boxplotofMVNP-2015-09-17-15-58-17.eps} \\ {\tt cubMC\_g} 
\caption{Box and whisker plot for execution time of integrating Multivariate Normal Probability function \eqref{MVNP} with lower triangular matrix $C$ \eqref{lowertrimatrixC} and other parameters \eqref{MVNPexp2param}. The horizonal axis denotes the different $\delta_t$ ranged from $0.01$ to $10$, the vertical axis denotes the computation time. \label{fig:MVNPadaptivitiy} }
\end{figure}
Another example with a random covariance matrix can be used to test the adaption of the algorithms. The way to construct the test function is described in the following steps. 

First, generate a random lower triangular matrix $C$ and covariance matrix $\Sigma$:
\begin{itemize}
\item Construct the lower triangular matrix of the form
\begin{equation}\label{lowertrimatrixC}
C = \left (\begin{array}{ccccc}
\sigma_1&0&0&\cdots&0\\
0&\sigma_2&0& \cdots &0 \\
\cdots\\
0&0&\cdots&0&\sigma_d \end{array}\right)+\delta_t  \left (\begin{array}{ccccc}
0&0&0&\cdots&0\\
x_{21}&0&0& \cdots &0 \\
\cdots\\
x_{d1}&x_{d2}&\cdots&x_{dd}&0 \end{array}\right), 
\end{equation}
where $\log_{10}\sigma_i$ is IID uniform on $[1,3]$, $\delta_t$ is a shrinkage parameter, $x_{ij}$ is IID uniform on $[-1,1]$.
\item The lower triangular matrix $C$ multiplied by its transpose $C'$ will be the covariance matrix $\Sigma = CC'$.
\end{itemize}

Second, choose integration dimension and interval as follows:
\begin{subequations} \label{MVNPexp2param}
\begin{gather}
d \text{ IID uniform on } [2,8] \\
a_1=\cdots=a_d =-\infty,\\
b_j \text{ IID uniform on } \left[0,\sqrt{d} \right].
\end{gather}
\end{subequations}

The computational time is recorded and plotted in Figure \ref{fig:MVNPadaptivitiy}. 500 replications are performed with each of four different shrinkage factor $\delta_t =  0.01,0.1, 1, 10$. The tolerance is $ \max \left(10^{-5}, 10^{-5}\abs{\mu}\right)$. As we can see, when $\delta_t$ gets large, the algorithm uses relatively more time. This plot shows the adaption of the algorithm {\tt cubMC\_g}, which, may adjust the sample or time required according the difficulty of the problem.


\Chapter{Guaranteed method for Bernoulli Random variables}\label{chapter:meanMCberg}

The Bernoulli trial is one of the simplest, yet most important, random processes. It takes two outcomes, namely, success or failure with a constant probability. Many real world problems can be modeled in terms of Bernoulli trials, i.e. estimating the probability of bankruptcy or a power failure.

 A Bernoulli random variable, $Y$, takes two values, 0 and 1, with the probability $\Pr(Y=1) = p$. Denote the distribution as $ \Ber(p)$. Sometimes the process governing $Y$ may have a complex form. This means that we may be able to generate IID $Y_i$, but not have a simple formula for computing $p$ analytically. 
 %This distribution is denoted by $\Ber(p)$. 

As being discussed in Chapter \ref{chapter:meanMCabsg} and \ref{chapter:meanMCg}, we present two algorithms for evaluating the mean of an arbitrary random variable based on the assumption that the kurtosis has a known upper bound. With knowledge of the distribution, we are looking into a better way to construct the fixed width confidence intervals for the Bernoulli random variables. Before presenting our algorithms, we first review some existing literature that based on the confidence interval construction of Bernoulli random variables.

Wald confidence interval \cite[Section 1.3.3]{Agresti02} is a commonly used one for Bernoulli random variables based on maximum likelihood estimate. Unfortunately, it performs poorly when the sample size $n$ is small or the true $p$ is close to 0 or 1. Agresti \cite[Section 1.4.2]{Agresti02} suggested constructing confidence intervals for binomial proportion by adding a pseudo-count of $z_{\alpha/2}/2$ successes and failures. Thus, the estimated mean will be $\tilde{p}_n =(n\hp_n+z_{\alpha/2}/2) / (n+z_{\alpha/2})$. This method is also called adjusted Wald interval or Wilson score interval, since it was first discussed by E. B. Wilson \cite{wilson27}. This method performs better than the Wald interval, however, it is an approximate result and carries no guarantee.

Clopper and Pearson \cite{CP34} suggested a tail method to calculate the exact confidence interval for a given sample size $n$ and confidence level $1-\alpha$. Sterne \cite{sterne54}, Crow \cite{crow56}, Blyth and Still \cite{BS83} and Blaker \cite{Blaker00} proposed different ways to improve the exact confidence interval, however, all of them were only tested on small sample sizes, $n$.  Moreover, there is no clue how to determine $n$ with fixed half-width $\varepsilon_a$ is given. 

Hoeffding's Inequality in Theorem \ref{hoeff} provides a way to confidently estimate the probability $p$ given the random variable is a Bernoulli one. We will use this inequality to construct the fixed width confidence interval for estimating Bernoulli random variables in the following sections.

In this chapter, we will present the ways to reliably construct a fixed width confidence interval for $p$ with a guaranteed confidence level. That is, given an absolute error tolerance, $\varepsilon_a$, or a relative error tolerance, $\varepsilon_r$, and confidence level, $1-\alpha$, the algorithm determines the sample size, $n$, needed to compute the sample mean, $\hp_n$, that satisfies the fixed width confidence interval condition $\Pr(\abs{p-\hp_n}\leq \varepsilon_a) \geq 1-\alpha$ or $\Pr(\abs{p-\hp_n}/p\leq \varepsilon_r) \geq 1-\alpha$.  

\Section{Absolute error criterion} \label{sec:meanMCberg}

Theorem \ref{hoeff} motivates the following algorithm for constructing a fixed width confidence interval for an unknown $p$ in terms of IID samples of $Y\sim \Ber(p)$.
\begin{algorithm}[{\tt meanMCBer\_g} with absolute error criterion]\label{algabs}
Given an absolute error tolerance $\varepsilon_a > 0$ and an uncertainty level $\alpha \in (0,1)$, one generates
\begin{equation}\label{hoeffn}
n = N_{\Hoeff}(\varepsilon_a,\alpha):= \left \lceil  \frac{\log(2/\alpha)}{2\varepsilon_a^2} \right \rceil
\end{equation}
IID Bernoulli random samples of $Y \sim \Ber(p)$, and uses them to compute sample mean:
\begin{equation} \label{abserrp}
\hat{p}_n = \frac1n\sum_{i =1}^n Y_i.
\end{equation}
Return $\hp_n$ as the answer.
\end{algorithm}
The following theorem ensures the success of the algorithm.
\begin{theorem}
Algorithm \ref{algabs} returns an answer, $\hat{p}_n$, that satisfies 
\begin{equation}\label{probabs}
\Pr\left(\abs{\hat{p}_n -p} \leq \varepsilon_a\right) \geq 1-\alpha.
\end{equation}
at a computational cost of $N_{\Hoeff}(\varepsilon_a,\alpha)$ samples.
\end{theorem}
\begin{proof}
The definition of the sample size in \eqref{hoeffn} implies that
\begin{equation*}
n = \left \lceil \frac{\log(2/\alpha)}{2\varepsilon_a^2} \right \rceil \geq  \frac{\log(2/\alpha)}{2\varepsilon_a^2} \Rightarrow 
1- 2e^{-2n\varepsilon_a^2} \geq 1-\alpha.
\end{equation*}
Applying Hoeffding's Inequality in Theorem \ref{hoeff} leads directly to \eqref{probabs}. 
\end{proof}

\Section{Relative error criterion}

Given the relative error tolerance, $\varepsilon_r$, we are looking into constructing the fixed width confidence interval for $p$ with the relative error criterion. By applying inequality \eqref{ineq:Hoefftwoside} in Theorem \ref{hoeff}, we have
\begin{align*}
\Pr\left(\frac{|\hp_n-p|}{p} \leq \varepsilon_r\right) \geq 1-2\exp\left(-2np^2\varepsilon_r^2\right).
\end{align*}
In order to make the right hand side of the inequality at least $1-\alpha$, i.e.:
$$1-2\exp\left(-2np^2\varepsilon_r^2\right) \geq 1-\alpha$$
the sample size $n$ should satisfy the following condition: 
\begin{align*}
n \geq -\frac{\log (\alpha/2)}{2 p^2 \varepsilon_r^2}.
\end{align*}
Since $p$ is unknown, the lower bound is needed to ensure $n$ is large enough to bound the relative error. By applying one-side Hoeffding's Inequality \eqref{ineq:HoeffonesideA} in Theorem \ref{hoeff}, the following inequality holds:
\begin{align}\label{onesidehoeffn}
\Pr\left(p \geq \hp_n-\varepsilon \right) \geq 1- \exp(-2n\varepsilon^2).
\end{align}
If the sample size 
$$n \geq -\frac{\log(\alpha)} {2\varepsilon^2 },$$
$p$ has a conservative lower bound $$\Pr(p \geq \hp_n-\varepsilon)\geq1-\alpha.$$

Based on the argument above, an algorithm that estimates the means of Bernoulli random variables to a prescribed relative error tolerance is developed. The basic idea is to find a reliable lower bound of $p$ at first, then estimate the $p$ to the prescribed relative error tolerance.
\begin{algorithm}[{\tt meanMCber\_g} with relative error criterion]\label{algrel}
The algorithm requires two inputs:
\begin{itemize}
\item the relative error tolerance, $\varepsilon_r > 0$, 
\item uncertainty level, $\alpha \in (0,1)$.
\end{itemize}
For $i = 1,2,\cdots$, do the following:
\begin{enumerate}
\item \label{step1} 
Compute the sample average $\hat{p}_{n_i}$ using 
\begin{align}\label{relalgni}
n_i = \left \lceil \frac{-4^i \log\left ( \alpha_i \right)}{2\varepsilon_r^2}\right \rceil
\end{align}
 IID Bernoulli random samples, where 
 \begin{equation}\label{alphaiBerRel}
 \alpha_i =1-(1-\alpha/2)^{2^{-i}}.
 \end{equation}
\item \label{step2}
\begin{enumerate}
\item If 
\begin{equation}\label{stoppingcriterionBerRel}
p_{n_i} \geq 3\varepsilon_r 2^{-i},
\end{equation} set $\tau=i$ and compute 
\begin{align}\label{def:hpl}
\hp_L= \hp_{n_\tau} - \varepsilon_r 2^{-\tau},
\end{align}
 a highly probable lower bound on $p$. Go to step \ref{step3}. 
\item Else, set $i = i+1$, and go back to step \ref{step1}.
\end{enumerate}
    \item \label{step3}
Compute $\hp = \hp_n$ using 
\begin{align}\label{reln}
n= \left \lceil \frac{\log (4/\alpha)}{2(\hp_L\varepsilon_{r})^2} \right \rceil 
\end{align}
  IID Bernoulli random samples that are independent of those used to compute $\hp_{n_1}, \cdots, \hp_{n_\tau}$. Terminate the algorithm.
  \end{enumerate}
\end{algorithm}
If the algorithm terminates, then the relative error criterion is satisfied. The success of the algorithm is guaranteed in the following theorem.
\begin{theorem}
Let $Y$ be a Bernoulli random variable with mean $p$, it follows that Algorithm \ref{algrel} using sample size \eqref{reln} yields an estimate $\hp_n$ that satisfies the fixed width confidence interval condition:
\begin{align}\label{probrel}
\Pr\left(\frac{\abs{\hp -p}}{p} \leq \varepsilon_r \right) \geq 1-\alpha.
\end{align}
\end{theorem}
\begin{proof}
At each step of iteration, the sample size is given in \eqref{relalgni}, i.e.
\begin{align*}
n_i \geq -\frac{4^i \log(\alpha_i)} {2\varepsilon_r^2 },
 \end{align*}
 which is equivalent to:
\begin{align}\label{nibound}
1- \exp\left(-2n_i4^{-i}\varepsilon_r^2\right) \geq 1-\alpha_i.
\end{align}
 Thus, by applying inequality \eqref{onesidehoeffn}, the following inequality holds:
 \begin{align} \label{pnibound}
\Pr\left(p \geq \hp_{n_i}-2^{-i}\varepsilon_r  \right) \geq 1- \exp\left(-2n_i4^{-i}\varepsilon_r^2\right).
 \end{align} 
Combining \eqref{nibound} and \eqref{pnibound} we get
 \begin{align} \label{lbpni}
 \Pr\left(p \geq \hp_{n_i}-2^{-i}\varepsilon_r \right) \geq 1-\alpha_i.
 \end{align}
Similarly, the definition of sample size in \eqref{reln} leads to the following inequality:
\begin{align}\label{nbound}
1-2\exp\left(-2 (\hp_L\varepsilon_{r})^2\right) \geq 1-\alpha/2.
\end{align}
Applying \eqref{ineq:Hoefftwoside} in Theorem \ref{hoeff} yields:
\begin{align}\label{pnbound}
 \Pr\left( \left|{\hp_n - p}\right| \leq \hp_L \varepsilon_r \right ) \geq  1-2\exp\left(-2  (\hp_L\varepsilon_{r})^2\right).
 \end{align}
Combining \eqref{nbound} and \eqref{pnbound} yields:
 \begin{align}\label{lbpn}
  \Pr \left(\abs{\hp_n - p} \leq \hp_L \varepsilon_r \right ) \geq 1-\alpha/2.
 \end{align}
Suppose the Algorithm \ref{algrel} terminates at time $\tau$, in which step, the relative error criterion 
\begin{align}
 \Pr \left ( \frac{\abs{\hp - p}}{p} \leq \varepsilon_r\right ) & \geq \Pr \left ( \abs{\hp_n - p} \leq \hp_L\varepsilon_r \ \ \& \ \   \hp_L \leq p \right ) \label{addpl}\\&
 \geq \Pr \left( \abs{\hp_n - p} \leq \hp_L \varepsilon_r \ \  \& \ \  \hp_{n_i} - 2^{-i}\varepsilon_r \leq p\  \forall i  \leq \tau \right) \label{makeforall}\\&
  = \Pr\left ( \abs{\hp_n - p} \leq \hp_L \varepsilon_r \right)+ \prod_{i = 1}^\tau \Pr \left(\hp_{n_i} - 2^{-i}\varepsilon_r  \leq p \right)-1\label{usecombineq} \\&
\geq (1-\alpha/2)+\prod_{i=1}^\tau(1-\alpha_i)-1 \label{useabove} \\&
\geq (1-\alpha/2)+\prod_{i=1}^\infty (1-\alpha_i)-1  \\&
 = 1-\alpha \label{constructalphai}.
\end{align}
The inequality \eqref{addpl} is true by adding an extra condition of a middle term  $\hp_L$, which makes the probability smaller. \eqref{makeforall} is true by enlarging the set, letting all the previous steps satisfying the lower bound condition. \eqref{usecombineq} is true by using the inequality $\Pr(A+B)\geq\Pr(A)+\Pr(B)-1$ and the property of independence. \eqref{useabove} is true by applying the inequality \eqref{lbpni} and \eqref{lbpn}. The final result is obtained by the way we construct $\alpha_i$ in\eqref{alphaiBerRel}.
\end{proof}

Next, we want to prove the computational cost of Algorithm \ref{algrel} has an upper bound. As shown in  Algorithm \ref{algrel}, the total cost is combined by two parts:
\begin{itemize}
\item For step \ref{step1} and \ref{step2}, the cost is $n_i = \left \lceil -4^i \log\left ( \alpha_i \right)/(2\varepsilon_r^2) \right \rceil$, for $i = 1 \cdots \tau$.
\item For step \ref{step3}, the cost is $n= \left \lceil -\log (\alpha/4)/(2  (\hp_L \varepsilon_{r})^2)\right \rceil $. 
\end{itemize}
Thus, the total cost can be written as: 
\begin{align}\label{totcostn}
n_{\tot}=\sum_{i=1}^{\tau} \left \lceil -\frac{4^i \log(\alpha_i)} {2\varepsilon_r^2 } \right\rceil+\left \lceil -\frac{\log (\alpha/4)}{2  (\hp_L \varepsilon_{r})^2} \right \rceil,
\end{align}
where $\hp_L$ is defined in \eqref{def:hpl}. There are two random parts in $n_{\tot}$, the stopping time $\tau$ and $\hp_L$. In order to find the upper bound on cost, we need the upper bound on $\tau$ and lower bound on $\hp_L$. Next, define the upper bound on the stopping time as
\begin{equation}\label{taubarBerRel}
\bar{\tau}: = \min \{i:  p-\varepsilon_r2^{-i} \geq 3\varepsilon_r 2^{-i}, i \in \naturals \} =\max(1,\left \lceil \log_2 (4\varepsilon_r/p)\right \rceil),
\end{equation}
and the upper bound on cost as:
\begin{align}
n_{\up} = \sum_{i=1}^{\bar{\tau}} \left \lceil -\frac{4^i \log(\alpha_i)} {2\varepsilon_r^2 } \right\rceil+\left \lceil -\frac{\log (\alpha/4)}{2  \varepsilon_r^4} \right \rceil.
\end{align}
Thus, we have the following theorem.

\begin{theorem} \label{costupperboundrel}
The cost of Algorithm \ref{algrel}, $n_{\tot}$,  has a probabilistic bound, $n_{\up}$, that satisfying the following inequality
\begin{equation}
\Pr\left(n_{\tot}<n_{\up}\right) \geq 1-\alpha/2.
\end{equation}
\end{theorem}
\begin{proof}
As the stopping criterion is: 
\begin{align*}
\tau &= \min\left\{i: \hp_{n_i}\geq 3\varepsilon_r 2^{-i}, i \in \naturals \right \},
\end{align*}
by the definition of $\bar{\tau}$ in \eqref{taubarBerRel}, and applying one-side Hoeffding's Inequality, we have
\begin{align} \label{tauboundBerRel}
\Pr (\tau \leq \bar{\tau}) \geq \Pr (\hp_{n_i} \geq  p-\varepsilon_r2^{-i}, i \leq \tau )
\geq \prod_{i=1}^\infty(1-\alpha_i) = 1-\alpha/2.
\end{align}
Moreover, since the stopping time $\tau \geq 1$ and by the stopping criterion defined in \eqref{stoppingcriterionBerRel}, $\hp_L$ has a lower bound, $\varepsilon_r$:
\begin{align}\label{hpUpperbound}
\Pr\left(\hp_L  \geq  \varepsilon_r \right)& = \Pr\left(\hp_{n_\tau} \geq \varepsilon_r 2^{-\tau}+\varepsilon_r  \right) \geq \Pr\left(\hp_{n_{\tau}}\geq 3\varepsilon_r2^{-{\bar{\tau}}}\right) =1
\end{align}
Combining \eqref{taubarBerRel} and \eqref{hpUpperbound}, the bound is obtained,
\begin{align*}
\Pr(n_{\tot} \leq n_{\up})  \geq \Pr(\tau \leq \bar{\tau}, \hp_L \geq \varepsilon_r) =  \Pr(\tau \leq \bar{\tau}) =1-\alpha/2.
\end{align*}
%\begin{align}
%&\Pr\left (\sum_{i=1}^{\tau} \left \lceil -\frac{4^i \log(\alpha_i)} {2\varepsilon_r^2 } \right\rceil+\left \lceil -\frac{\log (\alpha/4)}{2  (\hp_L \varepsilon_{r})^2} \right \rceil \leq N_{\text{up}}\right)\\ & \geq \Pr \left(\sum_{i=1}^{\tau} \left \lceil -\frac{4^i \log(\alpha_i)} {2\varepsilon_r^2 } \right\rceil \leq \sum_{i=1}^{\bar{\tau}} \left \lceil -\frac{4^i \log(\alpha_i)} {2\varepsilon_r^2 } \right\rceil , \left \lceil -\frac{\log (\alpha/4)}{2  (\hp_L \varepsilon_{r})^2} \right \rceil \leq \left \lceil -\frac{\log (\alpha/4)}{2  (\check{p}_L \varepsilon_{r})^2} \right \rceil\right)\\&
%\geq \Pr \left ( \tau \leq \bar{\tau}, \check{p}_L \leq \hp_L \right ) \\&
% = \Pr \left ( \check{p}_L \leq \hp_L | \tau \leq \bar{\tau}\right )\Pr \left( \tau \leq \bar{\tau}\right) \\&
% = \Pr \left( \tau \leq \bar{\tau}\right) = 1-\beta
%\end{align}
%By the argument above, the total cost $N_{\text{tot}}(\varepsilon_r,\alpha,p,\beta)$ will be bounded above by $N_{\text{up}}(\varepsilon_r, p,\alpha)$.
\end{proof}

\Section{Numerical example for {\tt meanMCber\_g}}\label{sec:meanMCbergexample}

Algorithm \ref{algabs} has been implemented in MATLAB, under the name {\tt meanMCber\_g} which is one the guaranteed algorithms in Guaranteed Automatic Integration Library (GAIL) \cite{GAIL_2_1}.

\Subsection{Numerical example for {\tt meanMCBer\_g} with absolute error tolerance}
To demonstrate its performance the algorithm {\tt meanMCber\_g} was run for $500$ replications, each with a different $p$ and $\varepsilon_a$, and with a fixed confidence level $1-\alpha=95\%$. The logarithms of $p$ and $\varepsilon_a$ were chosen independently and uniformly, namely 
\[
\log_{10} p \sim U[-3,-1], \qquad \log_{10} \varepsilon_a \sim U[-5,-2].
\]
For each replication, the inputs $\varepsilon_a$ and $\alpha=5\%$ were provided to {\tt meanMCber\_g}, along with a $\Ber(p)$ random number generator, and the answer $\hp_n$ was returned.  

Figure \ref{fig:abserrex} shows the ratio of the true error to the absolute error tolerance, $\abs{p-\hp_n}/\varepsilon_a$, for each of the $500$ replications, plotted against $p$.  All of the replications resulted in $\abs{p-\hp_n} \le \varepsilon_a$, which is better than guaranteed $95\%$ confidence level.  For some of these replications, {\tt meanMCber\_g} was asked to exceed its sample budget of $10^{10}$, namely when $\varepsilon_a \le \sqrt{\log(2/0.05)} \times 10^{-5} = 3.69 \times 10^{-5}$.

  \begin{figure}[htbp]
    \centering
    \includegraphics[width=9cm]{PlotBer/abs.eps} % requires the graphicx package
    \caption{Ratio of the actual absolute error to the absolute error tolerance from {\tt meanMCber\_g} versus $p$, for different random samples of $\Ber(p)$ random variables.}
    \label{fig:abserrex}
 \end{figure}

While it is encouraging to see that {\tt meanMCber\_g} provides the correct answer in all cases, it is concerning that {\tt meanMCber\_g} is rather conservative for small $p$.  This is due to the fact that the error of $\hp_n$ using $n$ samples is expected to be proportional to $\sqrt{\var(Y)/n}=\sqrt{p(1-p)/n}$.  Even though the error is small for small $p$, our algorithm does not take advantage of that fact.  To do so will require at least a loose lower bound on $p$ at the same time that the algorithm is trying to determine the sample size needed to estimate $p$ carefully.

\Subsection{Numerical example for {\tt meanMCBer\_g} with absolute error tolerance}
Using the sample parameters as in the previous example, we test the {\tt meanMCBer\_g} with the relative error tolerance, $\varepsilon_r$, which is random chosen with the distribution $\log_{10} \varepsilon_r \sim U[-2,-1]$. The plot is given in Figure \ref{fig:relerrex}.
  \begin{figure}[htbp]
    \centering
    \includegraphics[width=9cm]{PlotBer/rel.eps} % requires the graphicx package
    \caption{Ratio of the actual relative error to the relative error tolerance from {\tt meanMCber\_g} versus $p$, for different random samples of $\Ber(p)$ random variables.}
    \label{fig:relerrex}
 \end{figure}
Similar to the Figure \ref{fig:abserrex}, $100\%$ of the points satisfy the relative error criterion although the confidence level is only $95\%$. Our algorithm is conservative and tends to use more samples. When $p$ gets smaller, more points exceed the sample budget. The reason is that our algorithm determines the sample size based on the lower bound estimation of $p$, when $p$ is small, this information makes the algorithm require more samples, hence, it is more likely to get over budget.

%it is more likely to have the sample budget exceeded, as it will be more difficult to estimate the mean with a smaller $p$ than with a larger $p$ with a given relative error tolerance. In more details, we want the condition \eqref{probrel} to be satisfied, there is a denominator which is between $0$ and $1$,  when it gets smaller, we will need a larger sample size to meet this condition.

\Subsection{CLT \& Hoeffding's Inequality Confidence Interval Cost Comparison}
By using Hoeffding's Inequality to construct guaranteed fixed-width confidence interval, we definitely incur additional cost compared to an approximate CLT confidence interval.  The ratio of this cost is 
\begin{equation}
\frac{N_{\Hoeff}}{N_{\CLT}} = \frac{\left \lceil \log(2/\alpha)/{2\varepsilon_a^2} \right \rceil}{\left \lceil{ \Phi^{-1}(1-\alpha/2)}/{4 \varepsilon_a^2}\right\rceil} \approx  \frac{2\log(2/\alpha)}{\Phi^{-1}(1-\alpha/2)}.
\end{equation}
This ratio essentially depends on the uncertainty level $\alpha$ and is plotted in Figure \ref{fig:ratiovsalpha}. For $\alpha$ between $0.01\%$ to $10\%$ this ratio is between 3.64 to 5.09, which we believe is a reasonable price to pay for the added certainty of {\tt meanMCber\_g}.

  \begin{figure}[htbp]
    \centering
    \includegraphics[width=8cm]{PlotBer/plotHoeffCLTr.eps} % requires the graphicx package
    \caption{The computational cost ratio of using Hoeffding's Inequality and the CLT to construct a fixed-width confidence interval.}
    \label{fig:ratiovsalpha}
 \end{figure}
 
 
 \Chapter{Guaranteed Automatic Integration Library}\label{chapter:gail}
 
Guaranteed Automatic Integration Library (GAIL) \cite{GAIL_2_1} is a MATLAB toolbox created and developed by a group of researchers including Sou-Cheng T. Choi, Yuhan Ding, Fred J. Hickernell, Lan Jiang, Llu\'{i}s Antoni Jim\'{e}nez Rugama, Xin Tong, Yizhi Zhang and Xuan Zhou. It is a suite of algorithms including automatic and adaptive univariate function approximation ({\tt funappx\_g}), function minimization using linear splines ({\tt funmin\_g}), one-dimensional numerical integration using trapezoidal rule ({\tt integral\_g}) and mean estimation and numerical integration using Monte Carlo methods ({\tt meanMC\_g},{\tt cubMC\_g} and {\tt meanMCBer\_g}). This purpose is to create a reliable and robust open source MATLAB package, following the philosophy of reproducible research championed by Claerbout \cite{Claerbout10} and Donoho\cite{BuckheitDonoho95} and developing supportable scientific software that promote reliable reproducible research. In the next section, we will describe the structure of GAIL and the procedures by which we have been developing it.

\begin{sidewaysfigure}[hb]
\centering
\includegraphics[width=8in]
{GAILTree.pdf} 
\caption{GAIL directory hierarchy \label{GAILTree}}
\end{sidewaysfigure}

\Section{Introduction to GAIL}

Figure \ref{GAILTree} shows the structure and hierarchy of GAIL. It contains several levels of directories.
\begin{itemize}
\item Documentations: It includes the scripts that generate the HTMLs that support and explain the algorithms.
\item Papers: It includes the published papers related to GAIL.
\item Algorithms: It is the core part of GAIL, which contains all the algorithms shown in Figure \ref{GAILTree}.
\item Tests: It includes doctest and unittest for all the algorithms.
\item Workouts: The practical examples and test functions used to test GAIL.
\item Output Files: The output files generated from workouts and papers.
\item Third Party: The useful third party tools including doctest package and chebfun package.
\end{itemize}

 \Subsection{Downloads}
GAIL can be downloaded from

\url{http://code.google.com/p/gail/}

Alternatively, you can get a local copy of the GAIL repository with this command:
\begin{lstlisting}
git clone https://github.com/GailGithub/GAIL_Dev.git
\end{lstlisting}
You will need to install MATLAB 7 or a later version.


\Subsection{Documentation}
Detailed documentations are available at GAIL\_ Matlab/Documentation directory.

\Subsection{Algorithms}
GAIL Version 2.1 \cite{GAIL_2_1} includes the following eight algorithms:
\begin{itemize}
\item {\tt funappx\_g}: 1-D function approximation on a bounded interval with local adaption.
\item {\tt funmin\_g}: Global univariate function minimization on a bounded interval.
\item {\tt integral\_g}: 1-D numerical integration on a bounded interval based on Trapezoidal rule.
\item {\tt cubLattice\_g}: Numerical integration via quasi-Monte Carlo method using rank-1 Lattices.
\item {\tt cubSobol\_g}: Numerical integration via quasi-Monte Carlo method using Sobol points.
%\item {\tt meanMCabs\_g}: Monte Carlo method for estimating mean of a random variable with an absolute error tolerance.
\item {\tt meanMC\_g}: Monte Carlo method for estimating the mean of a random variable with a generalized error tolerance.
%\item {\tt cubMCabs\_g}: Monte Carlo method for numerical integration with an absolute error tolerance.
\item {\tt cubMC\_g}: Monte Carlo method for the numerical integration with a generalized error tolerance.
\item  {\tt meanMCBer\_g}: Monte Carlo method for estimating the mean of a Bernoulli random variable.
\end{itemize}

\Subsection{Installation Instruction}
\begin{enumerate}
\item Unzip the contents of the zip file to a directory and maintain the existing directory and sub-directory structure. (Please note: If you install into the toolbox sub-directory of the MATLAB program hierarchy, you will need to click the button "Update toolbox path cache" from the File/Preferences... dialog in MATLAB.)
\item In MATLAB, add the GAIL directory to your path. This can be done by running {\tt GAIL{\_}Install.m}. Alternatively, this can be done by selecting "File/Set Path..." from the main or Command window menus, or with the command path tool. We recommend that you select the "Save" button on this dialog so that GAIL is on the path automatically in future MATLAB sessions.
\item To check if you have installed GAIL successfully, type {\tt help meanMC\_g} to see if its documentation shows up.
\end{enumerate}

Alternatively, you could do this:
\begin{enumerate}
\item Download {\tt DownloadInstallGail 2\_1.m} and put it where you want GAIL to be installed.
\item Execute it in MATLAB.
\end{enumerate}

To uninstall GAIL, execute {\tt GAIL\_Uninstall}.

\Subsection{Tests}
Two tests are provided here, doctest and unittest, for each of the algorithms.
\begin{itemize}
\item To run doctests, simply call {\tt doctest meanMC\_g}
\item To run unittest, call {\tt run(ut\_meanMC\_g)}. Please note, unittest is only supported for MTALAB version 8 or later.
\end{itemize}


\Chapter{Conclusion and Future work}\label{chapter:comclusion}

Practitioners often use CLT based confidence intervals to construct Monte Carlo algorithms that estimating the mean of a random variable with the true variance been estimated by the sample variance, perhaps multiplied by some inflation factor.  Often, this approach works, but it has no guarantee of success. This thesis suggests a more trustworthy way to estimate the mean of a random variable, which is, our algorithms carries a guarantee that our answer will be true with probability at least $99\%$.

Figure \ref{fig:ratioBEnCLTn} shows the ratio of the sample size calculated by Berry-Esseen inequality and CLT with $\alpha = 1\%$, $n_\sigma = 10000$, $\alpha_\sigma = 0.5\%$ and the fudge factor 1.1, 1.2, 1.3.  Thus, the corresponding kurtosis maximum will be 2.5172, 5.7033 and 9.3976.  Here, $N_{\CLT}$ denotes the computational cost if we know the ratio $\sigma/\varepsilon$. The cost ratio is the penalty for having a guaranteed algorithm with the sample size derived from the Berry-Esseen Inequality. For smaller values of $N_{\CLT}$ , equivalently smaller $\sigma/\varepsilon$, this cost ratio can be rather large. However the absolute effect of this large penalty is mitigated by the fact that the total number of samples needed is not much. For larger $N_{\CLT}$ , equivalently larger $\sigma/\varepsilon$,  the cost ratio approaches 1.18. When the fudge factor becomes large, the kurtosis maximum increases. Hence, the sample size required will increase.

 \begin{figure}[htbp]
    \centering
    \includegraphics[width=8cm]{BE_CLT_n_ratio_fudge-2016-02-02-00-40-15.eps} % requires the graphicx package
    \caption{The ratio of the Berry-Esseen sample size versus the CLT sample size with different choice of the fudge factor}
    \label{fig:ratioBEnCLTn}
 \end{figure}
%
%with the true variance estimated by the sample variance, perhaps multiplied by some inflation factor.  Often, this approach works, but it has no guarantee of success.  The algorithms presented in this thesis is similar to the approach just described, but it carries guarantees. 
%
%Central Limit Theorem 
%
% The conclusion and future work should be more reflective about the importance of what you have accomplished.  You are trying to make MC computations more trustworthy even at the cost of additional samples.  You should discuss how large that price is, emphasize the inflation factor of your n versus a CLT choice of n.
%
%
%
%
%
%Practitioners often CLT-based confidence intervals with the true variance estimated by the sample variance, perhaps multiplied by some inflation factor.  Often, this approach works, but it has no guarantee of success.  The algorithms presented in this thesis is similar to the approach just described, but it carries guarantees. 

%In certain cases our procedure multiplies the computational cost by a large factor such as $2$ or $10$ or even $100$ compared to what one might spend based on the CLT with a known value of $\sigma$ (see Figure \ref{Costfig}).  While this seems inefficient, one should remember that the total elapsed time may still be well below several seconds.  Furthermore, one typically does not know $\sigma$ in advance, and our adaptive algorithm estimates $\sigma$ and then an appropriate sample size $n_\mu$ from the data.  Our algorithmic cost will be low when the unknown $\sigma$ is small and large when $\sigma$ is large.

Like any algorithm with guarantees, our algorithm does need to make assumptions about the random variable $Y$.  We assume a known bound on the kurtosis of $Y$, either specified directly or implied by the user's choice of the sample size for estimating the variance, $n_\sigma$, and the fudge factor, $\fudge$.  This is a philosophical choice.  We prefer not to construct an algorithm that assumes a bound on the variance of $Y$, because such an algorithm would not be guaranteed for $cY$ with $\abs{c}$ large enough.  If our algorithm works for $Y$, it will also work for $cY$, no matter how large $\abs{c}$ is. 

In practice the user may not know a priori if $\kappa \le \kmax$ since it is even more difficult to estimate $\kappa$ from a sample than it is to estimate $\sigma^2$. Thus, the choice of $\kmax$ relies on the user's best judgment.  Here are a few thoughts that might help.  One might try a sample of typical problems for which one knows the answers and use these problems to suggest an appropriate $\kmax$.  Alternatively, one may think of $\kmax$ not as a parameter to be prescribed, but as a reflection of the robustness of one's Monte Carlo algorithm having chosen $\alpha_\sigma$, $n_\sigma$ and $\fudge$. 

% Another way to look at the Theorem \ref{mainadaptthm} is that, like a pathologist, it tells you what went wrong if the algorithms fails: the kurtosis of the random variable must have been too large.
%In any case, as one can see in Figure \ref{Costfig}, in the limit of vanishing $\varepsilon/\sigma$, i.e., $N_{\text{CLT}} \to \infty$, the choice of $\tkappa_{\max}$ makes a negligible contribution to the total cost of the algorithm.  The main determinant of computational cost is $\varepsilon/\sigma$.
%
%\cite{BahSav56} prove in Corollary 2 that it is \emph{impossible} to construct exact confidence intervals for the mean of random variable whose distribution lies in a set satisfying a few assumptions. One of these assumptions is that the set of distributions is convex.  This assumption is violated by our assumption of bounded kurtosis in Theorem \ref{mainadaptthm}. Thus, we are able to construct guaranteed confidence intervals.
%
%Our algorithm is adaptive because $n_\mu$ is determined from the sample variance.  Information-based complexity theory tells us that adaptive information does not help for the integration problem for symmetric, convex sets of integrands, $f$, in the worst case and probabilistic settings \cite[Chapter 4, Theorem 5.2.1; Chapter 8, Corollary 5.3.1]{TraWasWoz88}.  Here, in Corollary \ref{integcor} the cone, $\cc_{\kmax}$, although symmetric, is not a convex set, so it is possible for adaption to help.

There are a couple of areas that suggest themselves for further investigation. For the algorithms {\tt meanMC\_g}, the cost upper bound has been derived, however, it is loose. An tighter upper bound could be derived. Also, the lower bound derivation is another possible research area. 

While {\tt meanMCBer\_g} in Algorithm \ref{algrel} satisfies a relative error tolerance. One would expect the number of samples required then to be proportional to $\var(Y)/(p \varepsilon_r)^2 \sim 1/(p \varepsilon_r^2)$ as $p\varepsilon_r \to 0$.  Our attempts so far at using Hoeffding's inequality results in an algorithm with computational cost proportional to $1/(p \varepsilon_r)^2$ as $p\varepsilon_r \to 0$.  Although, the algorithm for estimating the parameter $p$ to some specified relative error tolerance is successful, the computational cost is extravagant for small $p$. The literature mentioned in the beginning of Chapter \ref{chapter:meanMCberg} may provide a clue to an algorithm with optimal cost.  If this problem can be solved, then a natural extension would be to construct confidence intervals that satisfy either an absolute or relative error criterion, i.e., confidence intervals of the form $\Pr(\abs{p-\hp}/p\leq \max(\varepsilon_a, \varepsilon_r p)) \geq 1-\alpha$.
%
%One is relative error, i.e., a fixed width confidence interval of the form 
%\[
%\Pr[\abs{\mu-\hmu} \le \varepsilon \abs{\mu}] \ge 1-\alpha.
%\]
%Here the challenge is that the right hand side of the first inequality includes the unknown mean.
%
%Another area for further work is to provide guarantees for automatic quasi-Monte Carlo algorithms. Here the challenge is finding reliable formulas for error estimation.  Typical error bounds involve a semi-norm of the integrand that is harder to compute than the original integral.  For randomized quasi-Monte Carlo an estimate of the variance of the sample mean using $n$ samples does not tell you much about the variance of the sample mean using a different number of samples.
%
%
%In this thesis, we studied several algorithms that estimate the mean of a random variable, perform numerical integration via Monte Carlo sampling and estimate the probability. All these algorithms carry guarantees that the estimators satisfy fixed width confidence interval conditions.
%
%With the given knowledge of bounded kurtosis condition in \eqref{def:boundedkurtosis}, we first introduced two algorithms, {\tt meanMCabs\_g} in Algorithm \ref{alg:meanMCabsg} and {\tt cubMCabs\_g} in Algorithm \ref{alg:cubMCabsg}, which estimate the mean of a random variable and perform numerical integration to some prescribed absolute error tolerance $\varepsilon_a$. A probabilistic upper bound on the cost is calculated in Section \ref{sec:meanMCabsgcost}. A couple numerical examples are performed including integrating a product function \eqref{productfun} and a single hump \eqref{GaussianTestFun}. Time and accuracy are recorded and plotted to compare these two algorithms with other algorithms including {\tt integral} in MATLAB, {\tt Chebfun} \cite{Chebfun14}, {\tt cubSobol\_g} and {\tt cubLattice\_g} in GAIL \cite{GAIL_2_1}, which are shown Figure \ref{fig:GaussianTestFun}, \ref{fig:GaussianTestFunHD}, \ref{fig:keistertestfunabstol} and \ref{fig:GeoMeanAsianOptionabstol}.
%
%If one want our answer correct to some specified relative error tolerance $\varepsilon_r$, or some hybrid error criterion defined in Definition \ref{def:tolfun}, another two algorithms are provided. They are iteratively estimating the mean or the integral until a stopping criterion is reached. In details, {\tt meanMC\_g} in Algorithm \ref{alg:meanMCg} is used to estimate the mean of a random variable $Y$ to some generalized error tolerance $\tol(\varepsilon_a, \varepsilon_r)$ with a high confidence level; {\tt cubMC\_g} in Algorithm \ref{alg:cubMCg} uses the same idea to do numerical integration and it carries the guarantee that the estimator $\tilde{I}$ satisfies the fixed width confidence interval condition \eqref{cubMCgCI}. Computational cost upper bound is derived in Section \ref{sec:meanmcgcost}. Several numerical examples are given in Section \ref{sec:cubmcgnumericalexample} with a single hump integrand \eqref{GaussiankerTestFun}, the Keister test function \eqref{KeisterTestFun} and Multivariate Normal Probability function \eqref{MVNP}. Time and accuracy results are recorded and plotted with the comparison with several different algorithms.
%
%Sometime, one may be interested in estimating the probability. Given a Bernoulli random variable, which only have value 1 and 0, with the distribution $\Pr(Y =1) = p$, one may want to estimate this value $p$ to some specified absolute error tolerance $\varepsilon_a$ or relative error tolerance $\varepsilon_r$. In Chapter \ref{chapter:meanMCberg}, we suggest a way to estimate $p$ to a prescribed absolute error tolerance in Algorithm \ref{algabs} and to a relative error tolerance $\varepsilon_r$ in Algorithm \ref{algrel}. Computational cost and numerical examples followed.


%\Chapter{Future work}\label{chapter: future work}
%
%We believe that the guaranteed adaptive algorithms are of great interests to recent practitioners and researchers, as we have suggested a few ways to solve the problems like numerical integration using 1-D trapezoid rule {\tt integral\_g} , unitvariate function approximation {\tt funappx\_g} and {\tt funappxglobal\_g}, Monte Carlo methods for evaluating the mean of a random variable {\tt meanMCabs\_g} and {\tt meanMC\_g}, numerical integration via Monte Carlo {\tt cubMCabs\_g} and {\tt cubMC\_g}, probability estimation using Bernoulli random variables {\tt meanMCber\_g}. These works have been published in \cite{CDHHZ13} and \cite{HJLO12}. Also, we believe these the algorithms followed by theoretical research should be carefully implemented, documented and published. Thus, we have developed our MATLAB toolbox: Guaranteed Automatic Integration Library (GAIL) \cite{GAIL_2_1}. The work in this article has became part of this toolbox. Some possible future work that could be done will be explained in the following paragraphs.
%
%As {\tt cubMC\_g} does Monte Carlo method for numerical integration, some variance reduction techniques could be applied to make the simulation more efficient. Such techniques includes control variate, importance sampling and quasi-Monte Carlo(qMC) methods. As one of our colleagues has been working on guaranteed adaptive quasi-Monte Carlo method using Sobol' points \cite{Sobol67} and Lattice points, two algorithms {\tt cubSobol\_g} and {\tt cubLattice\_g} are part of GAIL. The other two variance reduction techniques may be the possible future work.
%
%For the algorithms {\tt meanMCabs\_g} and {\tt meanMC\_g}, the cost upper bound has been derived, the lower bound derivation is another possible research area. 
%
%While {\tt meanMCBer\_g} in Algorithm \ref{algrel} satisfies a relative error tolerance. One would expect the number of samples required then to be proportional to $\var(Y)/(p \varepsilon_r)^2 \sim 1/(p \varepsilon_r^2)$ as $p\varepsilon_r \to 0$.  Our attempts so far at using Hoeffding's inequality results in an algorithm with computational cost proportional to $1/(p \varepsilon_r)^2$ as $p\varepsilon_r \to 0$.  Although, the algorithm for estimating the parameter $p$ to some specified relative error tolerance is successful, the computational cost is extravagant for small $p$. The literature mentioned in the the beginning of Chapter \ref{chapter:meanMCberg} may provide a clue to an algorithm with optimal cost.  If this problem can be solved, then a natural extension would be to construct confidence intervals that satisfy either an absolute or relative error criterion, i.e., confidence intervals of the form $\Pr(\abs{p-\hp}/p\leq \max(\varepsilon_a, \varepsilon_r p)) \geq 1-\alpha$.


%\appendix
%
%\Appendix{MATLAB Code of meanMCabs\_g}
%\lstinputlisting{meanMCabs_g.m}
%\Appendix{MATLAB Code of cubMCabs\_g}
%\lstinputlisting{cubMCabs_g.m}
%\Appendix{MATLAB Code of meanMC\_g}
%\lstinputlisting{meanMC_g.m}
%%\input{meanMC_g.m}
%\Appendix{MATLAB Code of cubMC\_g}
%\lstinputlisting{cubMC_g.m}
%\Appendix{MATLAB Code of meanMCber\_g}
%\lstinputlisting{meanMCber_g.m}

\bibliographystyle{alpha}
\bibliography{Ljiang}

%\input{meanMCgCode}
%\input{cubMCgCode}
%\input{meanMCBergCode}
\end{document}  % end of document
